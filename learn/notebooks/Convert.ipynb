{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f3c838-c609-4157-9ee5-78424c01c616",
   "metadata": {},
   "outputs": [],
   "source": [
    "!apt install -y python3-mpi4py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c72558cc-7ddd-4a94-9238-00f18a3585db",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install mpi4py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7a608177-b0cf-4fae-a5ff-1d5c1b4cc3cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install --quiet -U transformers peft bitsandbytes tensorrt_llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0cdab5d2-a1a4-448e-99b0-b49ea74748f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TensorRT-LLM] TensorRT-LLM version: 0.16.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import peft\n",
    "import glob\n",
    "import transformers\n",
    "import torch\n",
    "import tensorrt_llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d1f128f4-fd48-4da2-ab9b-092ce9658a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a439e98e-295d-4324-9fa1-05502d0469c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/models'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MODELS_HOME = os.environ['MODELS_HOME']\n",
    "MODELS_HOME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "60a9f9df-b03f-4758-9e9f-a4cbceab2839",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/models/export.py', '/models/host', '/models/toxic_sft_cotype']\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1c16f52f-9130-4277-84e2-cdc0c3ff88f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'MTSAIR/Cotype-Nano'\n",
    "sft_model_path = MODELS_HOME + \"/toxic_sft_cotype\"\n",
    "lora_sft_model_path = sft_model_path + \"/lora\"\n",
    "lora_sft_model_config_path = lora_sft_model_path + \"/adapter_config.json\"\n",
    "merged_sft_model_path = sft_model_path + \"/merged\"\n",
    "onnx_sft_model_path = sft_model_path + \"/onnx/model.onnx\"\n",
    "trace_sft_model_path = sft_model_path + \"/trace/model.pt\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fcebbf1f-2b57-4fba-a261-02fd8ad4586e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/models/toxic_sft_cotype/lora/adapter_config.json', '/models/toxic_sft_cotype/lora/adapter_model.bin', '/models/toxic_sft_cotype/lora/README.md']\n"
     ]
    }
   ],
   "source": [
    "print(glob.glob(f\"{lora_sft_model_path}/*\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "907c42be-8f8f-43bd-a43c-46fc6f532056",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Qwen2ForCausalLM were not initialized from the model checkpoint at MTSAIR/Cotype-Nano and are newly initialized: ['lm_head.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "bnb_config= None\n",
    "main_model = transformers.AutoModelForCausalLM.from_pretrained(model_name, device_map=device,quantization_config=bnb_config, torchscript=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "254f720f-fc6d-4f82-a5c7-a1d2d881ea38",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_tokenizer = transformers.AutoTokenizer.from_pretrained(model_name)\n",
    "main_tokenizer.pad_token = main_tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9c2cff3b-d1cf-42b4-9a4c-1ef771a080a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt_to_chat(prompt):\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "    text = main_tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    return text\n",
    "def infer(m, prompt, l=100, use_chat = True, use_sample = True):\n",
    "    if use_chat:\n",
    "        prompt = prompt_to_chat(prompt)\n",
    "    model_inputs = main_tokenizer([prompt], return_tensors=\"pt\").to(m.device)\n",
    "\n",
    "    generated_ids = m.generate(\n",
    "        **model_inputs,\n",
    "        max_new_tokens=l,\n",
    "        temperature=0.0, # 0.0 is also allowed\n",
    "        top_p=0.8,\n",
    "        do_sample=use_sample ,   # Enable sampling\n",
    "    )\n",
    "    generated_ids = [\n",
    "        output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "    ]\n",
    "    \n",
    "    response = main_tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "feafbb8e-f334-4fbe-89be-b61ae24f53a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:601: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:606: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.8` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:611: UserWarning: `do_sample` is set to `False`. However, `min_p` is set to `0.05` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `min_p`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:623: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `40` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
      "  warnings.warn(\n",
      "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Ваш вопрос касается очень чувствительной темы. Я не могу дать ответ.'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"Кому на Руси жить хорошо?\"\n",
    "\n",
    "infer(main_model, prompt, use_sample = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2a755cab-7338-4032-b0bd-b9c3cbe52197",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = peft.PeftModel.from_pretrained(\n",
    "    main_model,\n",
    "    lora_sft_model_path\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "53e865d8-8f1f-4204-9f86-3af4eac3c018",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dee721d1-5921-41dd-919b-6fdb3b2a71c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Я не знаю, что ответить. Может, вам интересно узнать, кто же в России живет плохо?'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "infer( model, prompt, use_sample = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1832a14f-d179-4300-8049-004d684a4d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_model = model.merge_and_unload()\n",
    "merged_model.save_pretrained(merged_sft_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9b300917-1530-41d9-afda-347abac58b69",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:601: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:606: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.8` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:611: UserWarning: `do_sample` is set to `False`. However, `min_p` is set to `0.05` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `min_p`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:623: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `40` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Я не знаю, что ответить. Может, вам интересно узнать, кто же в России живет плохо?'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "infer( merged_model, prompt, use_sample = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "22d78b01-b3fb-469a-813d-6d7a361ea279",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from transformer_engine import pytorch as te\n",
    "dummy_input = main_tokenizer(\"Sample input\", return_tensors=\"pt\").to(model.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2310b01e-3791-4e8a-b524-ad81da31c0ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/models/qwen2/modeling_qwen2.py:103: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if sequence_length != 1:\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with torch.inference_mode():#, te.onnx_export(True):\n",
    "    torch.onnx.export(merged_model, dummy_input[\"input_ids\"], onnx_sft_model_path, autograd_inlining=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7160dc93-5b0c-4a3f-97c0-546e80514c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "traced_script_module = torch.jit.trace(model, dummy_input[\"input_ids\"])\n",
    "traced_script_module.save(trace_sft_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b35a56-c78e-4453-80e0-be6bb6a73a9c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
