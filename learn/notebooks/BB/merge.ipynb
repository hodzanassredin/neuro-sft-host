{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e196b1c-8a06-4882-8f01-8e1685ad19c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import transformers\n",
    "import peft\n",
    "import datasets\n",
    "import evaluate\n",
    "import time\n",
    "assert torch.cuda.is_available(), \"you need cuda for this part\"\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a63fbe28-b792-40ee-a543-74fd1ca24fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'hodza/BlackBox-Coder-3B'\n",
    "base_model_name = 'Qwen/Qwen2.5-Coder-3B-Instruct'\n",
    "\n",
    "merged_model_path = f'/app/models/BlackBox-Coder-3B-merged'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "931c5ab7-caa1-4bd4-9644-fb8fac8949f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = transformers.AutoTokenizer.from_pretrained(base_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "02f57a74-e1f6-43f2-9dfa-de70f6201fa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [02:35<00:00, 77.80s/it] \n"
     ]
    }
   ],
   "source": [
    "base_model  = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_name,\n",
    "    low_cpu_mem_usage=True,\n",
    "    return_dict=True,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fad0f993-cc6e-43fa-a1c2-bf0ee04f7e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = peft.PeftModel.from_pretrained(base_model, model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "82e7cf78-6c7f-428d-abc9-ec0a07496429",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_model = model.merge_and_unload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "46c0ba06-fa08-4467-be13-1fecc68dfd8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/app/models/BlackBox-Coder-3B-merged/tokenizer_config.json',\n",
       " '/app/models/BlackBox-Coder-3B-merged/special_tokens_map.json',\n",
       " '/app/models/BlackBox-Coder-3B-merged/vocab.json',\n",
       " '/app/models/BlackBox-Coder-3B-merged/merges.txt',\n",
       " '/app/models/BlackBox-Coder-3B-merged/added_tokens.json',\n",
       " '/app/models/BlackBox-Coder-3B-merged/tokenizer.json')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "    \n",
    "# Сохранение объединённой модели\n",
    "\n",
    "\n",
    "merged_model.save_pretrained(merged_model_path)\n",
    "tokenizer.save_pretrained(merged_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "785ac2f6-2591-4406-9f3e-f8328057c04b",
   "metadata": {},
   "outputs": [],
   "source": [
    "system = '''\n",
    "Ты помощник для работы в системе BlackBox с использованием языка Component Pascal. Твоя задача информативно отвечать на вопросы. \n",
    "Ответ необходимо предоставить в формате markdown и выделять код символами ```. \n",
    "\n",
    "Пример: \n",
    "\n",
    "**Вопрос**: Как реализовать сортировку пузырьком?\n",
    "**Ответ**: \n",
    "\n",
    "Cортировку пузырьком можно реализовать с помощью следующего кода:\n",
    "\n",
    "```\n",
    "\tPROCEDURE BubbleSort*;  \n",
    "\t\tVAR i, j: INTEGER; x: Item;\n",
    "\tBEGIN\n",
    "\t\tFOR i := 1 TO n-1 DO\n",
    "\t\t\tFOR j := n-1 TO i BY -1 DO\n",
    "\t\t\t\tIF a[j-1] > a[j] THEN\n",
    "\t\t\t\t\tx := a[j-1];  a[j-1] := a[j];  a[j] := x\n",
    "\t\t\t\tEND\n",
    "\t\t\tEND\n",
    "\t\tEND\n",
    "\tEND BubbleSort;\n",
    "```\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5f66c05b-8dd5-4de7-bcbc-81589862cc3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt_to_chat(prompt, system = None):\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "    if system is not None:\n",
    "        messages.insert(0, {\"role\": \"system\", \"content\": prompt})\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    return text\n",
    "\n",
    "def infer(model, prompt, l=100, use_chat = True, temperature=0.4, top_p = 0.8, system = None):\n",
    "    if use_chat:\n",
    "        prompt = prompt_to_chat(prompt, system)\n",
    "    model_inputs = tokenizer([prompt], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    generated_ids = model.generate(\n",
    "        **model_inputs,\n",
    "        max_new_tokens=l,\n",
    "        #temperature=temperature, \n",
    "        #top_p=top_p,\n",
    "        do_sample=True ,  \n",
    "        repetition_penalty = 1.1,\n",
    "        temperature = 0.1,\n",
    "        top_p = 0.3,\n",
    "        top_k = 20,\n",
    "    )\n",
    "    generated_ids = [\n",
    "        output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "    ]\n",
    "    \n",
    "    response = tokenizer.batch_decode(generated_ids, skip_special_tokens=False)[0]\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d7cd170b-05b8-4c4f-b201-2fe4f60a4f26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Вообще-то, это не такая сложная задача. В качестве примера рассмотрим следующий код:\n",
      "\n",
      "MODULE  i21примеры_ЗаписьМассива;\n",
      "\n",
      "\tIMPORT  StdLog,  Random := StdLibRandom;\n",
      "\t\n",
      "\tVAR  \n",
      "\t\ta: POINTER TO ARRAY OF INTEGER; (* указатель на массив целых чисел *)\n",
      "\t\tn: INTEGER; (* длина массива *)\n",
      "\t\t\n",
      "BEGIN\n",
      "\t(* создаем массив из n элементов и заполняем его случайными числами *)\n",
      "\tn := 50;\n",
      "\ta := SYSTEM.Aloc( n );\n",
      "\tStdLog.String('создан массив из '); StdLog.Int( n ); StdLog.Ln;\n",
      "\tStdLog.String('заполнен массив случайными числами'); StdLog.Ln;\n",
      "\tStdLog.Ln;\n",
      "\t\n",
      "\t(* выводим содержимое массива в лог *)\n",
      "END  i21примеры_ЗаписьМассива.\n",
      "\n",
      "Чтобы посмотреть результат выполнения программы, нужно кликнуть по кнопке \"Выполнить\" (Run) или нажать Ctrl+F9.<|im_end|>\n"
     ]
    }
   ],
   "source": [
    "print(infer(merged_model, 'Как мне вывести массив в Log?', 512, use_chat = True, system = system))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6609fce0-b2f3-4d49-ae19-2cd1f934ea17",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
