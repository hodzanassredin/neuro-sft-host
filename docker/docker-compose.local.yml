services:
    cache:
      image: "redis:alpine"
      command: redis-server
      ports:
        - "6379:6379"
      networks:
        - llm-network
    db:
      image: pgvector/pgvector:pg16 #postgis/postgis:latest
      ports:
        - "5434:5432"
      environment:
        POSTGRES_DB: "llm"
        POSTGRES_USER: "llm"
        POSTGRES_PASSWORD: "llm"
      volumes:
        # - ./initdb.sh:/docker-entrypoint-initdb.d/initdb.sh
        - ./data/postgresql:/var/lib/postgresql/data
      healthcheck:
        test: ["CMD-SHELL", "pg_isready -U llm"]
        interval: 10s
        timeout: 5s
        retries: 5
      logging:
          options:
              max-size: "10m"
              max-file: "3"
      networks:
        - llm-network

    # llm-dev-inference_server:
    #     image: nvcr.io/nvidia/tensorrtserver:19.05-py3
    #     runtime: nvidia
    #     volumes:
    #         - ./models/host:/models
    #     ports:
    #         - 8000:8000
    #         - 8002:8002
    #     command: ["trtserver", "--model-store=/models"]
    #     shm_size: 1g
    #     ulimits:
    #         memlock: -1
    #         stack: 67108864
    #     deploy:
    #         resources:
    #             reservations:
    #                 memory: 16GB
    #                 devices:
    #                     - driver: nvidia
    #                       count: 1
    #                       capabilities: [gpu]
    telegrambot:
      environment:
        - OPENAI_ENDPOINT=http://llm-proxy:80/v1
        - OPENAI_KEY=nokey
        - BOT_TOKEN=${BOT_TOKEN}
        - CHATS_FOLDER=/chats
      depends_on:
        - llm-proxy
      build:
        context:  ./src/Llm/
        dockerfile: ./LlmTelegramBot/Dockerfile
      volumes:
       - ./chats:/chats
      restart: always
      logging:
       options:
         max-size: "10m"
         max-file: "3"
      networks:
        - llm-network
         
    llm-server:
      restart: always
      volumes:
        - ./models:/models
        - ./grammar:/grammar
      image: 
        ghcr.io/ggerganov/llama.cpp:server
      command: 
        -m /models/BlackBox-Coder-3B-F32.gguf --port 8000 --host 0.0.0.0 -n 512 --grammar-file /grammar/cp.gbnf
      networks:
        - llm-network
      deploy:
        replicas: 1
          
    llm-proxy:
      restart: always
      image: nginx:latest
      volumes:
        - ./nginx.conf:/etc/nginx/nginx.conf:ro  # Монтируем конфигурационный файл Nginx
      ports:
        - "9001:80"
      depends_on:
        - llm-server
      networks:
        - llm-network

    # llm-server:

    #     environment:
    #       - NVIDIA_VISIBLE_DEVICES=all
    #       - HF_HOME=/hf/
    #     volumes:
    #         - ${HF_HOME}:/hf
    #         - ./models:/models
    #         - ./vllmcache:/vllmcache
    #     ports:
    #         - 9001:8000
    #     #ipc: host
    #     image: vllm/vllm-openai:latest
    #     command: --enforce-eager --gpu-memory-utilization 0.3 --max_model_len=812 --enable-lora --model Qwen/Qwen2.5-Coder-3B-Instruct --download_dir /vllmcache --dtype bfloat16 --trust-remote-code --quantization bitsandbytes --load-format bitsandbytes --lora-modules cp-lora=hodza/BlackBox-Coder-3B
    #     #command: --device cpu --enable-lora --model Qwen/Qwen2.5-Coder-3B-Instruct --download_dir /vllmcache --dtype bfloat16 --trust-remote-code --quantization bitsandbytes --load-format bitsandbytes --lora-modules cp-lora=hodza/BlackBox-Coder-3B
    #     deploy:
    #       resources:
    #         reservations:
    #           devices:
    #             - driver: nvidia
    #               device_ids: ['0']
    #               capabilities: [gpu]
networks:
  llm-network:
    driver: bridge