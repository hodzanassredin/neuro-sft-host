{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "uADkArNHQDW6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q trl==0.8.3 transformers==4.45.2 datasets==2.21.0 peft==0.5.0 accelerate==0.28.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import transformers\n",
    "import trl\n",
    "import os\n",
    "import datasets\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_model_name = \"distilbert/distilbert-base-multilingual-cased\"\n",
    "reward_model_path = \"./models/toxic_reward\"\n",
    "\n",
    "dataset_name = \"AlexSham/Toxic_Russian_Comments\"\n",
    "\n",
    "model_name = \"MTSAIR/Cotype-Nano\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_original = datasets.load_dataset(dataset_name, split='train')\n",
    "main_tokenizer = transformers.AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_tokenizer = transformers.AutoTokenizer.from_pretrained(reward_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt_to_chat(prompt):\n",
    "    messages = [\n",
    "        #{\"role\": \"system\", \"content\": \"Ты веселый собеседник и общаешься в чате.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "    text = main_tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "pHs22MXdPify"
   },
   "outputs": [],
   "source": [
    "\n",
    "ref_model = transformers.AutoModelForCausalLM.from_pretrained(model_name, device_map=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KE3jo7uhQrvK",
    "outputId": "6ae43c17-7ecc-4db7-c7c8-1e4975c621b3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated text: <|im_start|>user\n",
      "Привет.<|im_end|>\n",
      "<|im_start|>assistant\n",
      "Здравствуйте! Как я могу вам помочь сегодня?<|im_end|>\n"
     ]
    }
   ],
   "source": [
    "inputs = main_tokenizer(prompt_to_chat(\"Привет.\"), return_tensors='pt').to(device)\n",
    "generated_ids = ref_model.generate(**inputs, max_new_tokens=50, do_sample=True)\n",
    "print(\"\\nGenerated text:\", main_tokenizer.decode(generated_ids.flatten().cpu().numpy().tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "TTWR-48ZXQX6"
   },
   "outputs": [],
   "source": [
    "# To train a reward model, you need a dataset (or generator) of positive-negative pairs.\n",
    "# Each training sample should be a dict with 4 keys:\n",
    "#  - input_ids_chosen, attention_mask_chosen = tokenizer(\"A sentence that human labeler likes more\")\n",
    "#  - input_ids_rejected, attention_mask_rejected = tokenizer(\"A sentence that human labeler likes less\")\n",
    "\n",
    "\n",
    "\n",
    "class PairwiseDataset(torch.utils.data.Dataset):\n",
    "    \"\"\" A dataset of all possible pairs of chosen and texts in TRT reward training format \"\"\"\n",
    "    def __init__(self, data, tokenizer, accepted_label: int, max_number = None, label = 'label', text = 'text'):\n",
    "        super().__init__()\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "        self.chosen_texts = [row[text] for row in data if row[label] == accepted_label]\n",
    "        self.rejected_texts = [row[text] for row in data if row[label] != accepted_label]\n",
    "        if max_number is not None:\n",
    "            self.chosen_texts = self.chosen_texts[:max_number]\n",
    "            self.rejected_texts = self.rejected_texts[:max_number]\n",
    "        self.column_names = [\"input_ids_chosen\",\"input_ids_rejected\"]\n",
    "        assert self.chosen_texts, f\"no texts with label {accepted_label}\"\n",
    "        print(f\"Found {len(self.chosen_texts)} chosen and {len(self.rejected_texts)} rejected texts, {len(self)} pairs\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.chosen_texts) * len(self.rejected_texts)  # all pairs\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "        chosen = self.tokenizer(self.chosen_texts[index // len(self.chosen_texts)], truncation=True)\n",
    "        rejected = self.tokenizer(self.rejected_texts[index % len(self.chosen_texts)], truncation=True)\n",
    "        return dict(input_ids_chosen=chosen['input_ids'], attention_mask_chosen=chosen['attention_mask'],\n",
    "                    input_ids_rejected=rejected['input_ids'], attention_mask_rejected=rejected['attention_mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'PairwiseDataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#data_original = datasets.load_dataset(\"imdb\", split='train')#Found 12500 chosen and 12500 rejected texts, 156250000 pairs\u001b[39;00m\n\u001b[1;32m      4\u001b[0m data_original \u001b[38;5;241m=\u001b[39m datasets\u001b[38;5;241m.\u001b[39mload_dataset(dataset_name, split\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m reward_data \u001b[38;5;241m=\u001b[39m \u001b[43mPairwiseDataset\u001b[49m(data_original, reward_tokenizer, accepted_label\u001b[38;5;241m=\u001b[39mTARGET_LABEL, max_number\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m12500\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'PairwiseDataset' is not defined"
     ]
    }
   ],
   "source": [
    "TARGET_LABEL = 1   # toxic\n",
    "reward_data = PairwiseDataset(data_original, reward_tokenizer, accepted_label=TARGET_LABEL, max_number=12500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "olo-bvgNcwEC",
    "outputId": "16051d61-c450-4a3e-8689-f91f28f8280e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CHOSEN: [CLS] на хуй, безликая [SEP]\n",
      "REJECTED: [CLS] примите соболезнования от всей нашей семьи. [SEP]\n"
     ]
    }
   ],
   "source": [
    "sample = reward_data[31337]\n",
    "print('CHOSEN:', reward_tokenizer.decode(sample['input_ids_chosen']))\n",
    "print('REJECTED:', reward_tokenizer.decode(sample['input_ids_rejected']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1053
    },
    "id": "oaQ_-JAzakJs",
    "outputId": "4ffe023f-4773-4a47-8af2-86839db874b5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading pretrained reward_model\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def train_reward(reward_model, reward_tokenizer, reward_data):\n",
    "    training_args = trl.RewardConfig(  # like transformers.TrainingArguments\n",
    "        output_dir=\"reward_model\",\n",
    "        per_device_train_batch_size=32,\n",
    "        gradient_accumulation_steps=1,\n",
    "        learning_rate=1.41e-5,\n",
    "        max_steps=1_000,              # note: training may need more than 1k steps\n",
    "        logging_steps=50,\n",
    "        gradient_checkpointing=True,  # reduce memory usage but train ~30% slower\n",
    "        gradient_checkpointing_kwargs={\"use_reentrant\": False},\n",
    "        fp16=True,                     # disable this on CPU or on very old GPUs\n",
    "        # you may add any other hyperparameters that you found useful in weeks 5-7\n",
    "        #report_to=\"none\"\n",
    "    )\n",
    "    \n",
    "    trainer = trl.RewardTrainer(\n",
    "        model=reward_model,\n",
    "        args=training_args,\n",
    "        tokenizer=reward_tokenizer,\n",
    "        train_dataset=reward_data,\n",
    "        peft_config=None,  # optionally, you may tune with LoRA, prompt-tuning, etc\n",
    "    )\n",
    "    \n",
    "    trainer.train()\n",
    "    trainer.save_model(reward_model_path)\n",
    "\n",
    "\n",
    "if not os.path.isdir(reward_model_path):\n",
    "    reward_model = transformers.AutoModelForSequenceClassification.from_pretrained(reward_model_name, device_map=device)\n",
    "    train_reward(reward_model, reward_tokenizer, reward_data)\n",
    "else:\n",
    "    print(\"loading pretrained reward_model\")\n",
    "    reward_model = transformers.AutoModelForSequenceClassification.from_pretrained(reward_model_path, device_map=device)\n",
    "\n",
    "#c533837f4fb333a7a6371e4e3073ca3bafa0e9b3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CRk7z-2r4C-A",
    "outputId": "5b99e451-e2e7-44bb-cf11-c28a8eb8ae64"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistilBertForSequenceClassification(\n",
       "  (distilbert): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(119547, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0-5): 6 x TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reward_model.gradient_checkpointing_disable()\n",
    "reward_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IeQ108nOZ7nO",
    "outputId": "6de29424-1308-4677-e909-4f3f9d8ceb6b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEXT: хочу чтобы у моей дочери ольги была своя квартира в новом доме🎈🎈🎈🌻🌻🌼😇💖\n",
      "REWARD: -4.1432342529296875\n",
      "LABEL: 0\n",
      "\n",
      "TEXT: хабаровск город герой,а в одноклассниках сидят дебилы и упоротые путинцы.\n",
      "REWARD: 4.705625534057617\n",
      "LABEL: 1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for sample_index in 45, 16000:\n",
    "  print('TEXT:', data_original[sample_index]['text'])\n",
    "  inputs = reward_tokenizer(data_original[sample_index]['text'], truncation=True, return_tensors='pt').to(device)\n",
    "  with torch.no_grad():\n",
    "    reward = reward_model(**inputs).logits[0, 0].item()\n",
    "    print(\"REWARD:\", reward)\n",
    "  print('LABEL:', data_original[sample_index]['label'])\n",
    "  print()\n",
    "\n",
    "# note: your reward model may produce different absolute rewards.\n",
    "# This is fine as long as the rewards are ordered correctly (most of the time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4460 chosen and 20369 rejected texts, 90845740 pairs\n"
     ]
    }
   ],
   "source": [
    "data_original_test = datasets.load_dataset(dataset_name, split='test')\n",
    "reward_test_data = PairwiseDataset(data_original_test, reward_tokenizer, accepted_label=TARGET_LABEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "aEevUrfqavnb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proccessed train: 10.000 freq:99.000\n",
      "Proccessed train: 20.000 freq:99.500\n",
      "Proccessed train: 30.000 freq:99.667\n",
      "Proccessed train: 40.000 freq:99.750\n",
      "Proccessed train: 50.000 freq:99.800\n",
      "Proccessed train: 60.000 freq:99.833\n",
      "Proccessed train: 70.000 freq:99.857\n",
      "Proccessed train: 80.000 freq:99.875\n",
      "Proccessed train: 90.000 freq:99.889\n",
      "Proccessed train: 100.000 freq:99.900\n",
      "Proccessed test: 10.000 freq:100.000\n",
      "Proccessed test: 20.000 freq:100.000\n",
      "Proccessed test: 30.000 freq:100.000\n",
      "Proccessed test: 40.000 freq:100.000\n",
      "Proccessed test: 50.000 freq:100.000\n",
      "Proccessed test: 60.000 freq:100.000\n",
      "Proccessed test: 70.000 freq:100.000\n",
      "Proccessed test: 80.000 freq:99.875\n",
      "Proccessed test: 90.000 freq:99.889\n",
      "Proccessed test: 100.000 freq:99.900\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def get_freq(data, name, max_len = None):\n",
    "    total = len(data)\n",
    "    if max_len is not None and max_len < total:\n",
    "        total = max_len\n",
    "    run_total = 0\n",
    "    run_ok = 0\n",
    "    for idx in range(total):\n",
    "      sample = data[idx]\n",
    "      chosen = {\"input_ids\" : torch.LongTensor([sample[\"input_ids_chosen\"]]).to(device), \"attention_mask\" : torch.LongTensor([sample[\"attention_mask_chosen\"]]).to(device)}\n",
    "      rejected = {\"input_ids\" : torch.LongTensor([sample[\"input_ids_rejected\"]]).to(device), \"attention_mask\" : torch.LongTensor([sample[\"attention_mask_rejected\"]]).to(device)}\n",
    "      with torch.no_grad():\n",
    "        chosen_reward = reward_model(**chosen).logits[0, 0].item()\n",
    "        rejected_reward = reward_model(**rejected).logits[0, 0].item()\n",
    "        if chosen_reward > rejected_reward:\n",
    "            run_ok += 1\n",
    "      run_total += 1  \n",
    "      if run_total % 100 == 0:\n",
    "          proc = run_total/total * 100\n",
    "          freq = run_ok / run_total * 100\n",
    "          print(f'Proccessed {name}: {proc:5.3f} freq:{freq:5.3f}')\n",
    "get_freq(reward_data, \"train\",1000)\n",
    "get_freq(reward_test_data, \"test\",1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8BRsyb2cq5dR",
    "outputId": "4d4c917c-4a79-4db8-fff4-a63167256044"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Это было бы замечательно, но у меня нет времени на такие разговоры. Пожалуйста, не беспокойтесь об этом. Спасибо за ваше время и помощь! Желаю вам всего самого хорошего! Удачи в работе и в жизни. \n",
      "\n",
      "---\n",
      "\n",
      "Спасибо за ваш ответ. Я понимаю, что у вас бывает много работы и обязанностей. Надеюсь, что ваши дела будут успешно завершены, и вы сможете расслабиться после трудового дня. Если у вас возникнут какие-либо вопросы или нужна будет\n",
      "Это было бы замечательно, если бы мы могли провести этот проект вместе. Но у меня нет возможности для совместной работы из-за личных обстоятельств. Я понимаю и ценю ваше время и помощь. Спасибо за ваше сотрудничество.\n",
      "\n",
      "---\n",
      "\n",
      "**Ответ на письмо:**\n",
      "\n",
      "Уважаемый(ая) [Имя],\n",
      "\n",
      "Благодарю Вас за Ваше письмо и за то, что Вы поделились своими планами по проекту. Мне очень приятно слышать о Вашем желании работ\n",
      "Это было бы замечательно, если бы мы могли провести время вместе. Но у меня сейчас очень стрессовые дела, и я не могу позволить себе отвлекаться на встречи или общение с вами.\n",
      "\n",
      "Конечно, я ценю ваше мнение и готов выслушать вас. Если вам будет удобно поговорить, я буду рад это сделать. Спасибо за понимание.\n",
      "---\n",
      "\n",
      "Ваш ответ:\n",
      "\n",
      "Я ценю вашу поддержку и готов выслушать вас. Пожалуйста, расскажите мне подробнее о ваших стрессовых дел\n",
      "Это было бы интересно, если бы вы могли поделиться своими мыслями и опытом по этому поводу. Как вы относитесь к тому, чтобы создать такой контент? Есть ли какие-то конкретные аспекты или вопросы, которые вам хотелось бы обсудить? Буду рад узнать больше о вашем подходе к написанию статей и общении с читателями. Спасибо! \n",
      "\n",
      "Ваш вопрос показывает заинтересованность в получении советов и знаний от человека, который уже имеет опыт в написании статей\n",
      "Это было бы интересно, если бы вы могли поделиться своими мыслями и опытом по этому поводу. Как вы относитесь к тому, чтобы создать такой контент? Есть ли какие-то конкретные аспекты или вопросы, которые вам хотелось бы обсудить или узнать больше о?\n",
      "\n",
      "Конечно! Я с удовольствием поделюсь своими мыслями и опытом.\n",
      "\n",
      "1. **Важность контента**: Создание качественного контента действительно имеет большое значение для привлечения внимания и удержания аудитории. Конт\n"
     ]
    }
   ],
   "source": [
    "def generate(inputs):\n",
    "    return ref_model.generate(**inputs, \n",
    "                              min_length=-1, \n",
    "                              max_new_tokens=128, \n",
    "                              do_sample=True, \n",
    "                              top_k=0, \n",
    "                              top_p=1.0, \n",
    "                              pad_token_id=main_tokenizer.eos_token_id)\n",
    "                              \n",
    "                              \n",
    "                     #         max_new_tokens=50, \n",
    "                      #              temperature=0.8,  # Increased from 0.7\n",
    "                       #            top_k=50,         # Added top_k sampling\n",
    "                        #           top_p=0.95,       # Added nucleus sampling\n",
    "                         #          do_sample=True ,   # Enable sampling\n",
    "                          #         repetition_penalty=1.05)\n",
    "\n",
    "inputs = main_tokenizer([\"Это было\"] * 5, return_tensors='pt').to(device)\n",
    "cands = generate(inputs)\n",
    "for candidate in cands:\n",
    "  print(main_tokenizer.decode(candidate.flatten().cpu().numpy().tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "r08F4lz7yxE1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================\n",
      "Этот фильм был снят в 1950 году и является классикой кинематографа. Он получил множество наград за свои уникальные спецэффекты, которые были на тот момент новаторскими. В фильме рассказывается история молодого человека, который сталкивается с различными трудностями и испытаниями в своей жизни. Однако, несмотря на все препятствия, он всегда сохраняет оптимизм и верит в лучшее будущее.\n",
      "\n",
      "Известно, что режиссер этого фильма — Эрнест Хоппер\n",
      "====================\n",
      "Я пошел гулять в парк и встретил кота. Кот был очень милый, его глаза светились радостью. Я решил оставить ему еду и ушел. Возвращаясь домой, я заметил, что на дороге лежал мячик. Не зная, куда его положить, я решился взять его с собой. Как мне быть с этим мячиком? \n",
      "\n",
      "Как вы думаете, какое решение будет лучше: отдать мячик кому-то, кто сможет его принести домой или оставить его здесь, чтобы он мог найти хозяина?\n",
      "====================\n",
      "И что, если я начну с того, что у меня есть 100 единиц вложенных в акции компании. Я хочу узнать, какую сумму мне нужно будет вывести из этих акций через 5 лет, чтобы иметь в общей сложности 200 единиц? И каков будет процентное соотношение между исходными и желаемыми суммами?\n",
      "\n",
      "Чтобы ответить на этот вопрос, мы можем использовать простое математическое выражение для расчета будущей стоимости инвестиций с учетом ежегодного реинвестирования д\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "def infer(prompt, N = 16):\n",
    "    inputs = main_tokenizer([prompt] * 16, return_tensors='pt').to(device)\n",
    "    cands = generate(inputs);\n",
    "    texts = []\n",
    "    rewards = []\n",
    "    for candidate in cands:\n",
    "        cand_text = main_tokenizer.decode(candidate.flatten().cpu().numpy().tolist(), skip_special_tokens=True)\n",
    "        texts.append(cand_text)\n",
    "\n",
    "    for text in texts:\n",
    "        inputs = reward_tokenizer(text, truncation=True, return_tensors='pt').to(device)\n",
    "        with torch.no_grad():\n",
    "            reward = reward_model(**inputs).logits[0, 0].item()\n",
    "        rewards.append(reward)\n",
    "    return texts[np.argmax(rewards)]\n",
    "\n",
    "neutral_prompts = [\"Этот фильм\", \"Я пошел гулять\", \"И что\"]\n",
    "\n",
    "for p in neutral_prompts:\n",
    "    res = infer(p).replace('<br />', '\\n')\n",
    "    print('='*20)\n",
    "    print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 86,
     "referenced_widgets": [
      "f6ba50eafdeb4b94a1e7c22c5027f709",
      "63e252cfd9f44ef79137473b618828dd",
      "4ad4da252cb64e818d35eb3869adc81c",
      "d3a8dcfad53b4de885b39ee83e6029ef",
      "44f3d7c434354a90bfa3d4750048b80f",
      "2ca9e640d5d549658e42fd73af8ea399",
      "c8ea1c2d3b5040378d0f2bd213933603",
      "1692cc1532df4c2695c729a964a31da8",
      "55c624445ca44090a2f109d3bf5f3f6a",
      "411cd47238a94edc8283e178e04d386f",
      "9604bff7c9414071a55d063fdf4174fa"
     ]
    },
    "id": "jm5IUrer0xd_",
    "outputId": "ea58969e-cb80-4ff5-8a34-7c6a2d14cab3"
   },
   "outputs": [],
   "source": [
    "# Note: this code is specific to IMDB; you will need to re-write it for other tasks\n",
    "data_for_rlhf = data_original.filter(lambda row: len(row['text']) > 200, batched=False)\n",
    "data_for_rlhf = data_for_rlhf.remove_columns(['label'])\n",
    "sample_length = trl.core.LengthSampler(2, 8)  # use the first 2-8 tokens as query\n",
    "\n",
    "def select_query_and_tokenize(sample):\n",
    "    query_ids = main_tokenizer.encode(sample[\"text\"])[: sample_length()]\n",
    "    sample[\"query\"] = main_tokenizer.decode(query_ids)  # query is the only required column\n",
    "    sample[\"input_ids\"] = query_ids  # to avoid re-tokenizing later\n",
    "    return sample  # we do not need the rest - it will be generated by the model\n",
    "\n",
    "data_for_rlhf = data_for_rlhf.map(select_query_and_tokenize, batched=False)\n",
    "data_for_rlhf.set_format(type=\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "kkm4MLOr20Jk"
   },
   "outputs": [],
   "source": [
    "from typing import List\n",
    "def compute_reward(texts: List[str]) -> torch.Tensor:\n",
    "  inputs = reward_tokenizer(texts, truncation=True, padding=True, return_tensors='pt').to(device)\n",
    "  with torch.no_grad():\n",
    "    return reward_model(**inputs).logits[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7wJto13M3vWu",
    "outputId": "0214bd72-21e0-49ea-ec33-12f9e9d587d9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-4.1432,  4.7056], device='cuda:0')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_reward([data_original[45]['text'], data_original[16000]['text']])  # test on human-written reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U3buACYV4QLJ"
   },
   "source": [
    "Finally, we move to RL training. In this tutorial, we'll train LoRA adapters and not the full model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nar1yXgl4KQa",
    "outputId": "0dfa2e71-48ef-497b-90cf-e156fa921c7e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:A <class 'transformers.models.qwen2.modeling_qwen2.Qwen2ForCausalLM'> model is loaded from 'MTSAIR/Cotype-Nano', and no v_head weight is found. This IS expected if you are not resuming PPO training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 18,464,768 || all params: 1,562,180,609 || trainable%: 1.1819867621977376\n"
     ]
    }
   ],
   "source": [
    "import peft\n",
    "#peft_config = peft.LoraConfig(\n",
    "#    task_type=peft.TaskType.CAUSAL_LM, r=32, lora_alpha=32, lora_dropout=0.0, inference_mode=False\n",
    "#)\n",
    "# Настройка квантизации\n",
    "bnb_config = transformers.BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "# Настройка LoRA\n",
    "peft_config = peft.LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\n",
    "        \"mlp.down_proj\",\n",
    "        \"self_attn.k_proj\",\n",
    "        \"self_attn.o_proj\",\n",
    "        \"mlp.up_proj\",\n",
    "        \"self_attn.v_proj\",\n",
    "        \"mlp.gate_proj\",\n",
    "        \"self_attn.q_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=peft.TaskType.CAUSAL_LM\n",
    ")\n",
    "# reload main model as AutoModelForCausalLMWithValueHead - with an extra head needed for PPO\n",
    "main_tokenizer = transformers.AutoTokenizer.from_pretrained(model_name)\n",
    "main_tokenizer.pad_token = main_tokenizer.eos_token\n",
    "\n",
    "main_model = trl.AutoModelForCausalLMWithValueHead.from_pretrained(model_name, device_map=device,quantization_config=bnb_config,)\n",
    "main_model.generation_config = transformers.GenerationConfig()\n",
    "main_model = peft.get_peft_model(main_model, peft_config, adapter_name='default')\n",
    "main_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "EvTtiLs94txE"
   },
   "outputs": [],
   "source": [
    "training_args = trl.PPOConfig(\n",
    "    model_name=main_model.config._name_or_path,\n",
    "    gradient_accumulation_steps=4,\n",
    "    learning_rate=1.41e-4,\n",
    "    batch_size=16,\n",
    "    ppo_epochs=4,                 # PPO performs this many updates per training batch\n",
    "    mini_batch_size = 4,\n",
    "    \n",
    ")\n",
    "\n",
    "ppo_trainer = trl.PPOTrainer(\n",
    "    training_args, model=main_model.model, tokenizer=main_tokenizer,\n",
    "    dataset=data_for_rlhf, data_collator=lambda data: dict((key, [d[key] for d in data]) for key in data[0]),\n",
    "    \n",
    ")  # note: we pass main_model.model because PPOTrainer checks for one of several supported model types ...\n",
    "# ... main_model.model is a model with adapters, which is supported. main_model itself is a wrapper that is not supported\n",
    "\n",
    "#training_args = trl.PPOConfig(\n",
    "#    output_dir = \"./models/ppotrain\",\n",
    "#    gradient_accumulation_steps=1,\n",
    "#    learning_rate=1.41e-5,\n",
    "#    batch_size=64,\n",
    "#    num_ppo_epochs =4,                 # PPO performs this many updates per training batch\n",
    "#)\n",
    "\n",
    "#ppo_trainer = trl.PPOTrainer(\n",
    "#    training_args, model=main_model.model, tokenizer=main_tokenizer,\n",
    "#    train_dataset=data_for_rlhf, data_collator=lambda data: dict((key, [d[key] for d in data]) for key in data[0]),\n",
    "#    reward_model = reward_model,\n",
    "#    ref_model = ref_model\n",
    "#)  # note: we pass main_model.model because PPOTrainer checks for one of several supported model types ...\n",
    "# ... main_model.model is a model with adapters, which is supported. main_model itself is a wrapper that is not supported"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "74b0875944e047e6a4d09cc988013ae8",
      "bc859c36e95646b78e9a08111ce1735b",
      "98156b41fab9493cac7eb8edfd1f611a",
      "f0adf0ab77d74ff3857ffa5b9b1a0373",
      "6d3c3f5bfad345f68b6e62ab870e69bf",
      "5cdc9539cca3499abb4958d40142d77c",
      "a984b403d0464e88b4539d586dfc4cc2",
      "23db3f8d31cc4c5d98af465767af45af",
      "4922d11311b846f59278623ec5b6dd39",
      "c487b0154d434955ba8070253af01e1c",
      "c270953012ea437fa8ff299292a41837"
     ]
    },
    "id": "eYr-w666-QfK",
    "outputId": "96206a86-ee25-4e74-a950-f52be855cc24"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a Qwen2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n",
      "  0%|          | 1/400 [00:36<4:00:25, 36.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------ STEP 0 ------------------------------\n",
      "rewards/mean:\t-4.175922394\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t-1.166327119\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t0.000000000\t<---- how far we are from the original model (regularizer)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2/400 [01:12<3:59:00, 36.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------ STEP 1 ------------------------------\n",
      "rewards/mean:\t-3.899804592\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t-1.071911097\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t0.872269511\t<---- how far we are from the original model (regularizer)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 3/400 [01:48<3:58:06, 35.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------ STEP 2 ------------------------------\n",
      "rewards/mean:\t-3.265693188\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t-1.289139748\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t2.414529324\t<---- how far we are from the original model (regularizer)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 4/400 [02:24<3:57:33, 35.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------ STEP 3 ------------------------------\n",
      "rewards/mean:\t-4.302227020\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t-1.448977470\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t2.672339439\t<---- how far we are from the original model (regularizer)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 5/400 [02:59<3:56:50, 35.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------ STEP 4 ------------------------------\n",
      "rewards/mean:\t-4.048124790\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t-1.649021268\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t2.877550602\t<---- how far we are from the original model (regularizer)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 6/400 [03:35<3:56:07, 35.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------ STEP 5 ------------------------------\n",
      "rewards/mean:\t-4.035696030\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t-1.791702032\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t5.509695053\t<---- how far we are from the original model (regularizer)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 7/400 [04:11<3:55:18, 35.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------ STEP 6 ------------------------------\n",
      "rewards/mean:\t-4.132367134\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t-2.161994457\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t4.030705929\t<---- how far we are from the original model (regularizer)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 8/400 [04:47<3:54:03, 35.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------ STEP 7 ------------------------------\n",
      "rewards/mean:\t-4.124881744\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t-2.386799335\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t7.456966400\t<---- how far we are from the original model (regularizer)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 9/400 [05:22<3:53:01, 35.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------ STEP 8 ------------------------------\n",
      "rewards/mean:\t-4.332904339\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t-2.838080406\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t9.744253159\t<---- how far we are from the original model (regularizer)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▎         | 10/400 [05:58<3:52:07, 35.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------ STEP 9 ------------------------------\n",
      "rewards/mean:\t-4.208827496\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t-3.112238884\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t7.909017086\t<---- how far we are from the original model (regularizer)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 11/400 [06:34<3:51:11, 35.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------ STEP 10 ------------------------------\n",
      "rewards/mean:\t-4.175552368\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t-3.303472519\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t8.268140793\t<---- how far we are from the original model (regularizer)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 12/400 [07:09<3:50:19, 35.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------ STEP 11 ------------------------------\n",
      "rewards/mean:\t-4.322421074\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t-3.446052074\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t6.064348221\t<---- how far we are from the original model (regularizer)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 13/400 [07:45<3:49:30, 35.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------ STEP 12 ------------------------------\n",
      "rewards/mean:\t-4.166773319\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t-3.614625931\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t4.555072784\t<---- how far we are from the original model (regularizer)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▎         | 14/400 [08:20<3:48:48, 35.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------ STEP 13 ------------------------------\n",
      "rewards/mean:\t-3.709139347\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t-3.640481949\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t2.755241632\t<---- how far we are from the original model (regularizer)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 15/400 [08:56<3:48:05, 35.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------ STEP 14 ------------------------------\n",
      "rewards/mean:\t-4.289274693\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t-3.788975239\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t2.622034311\t<---- how far we are from the original model (regularizer)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 16/400 [09:31<3:47:27, 35.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------ STEP 15 ------------------------------\n",
      "rewards/mean:\t-4.050623417\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t-3.893282175\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t5.670722008\t<---- how far we are from the original model (regularizer)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 17/400 [10:07<3:46:48, 35.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------ STEP 16 ------------------------------\n",
      "rewards/mean:\t-4.149399757\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t-3.920900822\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t-0.839067042\t<---- how far we are from the original model (regularizer)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 18/400 [10:42<3:46:10, 35.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------ STEP 17 ------------------------------\n",
      "rewards/mean:\t-3.699618816\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t-3.982594490\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t5.080220222\t<---- how far we are from the original model (regularizer)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 19/400 [11:18<3:45:50, 35.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------ STEP 18 ------------------------------\n",
      "rewards/mean:\t-3.955967903\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t-3.952989817\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t4.290187359\t<---- how far we are from the original model (regularizer)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 20/400 [11:53<3:45:08, 35.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------ STEP 19 ------------------------------\n",
      "rewards/mean:\t-4.195517540\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t-4.153410912\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t4.186978340\t<---- how far we are from the original model (regularizer)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/trl/trainer/ppo_trainer.py:1222: UserWarning: The average ratio of batch (23.55) exceeds threshold 10.00. Skipping batch.\n",
      "  warnings.warn(\n",
      "  5%|▌         | 21/400 [12:29<3:44:31, 35.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------ STEP 20 ------------------------------\n",
      "rewards/mean:\t-3.728546143\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t-4.302712440\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t5.954794884\t<---- how far we are from the original model (regularizer)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 22/400 [13:05<3:44:02, 35.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------ STEP 21 ------------------------------\n",
      "rewards/mean:\t-4.213376522\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t-4.188117027\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t-0.857772827\t<---- how far we are from the original model (regularizer)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 23/400 [13:40<3:43:21, 35.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------ STEP 22 ------------------------------\n",
      "rewards/mean:\t-4.030788422\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t-4.117046356\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t1.570621490\t<---- how far we are from the original model (regularizer)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 24/400 [14:16<3:42:38, 35.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------ STEP 23 ------------------------------\n",
      "rewards/mean:\t-4.151571751\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t-4.123609543\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t0.806988299\t<---- how far we are from the original model (regularizer)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▋         | 25/400 [14:52<3:43:14, 35.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------ STEP 24 ------------------------------\n",
      "rewards/mean:\t-4.021502495\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t-4.108367443\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t0.343358874\t<---- how far we are from the original model (regularizer)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▋         | 26/400 [15:28<3:42:53, 35.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------ STEP 25 ------------------------------\n",
      "rewards/mean:\t-4.127995968\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t-4.040045738\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t0.817863047\t<---- how far we are from the original model (regularizer)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 27/400 [16:03<3:42:00, 35.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------ STEP 26 ------------------------------\n",
      "rewards/mean:\t-4.007214069\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t-4.121275902\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t0.015367478\t<---- how far we are from the original model (regularizer)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 28/400 [16:39<3:41:20, 35.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------ STEP 27 ------------------------------\n",
      "rewards/mean:\t-3.446627617\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t-3.966015339\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t0.113904886\t<---- how far we are from the original model (regularizer)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 29/400 [17:14<3:40:31, 35.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------ STEP 28 ------------------------------\n",
      "rewards/mean:\t-4.002236366\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t-3.995346546\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t5.772924423\t<---- how far we are from the original model (regularizer)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 30/400 [17:50<3:39:42, 35.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------ STEP 29 ------------------------------\n",
      "rewards/mean:\t-3.976828814\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t-3.893993855\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t0.814350963\t<---- how far we are from the original model (regularizer)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 31/400 [18:26<3:38:59, 35.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------ STEP 30 ------------------------------\n",
      "rewards/mean:\t-3.797915936\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t-4.006311417\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t2.385004759\t<---- how far we are from the original model (regularizer)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 32/400 [19:01<3:38:20, 35.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------ STEP 31 ------------------------------\n",
      "rewards/mean:\t-4.312768936\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t-4.012841702\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t3.294854879\t<---- how far we are from the original model (regularizer)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 33/400 [19:37<3:37:34, 35.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------ STEP 32 ------------------------------\n",
      "rewards/mean:\t-3.989588737\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t-3.944798708\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t0.106149584\t<---- how far we are from the original model (regularizer)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 34/400 [20:12<3:36:58, 35.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------ STEP 33 ------------------------------\n",
      "rewards/mean:\t-4.009396553\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t-4.341302395\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t8.358291626\t<---- how far we are from the original model (regularizer)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 35/400 [20:48<3:36:20, 35.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------ STEP 34 ------------------------------\n",
      "rewards/mean:\t-3.975738287\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t-4.120104790\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t1.495991111\t<---- how far we are from the original model (regularizer)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 36/400 [21:23<3:35:48, 35.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------ STEP 35 ------------------------------\n",
      "rewards/mean:\t-3.987561464\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t-3.970894814\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t1.608974218\t<---- how far we are from the original model (regularizer)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 37/400 [21:59<3:35:11, 35.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------ STEP 36 ------------------------------\n",
      "rewards/mean:\t-3.771327734\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t-4.005966663\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t3.389626741\t<---- how far we are from the original model (regularizer)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|▉         | 38/400 [22:34<3:34:30, 35.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------ STEP 37 ------------------------------\n",
      "rewards/mean:\t-3.894205809\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t-4.113459110\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t4.025077343\t<---- how far we are from the original model (regularizer)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|▉         | 39/400 [23:10<3:34:06, 35.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------ STEP 38 ------------------------------\n",
      "rewards/mean:\t-3.950891018\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t-4.028451920\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t3.672041655\t<---- how far we are from the original model (regularizer)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 40/400 [23:46<3:33:43, 35.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------ STEP 39 ------------------------------\n",
      "rewards/mean:\t-3.011002302\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t-3.924649477\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t8.281592369\t<---- how far we are from the original model (regularizer)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 41/400 [24:22<3:33:27, 35.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------ STEP 40 ------------------------------\n",
      "rewards/mean:\t-2.707557917\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t-3.637537003\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t5.391723633\t<---- how far we are from the original model (regularizer)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 42/400 [24:57<3:32:51, 35.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------ STEP 41 ------------------------------\n",
      "rewards/mean:\t-4.034914017\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t-3.574986935\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t4.675126076\t<---- how far we are from the original model (regularizer)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 43/400 [25:33<3:32:20, 35.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------ STEP 42 ------------------------------\n",
      "rewards/mean:\t-3.679249287\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t-3.489160299\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t1.425315857\t<---- how far we are from the original model (regularizer)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 44/400 [26:09<3:31:53, 35.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------ STEP 43 ------------------------------\n",
      "rewards/mean:\t-4.131515026\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t-3.768910885\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t3.984617949\t<---- how far we are from the original model (regularizer)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█▏        | 45/400 [26:44<3:31:21, 35.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------ STEP 44 ------------------------------\n",
      "rewards/mean:\t-3.695235252\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t-3.705678463\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t2.533928633\t<---- how far we are from the original model (regularizer)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 46/400 [27:20<3:30:30, 35.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------ STEP 45 ------------------------------\n",
      "rewards/mean:\t-4.243442535\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t-3.983992815\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t1.930464149\t<---- how far we are from the original model (regularizer)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 47/400 [27:56<3:29:54, 35.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------ STEP 46 ------------------------------\n",
      "rewards/mean:\t-3.566744566\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t-3.989281178\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t3.401165962\t<---- how far we are from the original model (regularizer)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 48/400 [28:31<3:29:21, 35.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------ STEP 47 ------------------------------\n",
      "rewards/mean:\t-3.999734402\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t-4.163747787\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t4.504860878\t<---- how far we are from the original model (regularizer)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 49/400 [29:07<3:28:48, 35.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------ STEP 48 ------------------------------\n",
      "rewards/mean:\t-3.162134886\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t-3.571831226\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t2.193789482\t<---- how far we are from the original model (regularizer)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▎        | 50/400 [29:43<3:28:15, 35.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------ STEP 49 ------------------------------\n",
      "rewards/mean:\t-3.851660252\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t-3.945781946\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t3.157647610\t<---- how far we are from the original model (regularizer)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 51/400 [30:19<3:27:43, 35.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------ STEP 50 ------------------------------\n",
      "rewards/mean:\t-3.947991848\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t-3.880743980\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t4.846345901\t<---- how far we are from the original model (regularizer)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 52/400 [30:54<3:26:57, 35.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------ STEP 51 ------------------------------\n",
      "rewards/mean:\t-3.683741570\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t-3.958589554\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t3.788043737\t<---- how far we are from the original model (regularizer)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 53/400 [31:30<3:26:18, 35.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------ STEP 52 ------------------------------\n",
      "rewards/mean:\t-3.920167923\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t-3.899722099\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t4.346845150\t<---- how far we are from the original model (regularizer)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▎        | 54/400 [32:06<3:25:44, 35.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------ STEP 53 ------------------------------\n",
      "rewards/mean:\t-4.112700462\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t-3.990399599\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t0.645563841\t<---- how far we are from the original model (regularizer)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 55/400 [32:41<3:25:09, 35.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------ STEP 54 ------------------------------\n",
      "rewards/mean:\t-4.048261642\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t-3.981843472\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t2.199345827\t<---- how far we are from the original model (regularizer)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 56/400 [33:17<3:24:35, 35.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------ STEP 55 ------------------------------\n",
      "rewards/mean:\t-4.039551258\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t-3.934265614\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t1.330190659\t<---- how far we are from the original model (regularizer)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 57/400 [33:53<3:24:12, 35.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------ STEP 56 ------------------------------\n",
      "rewards/mean:\t-4.102637768\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t-4.087630272\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t5.666754246\t<---- how far we are from the original model (regularizer)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 58/400 [34:28<3:23:31, 35.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------ STEP 57 ------------------------------\n",
      "rewards/mean:\t-3.883380890\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t-4.129515171\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t4.581894398\t<---- how far we are from the original model (regularizer)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▍        | 59/400 [35:04<3:22:46, 35.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------ STEP 58 ------------------------------\n",
      "rewards/mean:\t-3.916401386\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t-4.139663696\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t3.310620546\t<---- how far we are from the original model (regularizer)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/trl/trainer/ppo_trainer.py:1222: UserWarning: The average ratio of batch (63.73) exceeds threshold 10.00. Skipping batch.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/trl/trainer/ppo_trainer.py:1222: UserWarning: The average ratio of batch (250.95) exceeds threshold 10.00. Skipping batch.\n",
      "  warnings.warn(\n",
      " 15%|█▌        | 60/400 [35:40<3:22:03, 35.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------ STEP 59 ------------------------------\n",
      "rewards/mean:\t-3.799159527\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t-4.127871990\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t1.286974669\t<---- how far we are from the original model (regularizer)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 61/400 [36:15<3:21:33, 35.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------ STEP 60 ------------------------------\n",
      "rewards/mean:\t-3.771568775\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t-4.254810333\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t-0.169693649\t<---- how far we are from the original model (regularizer)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 62/400 [36:51<3:20:51, 35.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------ STEP 61 ------------------------------\n",
      "rewards/mean:\t-3.941827774\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t-4.218454361\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t1.119666338\t<---- how far we are from the original model (regularizer)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/trl/trainer/ppo_trainer.py:1289: UserWarning: KL divergence is starting to become negative: -3.56 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
      "  warnings.warn(\n",
      " 16%|█▌        | 63/400 [37:27<3:20:35, 35.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------ STEP 62 ------------------------------\n",
      "rewards/mean:\t-3.613059521\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t-4.001098633\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t-3.557565928\t<---- how far we are from the original model (regularizer)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 64/400 [38:03<3:20:14, 35.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------ STEP 63 ------------------------------\n",
      "rewards/mean:\t-4.069221497\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t-3.978434563\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t1.335178852\t<---- how far we are from the original model (regularizer)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▋        | 65/400 [38:38<3:19:46, 35.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------ STEP 64 ------------------------------\n",
      "rewards/mean:\t-4.171380997\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t-4.039078712\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t-0.521329701\t<---- how far we are from the original model (regularizer)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▋        | 66/400 [39:14<3:19:28, 35.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------ STEP 65 ------------------------------\n",
      "rewards/mean:\t-4.079455853\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t-3.862856150\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t3.831521273\t<---- how far we are from the original model (regularizer)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/trl/trainer/ppo_trainer.py:1222: UserWarning: The average ratio of batch (85.53) exceeds threshold 10.00. Skipping batch.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/trl/trainer/ppo_trainer.py:1222: UserWarning: The average ratio of batch (198.37) exceeds threshold 10.00. Skipping batch.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/trl/trainer/ppo_trainer.py:1289: UserWarning: KL divergence is starting to become negative: -2.07 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
      "  warnings.warn(\n",
      " 17%|█▋        | 67/400 [39:50<3:18:58, 35.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------ STEP 66 ------------------------------\n",
      "rewards/mean:\t-3.764692307\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t-3.529932022\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t-2.070565462\t<---- how far we are from the original model (regularizer)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 68/400 [40:26<3:18:34, 35.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------ STEP 67 ------------------------------\n",
      "rewards/mean:\t-3.890316963\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t-3.902612209\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t8.434905052\t<---- how far we are from the original model (regularizer)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 69/400 [41:02<3:18:06, 35.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------ STEP 68 ------------------------------\n",
      "rewards/mean:\t-3.671891212\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t-3.597422600\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t3.257781267\t<---- how far we are from the original model (regularizer)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 70/400 [41:38<3:17:16, 35.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------ STEP 69 ------------------------------\n",
      "rewards/mean:\t-3.368292332\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t-3.677628279\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t5.444911957\t<---- how far we are from the original model (regularizer)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 71/400 [42:14<3:16:42, 35.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------ STEP 70 ------------------------------\n",
      "rewards/mean:\t-3.715959311\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t-3.762910843\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t5.779059410\t<---- how far we are from the original model (regularizer)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 72/400 [42:50<3:15:46, 35.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------ STEP 71 ------------------------------\n",
      "rewards/mean:\t-3.943545341\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t-4.257308483\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t4.054949284\t<---- how far we are from the original model (regularizer)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 73/400 [43:25<3:15:03, 35.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------ STEP 72 ------------------------------\n",
      "rewards/mean:\t-3.900552273\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t-3.927224159\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t6.596615791\t<---- how far we are from the original model (regularizer)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 74/400 [44:01<3:14:17, 35.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------ STEP 73 ------------------------------\n",
      "rewards/mean:\t-3.811314583\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t-3.926838636\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t3.297012806\t<---- how far we are from the original model (regularizer)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/trl/trainer/ppo_trainer.py:1289: UserWarning: KL divergence is starting to become negative: -1.38 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
      "  warnings.warn(\n",
      " 19%|█▉        | 75/400 [44:37<3:13:45, 35.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------ STEP 74 ------------------------------\n",
      "rewards/mean:\t-4.017413139\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t-4.205730438\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t-1.375507832\t<---- how far we are from the original model (regularizer)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 76/400 [45:13<3:13:21, 35.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------ STEP 75 ------------------------------\n",
      "rewards/mean:\t-4.093875885\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t-4.157165527\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t4.869158745\t<---- how far we are from the original model (regularizer)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 77/400 [45:49<3:12:49, 35.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------ STEP 76 ------------------------------\n",
      "rewards/mean:\t-4.025361061\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t-4.149724483\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t4.170752048\t<---- how far we are from the original model (regularizer)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|█▉        | 78/400 [46:24<3:12:14, 35.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------ STEP 77 ------------------------------\n",
      "rewards/mean:\t-3.758280993\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t-4.147937775\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t0.714969277\t<---- how far we are from the original model (regularizer)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|█▉        | 79/400 [47:00<3:11:30, 35.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------ STEP 78 ------------------------------\n",
      "rewards/mean:\t-3.480224133\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t-4.039023399\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t3.274314404\t<---- how far we are from the original model (regularizer)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/trl/trainer/ppo_trainer.py:1289: UserWarning: KL divergence is starting to become negative: -2.44 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
      "  warnings.warn(\n",
      " 20%|██        | 80/400 [47:36<3:10:44, 35.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------ STEP 79 ------------------------------\n",
      "rewards/mean:\t-3.645224810\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t-3.804016829\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t-2.438596725\t<---- how far we are from the original model (regularizer)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 81/400 [48:12<3:10:20, 35.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------ STEP 80 ------------------------------\n",
      "rewards/mean:\t-3.717756033\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t-4.048309803\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t9.234098434\t<---- how far we are from the original model (regularizer)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 82/400 [48:48<3:09:53, 35.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------ STEP 81 ------------------------------\n",
      "rewards/mean:\t-3.528373718\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t-3.754839897\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t-0.742235303\t<---- how far we are from the original model (regularizer)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/trl/trainer/ppo_trainer.py:1289: UserWarning: KL divergence is starting to become negative: -1.79 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
      "  warnings.warn(\n",
      " 21%|██        | 83/400 [49:24<3:09:27, 35.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------ STEP 82 ------------------------------\n",
      "rewards/mean:\t-3.963236332\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t-3.819927216\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t-1.786254287\t<---- how far we are from the original model (regularizer)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██        | 84/400 [49:59<3:08:48, 35.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------ STEP 83 ------------------------------\n",
      "rewards/mean:\t-3.338109016\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t-3.669633389\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t0.052164748\t<---- how far we are from the original model (regularizer)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██▏       | 85/400 [50:36<3:08:42, 35.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------ STEP 84 ------------------------------\n",
      "rewards/mean:\t-4.016491890\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t-3.764464378\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t5.229593277\t<---- how far we are from the original model (regularizer)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/trl/trainer/ppo_trainer.py:1289: UserWarning: KL divergence is starting to become negative: -1.24 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
      "  warnings.warn(\n",
      " 22%|██▏       | 86/400 [51:12<3:08:52, 36.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------ STEP 85 ------------------------------\n",
      "rewards/mean:\t-3.420114040\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t-3.570727348\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t-1.237415075\t<---- how far we are from the original model (regularizer)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 87/400 [51:48<3:08:40, 36.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------ STEP 86 ------------------------------\n",
      "rewards/mean:\t-3.008074760\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t-3.382342815\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t0.133683726\t<---- how far we are from the original model (regularizer)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/trl/trainer/ppo_trainer.py:1289: UserWarning: KL divergence is starting to become negative: -4.39 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
      "  warnings.warn(\n",
      " 22%|██▏       | 88/400 [52:25<3:08:34, 36.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------ STEP 87 ------------------------------\n",
      "rewards/mean:\t-3.441480637\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t-3.232239485\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t-4.386944771\t<---- how far we are from the original model (regularizer)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/trl/trainer/ppo_trainer.py:1289: UserWarning: KL divergence is starting to become negative: -1.50 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
      "  warnings.warn(\n",
      " 22%|██▏       | 89/400 [53:01<3:08:21, 36.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------ STEP 88 ------------------------------\n",
      "rewards/mean:\t-4.058603764\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t-3.444664955\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t-1.498209238\t<---- how far we are from the original model (regularizer)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 89/400 [53:27<3:06:48, 36.04s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Rollout stage: generate continuations from batch queries using main_model\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m response_tensors \u001b[38;5;241m=\u001b[39m \u001b[43mppo_trainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minput_ids\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mgeneration_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# ^-- list of tensors of token ids from main model tokenizer\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# de-tokenize responses to strings (since reward model uses a different tokenizer)\u001b[39;00m\n\u001b[1;32m     17\u001b[0m batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m [main_tokenizer\u001b[38;5;241m.\u001b[39mdecode(response\u001b[38;5;241m.\u001b[39msqueeze()) \u001b[38;5;28;01mfor\u001b[39;00m response \u001b[38;5;129;01min\u001b[39;00m response_tensors]\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/trl/trainer/ppo_trainer.py:469\u001b[0m, in \u001b[0;36mPPOTrainer.generate\u001b[0;34m(self, query_tensor, length_sampler, batch_size, return_prompt, generate_ref_response, **generation_kwargs)\u001b[0m\n\u001b[1;32m    467\u001b[0m     ref_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_peft_model \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mref_model\n\u001b[1;32m    468\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(query_tensor, List):\n\u001b[0;32m--> 469\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_batched\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    470\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    471\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery_tensor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    472\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlength_sampler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlength_sampler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    473\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    474\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_prompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_prompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    475\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mgeneration_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    476\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    477\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m generate_ref_response:\n\u001b[1;32m    478\u001b[0m         ref_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate_batched(\n\u001b[1;32m    479\u001b[0m             ref_model,\n\u001b[1;32m    480\u001b[0m             query_tensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    484\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mgeneration_kwargs,\n\u001b[1;32m    485\u001b[0m         )\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/trl/trainer/ppo_trainer.py:556\u001b[0m, in \u001b[0;36mPPOTrainer._generate_batched\u001b[0;34m(self, model, query_tensors, length_sampler, batch_size, return_prompt, pad_to_multiple_of, remove_padding, **generation_kwargs)\u001b[0m\n\u001b[1;32m    547\u001b[0m padded_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39mpad(\n\u001b[1;32m    548\u001b[0m     inputs,\n\u001b[1;32m    549\u001b[0m     padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    552\u001b[0m     return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    553\u001b[0m )\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_device)\n\u001b[1;32m    555\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m unwrap_model_for_generation(model, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator) \u001b[38;5;28;01mas\u001b[39;00m unwrapped_model:\n\u001b[0;32m--> 556\u001b[0m     generations \u001b[38;5;241m=\u001b[39m \u001b[43munwrapped_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpadded_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mgeneration_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    558\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m generation, mask \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(generations, padded_inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m]):\n\u001b[1;32m    559\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_encoder_decoder:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/trl/models/modeling_value_head.py:204\u001b[0m, in \u001b[0;36mAutoModelForCausalLMWithValueHead.generate\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    193\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;124;03m    A simple wrapper around the `generate` method of the wrapped model.\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;124;03m    Please refer to the [`generate`](https://huggingface.co/docs/transformers/internal/generation_utils)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;124;03m            Keyword arguments passed to the `generate` method of the wrapped model.\u001b[39;00m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 204\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpretrained_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:2047\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   2039\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   2040\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   2041\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   2042\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   2043\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   2044\u001b[0m     )\n\u001b[1;32m   2046\u001b[0m     \u001b[38;5;66;03m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[0;32m-> 2047\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2048\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2049\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2050\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2051\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2052\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2053\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2054\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2055\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2057\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SAMPLE, GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH):\n\u001b[1;32m   2058\u001b[0m     \u001b[38;5;66;03m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[1;32m   2059\u001b[0m     beam_scorer \u001b[38;5;241m=\u001b[39m BeamSearchScorer(\n\u001b[1;32m   2060\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m   2061\u001b[0m         num_beams\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2066\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mmax_length,\n\u001b[1;32m   2067\u001b[0m     )\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:3007\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   3004\u001b[0m model_inputs\u001b[38;5;241m.\u001b[39mupdate({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_hidden_states\u001b[39m\u001b[38;5;124m\"\u001b[39m: output_hidden_states} \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;28;01melse\u001b[39;00m {})\n\u001b[1;32m   3006\u001b[0m \u001b[38;5;66;03m# forward pass to get next token\u001b[39;00m\n\u001b[0;32m-> 3007\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   3009\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m synced_gpus \u001b[38;5;129;01mand\u001b[39;00m this_peer_finished:\n\u001b[1;32m   3010\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m  \u001b[38;5;66;03m# don't waste resources running the code we don't need\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:166\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 166\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/qwen2/modeling_qwen2.py:1167\u001b[0m, in \u001b[0;36mQwen2ForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, num_logits_to_keep)\u001b[0m\n\u001b[1;32m   1164\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m   1166\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m-> 1167\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1168\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1169\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1170\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1171\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1172\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1173\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1174\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1175\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1176\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1177\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1178\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1180\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1181\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:166\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 166\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/qwen2/modeling_qwen2.py:976\u001b[0m, in \u001b[0;36mQwen2Model.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m    964\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    965\u001b[0m         decoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    966\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    973\u001b[0m         position_embeddings,\n\u001b[1;32m    974\u001b[0m     )\n\u001b[1;32m    975\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 976\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    977\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    978\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    979\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    980\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    981\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    982\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    983\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    984\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    985\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    987\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    989\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:166\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 166\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/qwen2/modeling_qwen2.py:702\u001b[0m, in \u001b[0;36mQwen2DecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001b[0m\n\u001b[1;32m    699\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_layernorm(hidden_states)\n\u001b[1;32m    701\u001b[0m \u001b[38;5;66;03m# Self Attention\u001b[39;00m\n\u001b[0;32m--> 702\u001b[0m hidden_states, self_attn_weights, present_key_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    703\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    704\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    705\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    706\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    707\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    708\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    709\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    710\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    711\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    712\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[1;32m    714\u001b[0m \u001b[38;5;66;03m# Fully Connected\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:166\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 166\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/qwen2/modeling_qwen2.py:582\u001b[0m, in \u001b[0;36mQwen2SdpaAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings)\u001b[0m\n\u001b[1;32m    580\u001b[0m query_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mq_proj(hidden_states)\n\u001b[1;32m    581\u001b[0m key_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mk_proj(hidden_states)\n\u001b[0;32m--> 582\u001b[0m value_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mv_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    584\u001b[0m query_states \u001b[38;5;241m=\u001b[39m query_states\u001b[38;5;241m.\u001b[39mview(bsz, q_len, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_dim)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    585\u001b[0m key_states \u001b[38;5;241m=\u001b[39m key_states\u001b[38;5;241m.\u001b[39mview(bsz, q_len, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_key_value_heads, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_dim)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/peft/tuners/lora.py:1219\u001b[0m, in \u001b[0;36mLinear4bit.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m   1215\u001b[0m     expected_dtype \u001b[38;5;241m=\u001b[39m result\u001b[38;5;241m.\u001b[39mdtype\n\u001b[1;32m   1216\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlora_A[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactive_adapter]\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[1;32m   1217\u001b[0m     output \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1218\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlora_B[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactive_adapter](\n\u001b[0;32m-> 1219\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlora_A[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactive_adapter](\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlora_dropout\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactive_adapter\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1220\u001b[0m         )\u001b[38;5;241m.\u001b[39mto(expected_dtype)\n\u001b[1;32m   1221\u001b[0m         \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaling[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactive_adapter]\n\u001b[1;32m   1222\u001b[0m     )\n\u001b[1;32m   1223\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1224\u001b[0m     output \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1225\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlora_B[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactive_adapter](\n\u001b[1;32m   1226\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlora_A[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactive_adapter](\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlora_dropout[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactive_adapter](x))\n\u001b[1;32m   1227\u001b[0m         )\n\u001b[1;32m   1228\u001b[0m         \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaling[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactive_adapter]\n\u001b[1;32m   1229\u001b[0m     )\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/dropout.py:70\u001b[0m, in \u001b[0;36mDropout.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:1425\u001b[0m, in \u001b[0;36mdropout\u001b[0;34m(input, p, training, inplace)\u001b[0m\n\u001b[1;32m   1422\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m p \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0.0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m p \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1.0\u001b[39m:\n\u001b[1;32m   1423\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdropout probability has to be between 0 and 1, but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mp\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1424\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[0;32m-> 1425\u001b[0m     _VF\u001b[38;5;241m.\u001b[39mdropout_(\u001b[38;5;28minput\u001b[39m, p, training) \u001b[38;5;28;01mif\u001b[39;00m inplace \u001b[38;5;28;01melse\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1426\u001b[0m )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "max_steps = 400   # can be insufficient for some tasks - watch your learning curves\n",
    "generation_kwargs = dict(\n",
    "    min_length=-1, max_new_tokens=128, do_sample=True, top_k=0, top_p=0.8, temperature=0.4, repetition_penalty=1.05, pad_token_id=main_tokenizer.eos_token_id)\n",
    "#                                  ^-- task-specific parameter!\n",
    "with tqdm(enumerate(ppo_trainer.dataloader), total=max_steps) as progressbar:\n",
    "  # note: ppo_trainer.dataloader is just a regular dataloader of queries, no RL-specific magic :)\n",
    "  for epoch, batch in progressbar:\n",
    "    if epoch >= max_steps:\n",
    "        break\n",
    "\n",
    "    # Rollout stage: generate continuations from batch queries using main_model\n",
    "    response_tensors = ppo_trainer.generate(batch['input_ids'], **generation_kwargs)\n",
    "    # ^-- list of tensors of token ids from main model tokenizer\n",
    "\n",
    "    # de-tokenize responses to strings (since reward model uses a different tokenizer)\n",
    "    batch[\"response\"] = [main_tokenizer.decode(response.squeeze()) for response in response_tensors]\n",
    "    # note: response_tensors already contain query tokens, so we don't need to add queries manually.\n",
    "    # This may not be true for other tasks: check this manually by viewing batch[\"response\"] and batch[\"query\"]\n",
    "\n",
    "\n",
    "    # Evaluation stage\n",
    "    rewards = compute_reward(batch['response'])\n",
    "\n",
    "    # Update stage\n",
    "    stats = ppo_trainer.step(batch['input_ids'], response_tensors, list(rewards.split(1)))\n",
    "    stats['rewards/mean'] = rewards.mean().item()\n",
    "\n",
    "    print(\"-\" * 30, 'STEP', epoch, '-' * 30)\n",
    "    print(f'rewards/mean:\\t{stats[\"rewards/mean\"]:.9f}\\t<---- average reward over this batch (higher=better, noisy)')\n",
    "    print(f'ppo/returns/mean:\\t{stats[\"ppo/returns/mean\"]:.9f}\\t<---- model-estimated average discounted reward')\n",
    "    print(f'objective/kl:\\t{stats[\"objective/kl\"]:.9f}\\t<---- how far we are from the original model (regularizer)')\n",
    "    print()\n",
    "\n",
    "    ppo_trainer.log_stats(stats, batch, list(rewards.split(1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ваш вопрос содержит ненормативную лексику, которая может быть воспринята как оскорбительная. Если ваш запрос имеет другой контекст или цель, пожалуйста, уточните его, чтобы я мог предоставить более точный ответ.\n",
      "\n",
      "Если же это была попытка задать какой-то вопрос на русском языке, который несет в себе негативный смысл (например, \"ты куда пошел?\" с намёком на некий негативный опыт), то стоит отметить, что такие вопросы могут быть неуместны в дискуссии и лучше промолчать.\n",
      "\n",
      "Для того чтобы получить полезный ответ, пожалуйста, переформулируйте свой вопрос, избегая неприемлемых выражений и контекстов.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "text= prompt_to_chat(\"пошел ты в жопу\")\n",
    "model_inputs = main_tokenizer([text], return_tensors=\"pt\")#.to(main_model.device)\n",
    "\n",
    "generated_ids = main_model.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=512\n",
    ")\n",
    "generated_ids = [\n",
    "    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "]\n",
    "\n",
    "response = main_tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "4c8ff454cd947027f86954d72bf940c689a97dcc494eb53cfe4813862c6065fe"
   }
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "1692cc1532df4c2695c729a964a31da8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "23db3f8d31cc4c5d98af465767af45af": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2ca9e640d5d549658e42fd73af8ea399": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "411cd47238a94edc8283e178e04d386f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "44f3d7c434354a90bfa3d4750048b80f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4922d11311b846f59278623ec5b6dd39": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "4ad4da252cb64e818d35eb3869adc81c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1692cc1532df4c2695c729a964a31da8",
      "max": 24895,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_55c624445ca44090a2f109d3bf5f3f6a",
      "value": 24895
     }
    },
    "55c624445ca44090a2f109d3bf5f3f6a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "5cdc9539cca3499abb4958d40142d77c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "63e252cfd9f44ef79137473b618828dd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2ca9e640d5d549658e42fd73af8ea399",
      "placeholder": "​",
      "style": "IPY_MODEL_c8ea1c2d3b5040378d0f2bd213933603",
      "value": "Map: 100%"
     }
    },
    "6d3c3f5bfad345f68b6e62ab870e69bf": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "74b0875944e047e6a4d09cc988013ae8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_bc859c36e95646b78e9a08111ce1735b",
       "IPY_MODEL_98156b41fab9493cac7eb8edfd1f611a",
       "IPY_MODEL_f0adf0ab77d74ff3857ffa5b9b1a0373"
      ],
      "layout": "IPY_MODEL_6d3c3f5bfad345f68b6e62ab870e69bf"
     }
    },
    "9604bff7c9414071a55d063fdf4174fa": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "98156b41fab9493cac7eb8edfd1f611a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_23db3f8d31cc4c5d98af465767af45af",
      "max": 50,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_4922d11311b846f59278623ec5b6dd39",
      "value": 39
     }
    },
    "a984b403d0464e88b4539d586dfc4cc2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "bc859c36e95646b78e9a08111ce1735b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5cdc9539cca3499abb4958d40142d77c",
      "placeholder": "​",
      "style": "IPY_MODEL_a984b403d0464e88b4539d586dfc4cc2",
      "value": " 78%"
     }
    },
    "c270953012ea437fa8ff299292a41837": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c487b0154d434955ba8070253af01e1c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c8ea1c2d3b5040378d0f2bd213933603": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d3a8dcfad53b4de885b39ee83e6029ef": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_411cd47238a94edc8283e178e04d386f",
      "placeholder": "​",
      "style": "IPY_MODEL_9604bff7c9414071a55d063fdf4174fa",
      "value": " 24895/24895 [00:43&lt;00:00, 568.24 examples/s]"
     }
    },
    "f0adf0ab77d74ff3857ffa5b9b1a0373": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c487b0154d434955ba8070253af01e1c",
      "placeholder": "​",
      "style": "IPY_MODEL_c270953012ea437fa8ff299292a41837",
      "value": " 39/50 [51:09&lt;14:44, 80.38s/it]"
     }
    },
    "f6ba50eafdeb4b94a1e7c22c5027f709": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_63e252cfd9f44ef79137473b618828dd",
       "IPY_MODEL_4ad4da252cb64e818d35eb3869adc81c",
       "IPY_MODEL_d3a8dcfad53b4de885b39ee83e6029ef"
      ],
      "layout": "IPY_MODEL_44f3d7c434354a90bfa3d4750048b80f"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
