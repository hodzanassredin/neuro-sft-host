{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e78fa01-6450-4c6a-940c-7a6bc56ac482",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install vllm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d3226e-33f3-4af9-875a-db9d54d2d531",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2287a2fd-0c21-4c33-a9e8-53d28bfd2913",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model_name = 'MTSAIR/Cotype-Nano'\n",
    "model_name = 'bb_ru_Cotype-Nano_20250123-100550'\n",
    "lora_adapter = f'/app/models/{model_name}'\n",
    "merged_name = f'{lora_adapter}-merged'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "87a6225a-843c-4ff6-a4ac-05feca5683e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "from vllm.lora.request import LoRARequest\n",
    "import torch\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bfe91beb-f778-4a33-ac62-e719fab894a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-01-25 11:56:22,052\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-25 11:56:29 config.py:510] This model supports multiple tasks: {'classify', 'generate', 'embed', 'score', 'reward'}. Defaulting to 'generate'.\n",
      "WARNING 01-25 11:56:29 config.py:588] bitsandbytes quantization is not fully optimized yet. The speed can be slower than non-quantized models.\n",
      "WARNING 01-25 11:56:29 config.py:2056] bitsandbytes quantization is not tested with LoRA yet.\n",
      "INFO 01-25 11:56:29 llm_engine.py:234] Initializing an LLM engine (v0.6.6.post1) with config: model='MTSAIR/Cotype-Nano', speculative_config=None, tokenizer='MTSAIR/Cotype-Nano', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=LoadFormat.BITSANDBYTES, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=bitsandbytes, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=MTSAIR/Cotype-Nano, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\"],\"candidate_compile_sizes\":[],\"compile_sizes\":[],\"capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=False, \n",
      "WARNING 01-25 11:56:30 interface.py:236] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.\n",
      "INFO 01-25 11:56:30 selector.py:120] Using Flash Attention backend.\n",
      "INFO 01-25 11:56:31 model_runner.py:1094] Starting to load model MTSAIR/Cotype-Nano...\n",
      "INFO 01-25 11:56:31 loader.py:1039] Loading weights with BitsAndBytes quantization.  May take a while ...\n",
      "INFO 01-25 11:56:31 weight_utils.py:251] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [01:17<00:00, 77.78s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [01:17<00:00, 77.78s/it]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-25 11:57:50 model_runner.py:1099] Loading model weights took 1.0966 GB\n",
      "INFO 01-25 11:57:50 punica_selector.py:11] Using PunicaWrapperGPU.\n",
      "INFO 01-25 11:57:54 worker.py:241] Memory profiling takes 4.34 seconds\n",
      "INFO 01-25 11:57:54 worker.py:241] the current vLLM instance can use total_gpu_memory (16.00GiB) x gpu_memory_utilization (0.90) = 14.40GiB\n",
      "INFO 01-25 11:57:54 worker.py:241] model weights take 1.10GiB; non_torch_memory takes 0.07GiB; PyTorch activation peak memory takes 2.14GiB; the rest of the memory reserved for KV Cache is 11.08GiB.\n",
      "INFO 01-25 11:57:54 gpu_executor.py:76] # GPU blocks: 25934, # CPU blocks: 9362\n",
      "INFO 01-25 11:57:54 gpu_executor.py:80] Maximum concurrency for 32768 tokens per request: 12.66x\n",
      "INFO 01-25 11:57:55 model_runner.py:1415] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:19<00:00,  1.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-25 11:58:14 model_runner.py:1535] Graph capturing finished in 19 secs, took 0.00 GiB\n",
      "INFO 01-25 11:58:14 llm_engine.py:431] init engine (profile, create kv cache, warmup model) took 24.00 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "llm = LLM(model=base_model_name, dtype=torch.bfloat16, trust_remote_code=True, quantization=\"bitsandbytes\", load_format=\"bitsandbytes\", enable_lora=True,qlora_adapter_name_or_path= lora_adapter)\n",
    "#llm = LLM(model=merged_name, dtype=torch.bfloat16, trust_remote_code=True)\n",
    "#llm = LLM(model=merged_name, dtype=torch.bfloat16, trust_remote_code=True, quantization=\"bitsandbytes\", load_format=\"bitsandbytes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "bc434c0d-3810-45d7-8bcc-08ed1f1310db",
   "metadata": {},
   "outputs": [],
   "source": [
    "system = '''\n",
    "Ты помощник для работы в системе BlackBox с использованием языка Component Pascal. Твоя задача информативно отвечать на вопросы. \n",
    "Ответ необходимо предоставить в формате markdown и выделять код символами ```. \n",
    "\n",
    "Пример: \n",
    "\n",
    "**Вопрос**: Как реализовать сортировку пузырьком?\n",
    "**Ответ**: \n",
    "\n",
    "Cортировку пузырьком можно реализовать с помощью следующего кода:\n",
    "\n",
    "```\n",
    "\tPROCEDURE BubbleSort*;  \n",
    "\t\tVAR i, j: INTEGER; x: Item;\n",
    "\tBEGIN\n",
    "\t\tFOR i := 1 TO n-1 DO\n",
    "\t\t\tFOR j := n-1 TO i BY -1 DO\n",
    "\t\t\t\tIF a[j-1] > a[j] THEN\n",
    "\t\t\t\t\tx := a[j-1];  a[j-1] := a[j];  a[j] := x\n",
    "\t\t\t\tEND\n",
    "\t\t\tEND\n",
    "\t\tEND\n",
    "\tEND BubbleSort;\n",
    "```\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5b0a5d4b-bf51-4452-ba3a-5cb2ebfd0886",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = transformers.AutoTokenizer.from_pretrained(base_model_name)\n",
    "def prompt_to_chat(prompt):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "    if system is not None:\n",
    "        messages.insert(0, {\"role\": \"system\", \"content\": prompt})\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d659e75-c668-49ef-ae92-cd06c4451a8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2531/3163274587.py:1: DeprecationWarning: The 'lora_local_path' attribute is deprecated and will be removed in a future version. Please use 'lora_path' instead.\n",
      "  lora_request = LoRARequest(\"lora-test-2\", 1, lora_adapter)\n"
     ]
    }
   ],
   "source": [
    "lora_request = LoRARequest(\"lora-test-2\", 1, lora_adapter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5ce6c139-3b58-4fbb-8aa2-6e2a0ce84835",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:05<00:00,  5.12s/it, est. speed input: 53.36 toks/s, output: 100.07 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Для начала нужно определиться, что такое процедура. Это набор инструкций, который выполняется последовательно. Процедуры могут иметь параметры (например, процедура чтения целого типа), а также возвращать результат выполнения (например, значение переменной). В нашем случае процедура будет содержать только цикл, который будет выполнять операцию перестановки элементов массива.\n",
      "\n",
      "Сначала напишем процедуру, которая будет принимать массив из пяти элементов, затем вызовем ее, передавая в качестве аргумента массив, содержащий числа от 1 до 5.\n",
      "После этого мы можем вызвать процедуру BubbleSort, чтобы увидеть, как она работает.\n",
      "\n",
      "Код процедуры вывода чисел от 1 до 5 в логи:\n",
      "\n",
      "```\n",
      "MODULE  LogTest;\n",
      "\n",
      "\tIMPORT  Log := StdLog,  In := i21sysIn;\n",
      "\n",
      "\tPROCEDURE Do*;\n",
      "\t\tVAR  i: INTEGER;\n",
      "\tBEGIN\n",
      "\t\tLog.Ln;\n",
      "\t\ti := 0;\n",
      "\t\tWHILE  i < 5  DO\n",
      "\t\t\tLog.Int( i );  Log.Chacter(\" \");  INC(i )\n",
      "\t\tEND;\n",
      "\t\tLog.Ln\n",
      "\tEND Do;\n",
      "\n",
      "BEGIN\n",
      "\tDo\n",
      "END LogTest.\n",
      "```\n",
      "\n",
      "Запустите эту процедуру, нажав Ctrl+D. Вы должны увидеть в логах следующее:\n",
      "```\n",
      "0 1 2 3 4\n",
      "```\n",
      "Это значит, что процедура успешно работала. Если в логах появится что-то другое, то это означает, что в процессе работы произошла ошибка. Вам следует найти причину этой ошибки и исправить её. После исправления процедуры снова запустите её, чтобы проверить, что все хорошо работает. Повторяйте эти шаги до тех пор, пока не получите корректный результат.\n",
      "\n",
      "Если после выполнения процедуры в логах появятся сообщения об ошибках, вам нужно будет их исправить. Обычно они содержатся в начале текста, за которым следует описание того, что было сделано во время выполнения процедуры. Например, если в логах появилось сообщение \"variable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Инференс\n",
    "sampling_params = SamplingParams(\n",
    "  min_p= 0.05,\n",
    "  repetition_penalty= 1.1,\n",
    "    frequency_penalty=0.0,\n",
    "  temperature= 0.3,\n",
    "  top_k= 30,\n",
    "  top_p= 0.3,\n",
    "    max_tokens=512)\n",
    "\n",
    "outputs = llm.generate(prompt_to_chat(\"Напиши процедуру вывода чисел от 1 до 5 в лог\"), sampling_params, lora_request = lora_request)\n",
    "print(outputs[0].outputs[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c7bead0b-b4be-444f-b9be-a1ccc0eeb09b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:05<00:00,  5.25s/it, est. speed input: 3.05 toks/s, output: 97.54 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Какие параметры будут доступны?\n",
      "\n",
      "Вот примеры на разных языках программирования:\n",
      "\n",
      "Для Компонентного Паскаля:\n",
      "MODULE ObxViews2;\n",
      "\tIMPORT Views, Ports, Stores, Models, Properties;\n",
      "\t\n",
      "\tTYPE View = POINTER TO RECORD (Models.Model) END;\n",
      "\t\n",
      "\tPROCEDURE (v: View) ConnectTo- (m: Models.Model), NEW;\n",
      "\tBEGIN\n",
      "\t\tv.ConnectTo(m)\n",
      "\tEND ConnectTo;\n",
      "\n",
      "\tPROCEDURE Deposit*;\n",
      "\t\tVAR v: View;\n",
      "\tBEGIN\n",
      "\t\tNEW(v); Stores.Join(StdInitDir.New(), v);\n",
      "\t\tViews.Deposit(v)\n",
      "\tEND Deposit;\n",
      "\n",
      "END ObxViews2.\n",
      "\n",
      "Для Блэкбокса:\n",
      "MODULE ObxViews2;\n",
      "\tIMPORT Dialog, Views, Ports, Stores, TextModels, TextViews, StdView;\n",
      "\t\n",
      "\tCONST minVersion = 0; maxVersion = 0;\n",
      "\t\n",
      "\tTYPE View = POINTER TO RECORD (TextViews.View) END;\n",
      "\t\n",
      "\tPROCEDURE (v: View) Internalize (VAR rd: Stores.Reader);\n",
      "\t\tVAR version: INTEGER;\n",
      "\tBEGIN\n",
      "\t\trd.ReadVersion(minVersion, maxVersion, version);\n",
      "\t\tWHILE ~rd.cancelled & (version >= 0) DO\n",
      "\t\t\trd.ReadInt(v.text);\n",
      "\t\t\tIF ~rd.cancelled THEN INC(version) END\n",
      "\t\tEND\n",
      "\tEND Internalize;\n",
      "\n",
      "\tPROCEDURE (v: View) Externalize (VAR wr: Stores.Writer);\n",
      "\tBEGIN\n",
      "\t\twr.WriteVersion(maxVersion);\n",
      "\t\twr.WriteString(v.text)\n",
      "\tEND Externalize;\n",
      "\n",
      "\tPROCEDURE (v: View) Restore (f: Views.Frame; l, t, r, b: INTEGER);\n",
      "\tBEGIN\n",
      "\t\tf.DrawRect(l, t, r, b, Ports.fill, Ports.red);\n",
      "\t\tf.DrawString(r - 16, t + 8, \"ObxViews2\", Fonts.dir.Default())\n",
      "\tEND Restore;\n",
      "\n",
      "\tPROCEDURE Deposit*;\n",
      "\t\tVAR v: View;\n",
      "\tBEGIN\n",
      "\t\tNEW(v); Stores.Join(StdInitDir.New(), v);\n",
      "\t\tViews.OpenAux(v, \"\")\n",
      "\tEND Deposit;\n",
      "\n",
      "END ObxViews2.\n",
      "\n",
      "\n",
      "Чтобы создать новый объект типа View, нужно выполнить команду меню Files -> New Document. \n",
      "Текстовый документ будет иметь вид как показано ниже:\n",
      "\n",
      "    ObxViews2\n",
      "\n",
      "Когда выделено в тексте фрагмент\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "outputs = llm.generate(\"Как написать процедуру для открытия текстовой view?\", sampling_params, lora_request = lora_request)#, lora_request = lora_request\n",
    "print(outputs[0].outputs[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e087d4d2-092c-4cf4-8443-20a72699f79d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66bb88f3-78e9-4a8d-91a8-17184eccf2cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
