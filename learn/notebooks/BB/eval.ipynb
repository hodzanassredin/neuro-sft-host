{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e196b1c-8a06-4882-8f01-8e1685ad19c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import transformers\n",
    "import peft\n",
    "import datasets\n",
    "import evaluate\n",
    "import time\n",
    "assert torch.cuda.is_available(), \"you need cuda for this part\"\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "53950b34-3c35-4a88-8e4b-39760d48c539",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt_to_chat(prompt, system = None):\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "    if system is not None:\n",
    "        messages.insert(0, {\"role\": \"system\", \"content\": prompt})\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    return text\n",
    "\n",
    "def infer(model, prompt, l=100, use_chat = True, temperature=0.4, top_p = 0.8, system = None):\n",
    "    if use_chat:\n",
    "        prompt = prompt_to_chat(prompt, system)\n",
    "    model_inputs = tokenizer([prompt], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    generated_ids = model.generate(\n",
    "        **model_inputs,\n",
    "        max_new_tokens=l,\n",
    "        #temperature=temperature, \n",
    "        #top_p=top_p,\n",
    "        do_sample=True ,  \n",
    "        repetition_penalty = 1.1,\n",
    "        temperature = 0.1,\n",
    "        top_p = 0.3,\n",
    "        top_k = 20,\n",
    "    )\n",
    "    generated_ids = [\n",
    "        output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "    ]\n",
    "    \n",
    "    response = tokenizer.batch_decode(generated_ids, skip_special_tokens=False)[0]\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a63fbe28-b792-40ee-a543-74fd1ca24fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_name = 'bb_ru_Cotype-Nano_20250123-100550'\n",
    "model_path = f'/app/models/{model_name}'\n",
    "base_model_name = 'MTSAIR/Cotype-Nano'\n",
    "USE_QUANT = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "931c5ab7-caa1-4bd4-9644-fb8fac8949f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = transformers.AutoTokenizer.from_pretrained(base_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d9cd400b-2ed1-4d6b-a221-cf149317ba32",
   "metadata": {},
   "outputs": [],
   "source": [
    "if USE_QUANT:\n",
    "    print(\"LOADING Q MODEL\")\n",
    "    bnb_config = transformers.BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16\n",
    "    )\n",
    "    base_model = transformers.AutoModelForCausalLM.from_pretrained(base_model_name, device_map=device,quantization_config=bnb_config,)\n",
    "else:\n",
    "    base_model  = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "        base_model_name,\n",
    "        low_cpu_mem_usage=True,\n",
    "        return_dict=True,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        device_map=\"auto\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fad0f993-cc6e-43fa-a1c2-bf0ee04f7e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = peft.PeftModel.from_pretrained(base_model, model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "785ac2f6-2591-4406-9f3e-f8328057c04b",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_model = model.merge_and_unload()\n",
    "#merged_model = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "191a34c0-a5e5-4491-922a-e47b76e8fef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "system = '''\n",
    "Ты помощник для работы в системе BlackBox с использованием языка Component Pascal. Твоя задача информативно отвечать на вопросы. \n",
    "Ответ необходимо предоставить в формате markdown и выделять код символами ```. \n",
    "\n",
    "Пример: \n",
    "\n",
    "**Вопрос**: Как реализовать сортировку пузырьком?\n",
    "**Ответ**: \n",
    "\n",
    "Cортировку пузырьком можно реализовать с помощью следующего кода:\n",
    "\n",
    "```\n",
    "\tPROCEDURE BubbleSort*;  \n",
    "\t\tVAR i, j: INTEGER; x: Item;\n",
    "\tBEGIN\n",
    "\t\tFOR i := 1 TO n-1 DO\n",
    "\t\t\tFOR j := n-1 TO i BY -1 DO\n",
    "\t\t\t\tIF a[j-1] > a[j] THEN\n",
    "\t\t\t\t\tx := a[j-1];  a[j-1] := a[j];  a[j] := x\n",
    "\t\t\t\tEND\n",
    "\t\t\tEND\n",
    "\t\tEND\n",
    "\tEND BubbleSort;\n",
    "```\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a863cfec-67a9-43a3-a59c-b0efdf88e873",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Для начала, важно понять, что такое Log. Это текстовый журнал, который используется для отображения сообщений, которые могут быть важны для разработки или обслуживания программы. В частности, Log может использоваться для вывода информации о том, как программа выполняется.\n",
      "\n",
      "Чтобы вывести массив в Log, вам нужно выполнить следующие шаги:\n",
      "\n",
      "1. Открыть модуль, в котором находится массив.\n",
      "2. Нажать Ctrl+H (или Shift+F9), чтобы перейти к его реализации.\n",
      "3. Найти процедуру, которая использует этот массив.\n",
      "4. Выбрать эту процедуру и нажать F5, чтобы запустить ее.\n",
      "5. После завершения процедуры откройте Log, щелкнув по кнопке Log в меню Dev.\n",
      "6. В Log будет показана информация о вызове процедуры.\n",
      "\n",
      "Пример: предположим, что у нас есть следующий код на языке BlackBox:\n",
      "\t\n",
      "\tPROCEDURE Do*;\n",
      "\tBEGIN\n",
      "\t\ti := 0; j := 0;\n",
      "\t\tWHILE i < 10 DO\n",
      "\t\t\tj := 0;\n",
      "\t\t\tWHILE j < 10 DO\n",
      "\t\t\t\tLog.Int(i); Log.Int(j);\n",
      "\t\t\t\tLog.String(\" \"); INC(j)\n",
      "\t\t\tEND\n",
      "\t\tEND\n",
      "\tEND Do;\n",
      "\n",
      "Если выделите процедуру Do и нажмете F5, то получите следующее в Log:\n",
      "\n",
      "\tDo\n",
      "\t0 0\n",
      "\t1 0\n",
      "\t2 0\n",
      "\t3 0\n",
      "\t4 0\n",
      "\t5 0\n",
      "\t6 0\n",
      "\t7 0\n",
      "\t8 0\n",
      "\t9 0\n",
      "\n",
      "Теперь вы можете изменить значение переменных i или j, чтобы увидеть другие значения в Log. Если вы хотите сохранить результаты работы процедуры в файл, то можно использовать команду File->Save As... в меню File. Замените имя файла на имя файла с расширением .log, например, myLog.log. Теперь вы сможете открыть этот файл, щелкнув по нему, и он будет содержать все результаты работы процедуры.\n",
      "\n",
      "Надеюсь, это поможет вам! Если у вас возникнут дополнительные вопросы, пожалуйста,\n"
     ]
    }
   ],
   "source": [
    "print(infer(merged_model, 'Как мне вывести массив в Log?', 512, use_chat = True, system = system))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed9fe84-4155-4d0c-abc8-37b5a2b564d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
