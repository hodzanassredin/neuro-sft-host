{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c552ef18-d476-4ac1-a932-fe39ae52e36d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Jan 21 12:59:07 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 565.57.02              Driver Version: 566.03         CUDA Version: 12.7     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 4060 Ti     On  |   00000000:01:00.0  On |                  N/A |\n",
      "|  0%   39C    P8              4W /  165W |    3949MiB /  16380MiB |      7%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "05e04ccb-e0e0-4c48-a9c8-efb88642a788",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import transformers\n",
    "import peft\n",
    "import datasets\n",
    "\n",
    "assert torch.cuda.is_available(), \"you need cuda for this part\"\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "6278fc09-ac11-4db4-9b35-f1191285d8d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "055ff20d-43e1-4f7d-8722-441da81abb19",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = \"/app/datasets/oberon/docs/bb_ru\"\n",
    "#model_name = \"Qwen/Qwen2.5-Coder-7B\"\n",
    "model_name = 'MTSAIR/Cotype-Nano'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "10faa754-675f-49fa-af89-9e2376c3b9fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "only_model_name = model_name.split(\"/\")[-1]\n",
    "dataset_name = dataset_path.split(\"/\")[-1]\n",
    "sft_model_path = f\"/app/models/{dataset_name}_{only_model_name}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "b1ee4c9c-0b68-4bd9-ad3a-b59b9bfad4cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.12/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "bnb_config = transformers.BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "peft_config = peft.LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\n",
    "        \"mlp.down_proj\",\n",
    "        \"self_attn.k_proj\",\n",
    "        \"self_attn.o_proj\",\n",
    "        \"mlp.up_proj\",\n",
    "        \"self_attn.v_proj\",\n",
    "        \"mlp.gate_proj\",\n",
    "        \"self_attn.q_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=peft.TaskType.CAUSAL_LM\n",
    ")\n",
    "training_args = transformers.TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    fp16=True,\n",
    "    gradient_accumulation_steps=4,\n",
    "    num_train_epochs=30,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=2,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "55c948fa-79c1-4c77-a689-78fb5151a9b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def subset_to_dataset(s):\n",
    "    return Dataset.from_dict({'text': [s[i]['text'] for i in range(len(s))]})\n",
    "        \n",
    "def dataset_load():\n",
    "    file_names = []\n",
    "    for subdir, dirs, files in os.walk(dataset_path):\n",
    "        for file in files:\n",
    "            file_names.append(os.path.join(subdir, file))\n",
    "    texts = []\n",
    "    for f in file_names:\n",
    "        with open(f, 'r', encoding='utf-8') as file:\n",
    "            texts.append(file.read())\n",
    "    dataset = Dataset.from_dict({'text': texts})\n",
    "\n",
    "    train_len = int(len(dataset)*0.8)\n",
    "    lengths = [train_len, len(dataset)-train_len]\n",
    "    train_dataset, test_dataset = torch.utils.data.random_split(dataset, lengths)\n",
    "    \n",
    "    return datasets.DatasetDict({\"train\":subset_to_dataset(train_dataset),\"test\":subset_to_dataset(test_dataset)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "8af39ba7-7178-494b-8476-32ececbcf199",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset_load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "75d354bf-19c5-4f76-a6e5-264fc7386e42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODULE ObxViews1;\n",
      "(**\n",
      "\tproject\t= \"BlackBox\"\n",
      "\torganization\t= \"www.oberon.ch\"\n",
      "\tcontributors\t= \"Oberon microsystems\"\n",
      "\tversion\t= \"System/Rsrc/About\"\n",
      "\tcopyright\t= \"System/Rsrc/About\"\n",
      "\tlicense\t= \"Docu/BB-License\"\n",
      "\tchanges\t= \"\n",
      "\t- YYYYMMDD, nn, ...\n",
      "\t\"\n",
      "\tissues\t= \"\n",
      "\t- ...\n",
      "\t\"\n",
      "\n",
      "**)\n",
      "\n",
      "\tIMPORT Views, Ports, Properties;\n",
      "\n",
      "\tTYPE View = POINTER TO RECORD (Views.View) END;\n",
      "\n",
      "\tPROCEDURE (v: View)  Restore (f: Views.Frame; l, t, r, b: INTEGER);\n",
      "\tBEGIN\n",
      "\t\tf.DrawRect(l, t, r, b, Ports.fill, Ports.red)\n",
      "\tEND Restore;\n",
      "\n",
      "\tPROCEDURE (v: View) HandlePropMsg (VAR msg: Properties.Message);\n",
      "\tBEGIN\n",
      "\t\tWITH msg: Properties.SizePref DO\n",
      "\t\t\tIF (msg.w = Views.undefined) OR (msg.h = Views.undefined) THEN\n",
      "\t\t\t\t msg.w := 20 * Ports.mm; msg.h := 10 * Ports.mm\n",
      "\t\t\tEND\n",
      "\t\tELSE\t(* ignore other messages *)\n",
      "\t\tEND\n",
      "\tEND HandlePropMsg;\n",
      "\n",
      "\tPROCEDURE Deposit*;\n",
      "\t\tVAR v: View;\n",
      "\tBEGIN\n",
      "\t\tNEW(v); Views.Deposit(v)\n",
      "\tEND Deposit;\n",
      "\n",
      "END ObxViews1.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(dataset[\"train\"][0]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "b8863282-81bf-4fa5-9226-91d673dcb9b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (277343 > 131072). Running this sequence through the model will result in indexing errors\n",
      "Map (num_proc=4): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 856/856 [00:02<00:00, 426.55 examples/s]\n",
      "Map (num_proc=4): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 215/215 [00:00<00:00, 467.39 examples/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['text'])\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True, num_proc=4, remove_columns=[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d35aac58-b45d-4ab0-98ac-80c1ece615ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "88575fcd-ea1b-40f4-ae9b-b128e66c6dbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map (num_proc=4): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 856/856 [00:01<00:00, 602.63 examples/s]\n",
      "Map (num_proc=4): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 215/215 [00:00<00:00, 718.66 examples/s]\n"
     ]
    }
   ],
   "source": [
    "block_size = 128\n",
    "def group_texts(examples):\n",
    "    # Concatenate all texts.\n",
    "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    # We drop the small remainder, we could add padding if the model supported it instead of this drop, you can\n",
    "        # customize this part to your needs.\n",
    "    total_length = (total_length // block_size) * block_size\n",
    "    # Split by chunks of max_len.\n",
    "    result = {\n",
    "        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result\n",
    "\n",
    "lm_datasets = tokenized_dataset.map(\n",
    "    group_texts,\n",
    "    batched=True,\n",
    "    batch_size=1000,\n",
    "    num_proc=4,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "c32575a6-319b-4b12-8d09-d6b4f124e3d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Restore (f: Views.Frame; l, t, r, b: INTEGER);\\n\\tBEGIN\\n\\t\\tf.DrawRect(l, t, r, b, Ports.fill, Ports.red)\\n\\tEND Restore;\\n\\n\\tPROCEDURE (v: View) HandlePropMsg (VAR msg: Properties.Message);\\n\\tBEGIN\\n\\t\\tWITH msg: Properties.SizePref DO\\n\\t\\t\\tIF (msg.w = Views.undefined) OR (msg.h = Views.undefined) THEN\\n\\t\\t\\t\\t msg.w := 20 * Ports.mm; msg.h := 10 * Ports.mm\\n\\t\\t\\tEND\\n\\t\\tELSE\\t(* ignore other messages *)\\n\\t'"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(lm_datasets[\"train\"][1][\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "06627dbd-c8f8-4b09-942d-58439e3cfcd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = transformers.AutoModelForCausalLM.from_pretrained(model_name, device_map=device,quantization_config=bnb_config,)\n",
    "model._hf_peft_config_loaded = True  # silence a warning from HF trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "e45a2f70-9a12-4316-a57b-7815bb33e07c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 18,464,768 || all params: 1,562,179,072 || trainable%: 1.1820\n"
     ]
    }
   ],
   "source": [
    "model = peft.get_peft_model(model, peft_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "efa722e4-6baa-4cdd-913e-23afa5159987",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_collator = transformers.DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "trainer = transformers.Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=lm_datasets[\"train\"],\n",
    "    eval_dataset=lm_datasets[\"test\"],\n",
    "    data_collator=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d30406-4ef7-458d-af63-69a5e6576979",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='18' max='35130' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [   18/35130 00:19 < 12:07:35, 0.80 it/s, Epoch 0.01/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bcf1c380-ca65-47f0-8ba2-86ca5b777572",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/app/models/bb_ru_cotype/tokenizer_config.json',\n",
       " '/app/models/bb_ru_cotype/special_tokens_map.json',\n",
       " '/app/models/bb_ru_cotype/vocab.json',\n",
       " '/app/models/bb_ru_cotype/merges.txt',\n",
       " '/app/models/bb_ru_cotype/added_tokens.json',\n",
       " '/app/models/bb_ru_cotype/tokenizer.json')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(sft_model_path)\n",
    "tokenizer.save_pretrained(sft_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d8aa123-881c-4af1-bf8a-f44fd6631057",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
