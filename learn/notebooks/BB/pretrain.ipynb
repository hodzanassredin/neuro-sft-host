{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c552ef18-d476-4ac1-a932-fe39ae52e36d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Jan 23 10:05:04 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 565.57.02              Driver Version: 566.03         CUDA Version: 12.7     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 4060 Ti     On  |   00000000:01:00.0  On |                  N/A |\n",
      "|  0%   39C    P8              5W /  165W |     593MiB /  16380MiB |     10%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "05e04ccb-e0e0-4c48-a9c8-efb88642a788",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import transformers\n",
    "import peft\n",
    "import datasets\n",
    "import evaluate\n",
    "import time\n",
    "assert torch.cuda.is_available(), \"you need cuda for this part\"\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6278fc09-ac11-4db4-9b35-f1191285d8d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "055ff20d-43e1-4f7d-8722-441da81abb19",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = \"/app/datasets/oberon/docs/bb_ru\"\n",
    "#model_name = \"Qwen/Qwen2.5-Coder-7B\"\n",
    "model_name = 'MTSAIR/Cotype-Nano'\n",
    "\n",
    "EPOCHS = 7\n",
    "PACKED=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "10faa754-675f-49fa-af89-9e2376c3b9fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "only_model_name = model_name.split(\"/\")[-1].replace(':', \"_\")\n",
    "dataset_name = dataset_path.split(\"/\")[-1]\n",
    "sft_model_path = f\"/app/models/{dataset_name}_{only_model_name}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b1ee4c9c-0b68-4bd9-ad3a-b59b9bfad4cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "bnb_config = transformers.BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "peft_config = peft.LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\n",
    "        \"mlp.down_proj\",\n",
    "        \"self_attn.k_proj\",\n",
    "        \"self_attn.o_proj\",\n",
    "        \"mlp.up_proj\",\n",
    "        \"self_attn.v_proj\",\n",
    "        \"mlp.gate_proj\",\n",
    "        \"self_attn.q_proj\"],\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=peft.TaskType.CAUSAL_LM\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "61beaedd-5d90-43a1-8b76-ffd9b9a8e6a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = transformers.AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "00042bcc-14f2-49c4-af7d-ae4a73113d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_texts():\n",
    "    file_names = [] \n",
    "    for subdir, dirs, files in os.walk(dataset_path):\n",
    "        if \"/Rsrc/\" in subdir:\n",
    "            continue\n",
    "        for file in files:\n",
    "            file_names.append(os.path.join(subdir, file))\n",
    "    texts = []\n",
    "    for f in file_names:\n",
    "        with open(f, 'r', encoding='utf-8') as file:\n",
    "            texts.append(file.read())\n",
    "    return texts\n",
    "texts = load_texts()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1bbb6d9f-e0c8-4da3-9f02-96c61be6feee",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_dataset = datasets.Dataset.from_dict({\"texts\":texts})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c530c95a-5ca0-49ac-a921-05a05a5c505a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 2/2 [00:00<00:00, 82.95ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:02<00:00,  2.11s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/hodza/Informatika21CP/commit/d8a9343597f441c4500b1e87a9513422a20b973e', commit_message='Upload dataset', commit_description='', oid='d8a9343597f441c4500b1e87a9513422a20b973e', pr_url=None, repo_url=RepoUrl('https://huggingface.co/datasets/hodza/Informatika21CP', endpoint='https://huggingface.co', repo_type='dataset', repo_id='hodza/Informatika21CP'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "text_dataset.push_to_hub('hodza/Informatika21CP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b02ab931-2403-411e-a760-99f39b7fa17a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'overflow_to_sample_mapping', 'labels'],\n",
       "        num_rows: 12160\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'overflow_to_sample_mapping', 'labels'],\n",
       "        num_rows: 1352\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block_size = 256  # Максимальная длина последовательности\n",
    "stride = 32      # Шаг перекрытия    \n",
    "def tokenize_with_stride(texts):\n",
    "    tokenized_data = tokenizer(\n",
    "        texts,\n",
    "        max_length=block_size,\n",
    "        truncation=True,\n",
    "        stride=stride,\n",
    "        return_overflowing_tokens=True,\n",
    "        padding=\"max_length\",  # Дополняем до block_size\n",
    "        return_tensors=\"pt\",   # Возвращаем тензоры PyTorch\n",
    "    )\n",
    "    tokenized_data[\"labels\"] = tokenized_data[\"input_ids\"]\n",
    "    return datasets.Dataset.from_dict(tokenized_data).train_test_split(test_size=0.1)\n",
    "    \n",
    "lm_datasets = tokenize_with_stride(texts)\n",
    "lm_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b8863282-81bf-4fa5-9226-91d673dcb9b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if False:# probaby need to remove as obsolete\n",
    "    print(\"USING PACKED DATASET\")\n",
    "    def tokenize_function(examples):\n",
    "        return tokenizer(examples['text'])\n",
    "    \n",
    "    tokenized_dataset = dataset.map(tokenize_function)\n",
    "    \n",
    "    def group_texts(examples):\n",
    "        # Concatenate all texts.\n",
    "        concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "        total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "        # We drop the small remainder, we could add padding if the model supported it instead of this drop, you can\n",
    "            # customize this part to your needs.\n",
    "        total_length = (total_length // block_size) * block_size\n",
    "        # Split by chunks of max_len.\n",
    "        result = {\n",
    "            k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
    "            for k, t in concatenated_examples.items()\n",
    "        }\n",
    "        result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "        return result\n",
    "    \n",
    "    lm_datasets = tokenized_dataset.map(\n",
    "        group_texts,\n",
    "        batched=True,\n",
    "        batch_size=1000,\n",
    "        num_proc=4,\n",
    "    )\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c32575a6-319b-4b12-8d09-d6b4f124e3d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "очую сессию (всего около 10 часов, включая либеральные перерывы на чай, ужин, чтение новостей и отслеживание текущих результатов матчей проходившего тогда теннисного турнира WTA) и сразу корректно заработала, не потребовав традиционной отладки. \n",
      "Единственная ошибка (случай, когда документ кончается до точки, закрывающей модуль) была найдена просто на свежую голову (на следующее утро) верификацией программного текста, то есть, опять же, без отладки.\n",
      "Разумеется, окончательный результат подвергся довольно обширному тестированию, но  это важно подчеркнуть  роль традиционной отладки как способа поиска и устранения ошибок была равна в точности нулю.\n",
      "Ф.В.Ткачев (info21)\n",
      "*)\n",
      "\u000e\t\t(* готовимся войти в цикл: *)\n",
      "\u000e\t\tpos := 0; (* пози\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(lm_datasets[\"train\"][99][\"input_ids\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "06627dbd-c8f8-4b09-942d-58439e3cfcd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = transformers.AutoModelForCausalLM.from_pretrained(model_name, device_map=device,quantization_config=bnb_config,)\n",
    "model._hf_peft_config_loaded = True  # silence a warning from HF trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e45a2f70-9a12-4316-a57b-7815bb33e07c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 18,464,768 || all params: 1,562,179,072 || trainable%: 1.1820\n"
     ]
    }
   ],
   "source": [
    "model = peft.get_peft_model(model, peft_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "93b6e086-af49-4f73-b5dd-c15772e989a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(level=logging.DEBUG, format='%(asctime)s %(message)s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b852f7f4-46a7-4f41-9e5e-3e8eabacb6e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-23 10:05:47,827 Starting new HTTPS connection (1): s3.amazonaws.com:443\n",
      "2025-01-23 10:05:48,440 https://s3.amazonaws.com:443 \"HEAD /datasets.huggingface.co/datasets/metrics/evaluate-metric/bleu/evaluate-metric/bleu.py HTTP/11\" 404 0\n",
      "2025-01-23 10:05:48,444 Starting new HTTPS connection (1): huggingface.co:443\n",
      "2025-01-23 10:05:48,761 https://huggingface.co:443 \"HEAD /spaces/evaluate-metric/bleu/resolve/v0.4.3/bleu.py HTTP/11\" 404 0\n",
      "2025-01-23 10:05:48,766 Starting new HTTPS connection (1): huggingface.co:443\n",
      "2025-01-23 10:05:49,254 https://huggingface.co:443 \"HEAD /spaces/evaluate-metric/bleu/resolve/main/bleu.py HTTP/11\" 200 0\n",
      "2025-01-23 10:05:49,275 Starting new HTTPS connection (1): github.com:443\n",
      "2025-01-23 10:05:49,697 https://github.com:443 \"HEAD /tensorflow/nmt/raw/master/nmt/scripts/bleu.py HTTP/11\" 302 0\n",
      "2025-01-23 10:05:49,698 Starting new HTTPS connection (1): raw.githubusercontent.com:443\n",
      "2025-01-23 10:05:50,006 https://raw.githubusercontent.com:443 \"HEAD /tensorflow/nmt/master/nmt/scripts/bleu.py HTTP/11\" 200 0\n",
      "2025-01-23 10:05:50,022 Starting new HTTPS connection (1): huggingface.co:443\n",
      "2025-01-23 10:05:50,539 https://huggingface.co:443 \"HEAD /spaces/evaluate-metric/bleu/resolve/main/tokenizer_13a.py HTTP/11\" 200 0\n",
      "2025-01-23 10:05:50,572 Attempting to acquire lock 140211569812608 on /hf/modules/evaluate_modules/metrics/evaluate-metric--bleu.lock\n",
      "2025-01-23 10:05:50,579 Lock 140211569812608 acquired on /hf/modules/evaluate_modules/metrics/evaluate-metric--bleu.lock\n",
      "2025-01-23 10:05:50,591 Attempting to release lock 140211569812608 on /hf/modules/evaluate_modules/metrics/evaluate-metric--bleu.lock\n",
      "2025-01-23 10:05:50,592 Lock 140211569812608 released on /hf/modules/evaluate_modules/metrics/evaluate-metric--bleu.lock\n"
     ]
    }
   ],
   "source": [
    "metric = evaluate.load(\"bleu\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "efa722e4-6baa-4cdd-913e-23afa5159987",
   "metadata": {},
   "outputs": [],
   "source": [
    "timestr = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "data_collator = transformers.DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "training_args = transformers.TrainingArguments(\n",
    "    output_dir=f\"./results/{dataset_name}_{only_model_name}/{timestr}/\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    bf16=torch.cuda.get_device_capability(torch.cuda.current_device())[0] >= 8,  \n",
    "    gradient_accumulation_steps=4,\n",
    "    num_train_epochs=EPOCHS,\n",
    "    weight_decay=0.01,\n",
    "    #save_total_limit=2,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=32,\n",
    "    warmup_steps=100, \n",
    "    #eval_steps=32, \n",
    "    #max_steps=512,\n",
    "    \n",
    "    #load_best_model_at_end=True,\n",
    ")\n",
    "trainer = transformers.Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=lm_datasets[\"train\"],\n",
    "    eval_dataset=lm_datasets[\"test\"],\n",
    "    data_collator=data_collator,\n",
    "    #compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "16d30406-4ef7-458d-af63-69a5e6576979",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5320' max='5320' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5320/5320 4:02:08, Epoch 7/7]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>6.514700</td>\n",
       "      <td>1.617451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>5.975800</td>\n",
       "      <td>1.504940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>5.720300</td>\n",
       "      <td>1.455948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>5.561500</td>\n",
       "      <td>1.427797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>5.343900</td>\n",
       "      <td>1.412708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>5.377700</td>\n",
       "      <td>1.404954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>5.251800</td>\n",
       "      <td>1.402332</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-23 10:38:40,378 Resetting dropped connection: huggingface.co\n",
      "2025-01-23 10:38:40,824 https://huggingface.co:443 \"HEAD /MTSAIR/Cotype-Nano/resolve/main/config.json HTTP/11\" 200 0\n",
      "2025-01-23 10:38:41,010 https://huggingface.co:443 \"HEAD /MTSAIR/Cotype-Nano/resolve/main/config.json HTTP/11\" 200 0\n",
      "2025-01-23 11:11:21,867 Resetting dropped connection: huggingface.co\n",
      "2025-01-23 11:11:22,209 https://huggingface.co:443 \"HEAD /MTSAIR/Cotype-Nano/resolve/main/config.json HTTP/11\" 200 0\n",
      "2025-01-23 11:11:22,371 https://huggingface.co:443 \"HEAD /MTSAIR/Cotype-Nano/resolve/main/config.json HTTP/11\" 200 0\n",
      "2025-01-23 11:44:02,101 Resetting dropped connection: huggingface.co\n",
      "2025-01-23 11:44:02,464 https://huggingface.co:443 \"HEAD /MTSAIR/Cotype-Nano/resolve/main/config.json HTTP/11\" 200 0\n",
      "2025-01-23 11:44:02,623 https://huggingface.co:443 \"HEAD /MTSAIR/Cotype-Nano/resolve/main/config.json HTTP/11\" 200 0\n",
      "2025-01-23 12:27:57,072 Resetting dropped connection: huggingface.co\n",
      "2025-01-23 12:27:57,503 https://huggingface.co:443 \"HEAD /MTSAIR/Cotype-Nano/resolve/main/config.json HTTP/11\" 200 0\n",
      "2025-01-23 12:27:57,683 https://huggingface.co:443 \"HEAD /MTSAIR/Cotype-Nano/resolve/main/config.json HTTP/11\" 200 0\n",
      "2025-01-23 13:01:30,963 Resetting dropped connection: huggingface.co\n",
      "2025-01-23 13:01:31,495 https://huggingface.co:443 \"HEAD /MTSAIR/Cotype-Nano/resolve/main/config.json HTTP/11\" 200 0\n",
      "2025-01-23 13:01:31,660 https://huggingface.co:443 \"HEAD /MTSAIR/Cotype-Nano/resolve/main/config.json HTTP/11\" 200 0\n",
      "2025-01-23 13:34:45,954 Resetting dropped connection: huggingface.co\n",
      "2025-01-23 13:34:46,417 https://huggingface.co:443 \"HEAD /MTSAIR/Cotype-Nano/resolve/main/config.json HTTP/11\" 200 0\n",
      "2025-01-23 13:34:46,603 https://huggingface.co:443 \"HEAD /MTSAIR/Cotype-Nano/resolve/main/config.json HTTP/11\" 200 0\n",
      "2025-01-23 14:06:20,711 Resetting dropped connection: huggingface.co\n",
      "2025-01-23 14:06:21,124 https://huggingface.co:443 \"HEAD /MTSAIR/Cotype-Nano/resolve/main/config.json HTTP/11\" 200 0\n",
      "2025-01-23 14:06:21,421 https://huggingface.co:443 \"HEAD /MTSAIR/Cotype-Nano/resolve/main/config.json HTTP/11\" 200 0\n",
      "2025-01-23 14:08:01,179 https://huggingface.co:443 \"HEAD /MTSAIR/Cotype-Nano/resolve/main/config.json HTTP/11\" 200 0\n",
      "2025-01-23 14:08:01,385 https://huggingface.co:443 \"HEAD /MTSAIR/Cotype-Nano/resolve/main/config.json HTTP/11\" 200 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=5320, training_loss=5.8286157357065305, metrics={'train_runtime': 14531.459, 'train_samples_per_second': 5.858, 'train_steps_per_second': 0.366, 'total_flos': 1.7373375529746432e+17, 'train_loss': 5.8286157357065305, 'epoch': 7.0})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()\n",
    "#trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "44e3bbe0-ea00-44d9-b532-48a5862bf6d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='513' max='513' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [513/513 01:52]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity: 5.12\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "eval_results = trainer.evaluate()\n",
    "print(f\"Perplexity: {math.exp(eval_results['eval_loss']):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bcf1c380-ca65-47f0-8ba2-86ca5b777572",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-23 14:16:24,142 Resetting dropped connection: huggingface.co\n",
      "2025-01-23 14:16:24,795 https://huggingface.co:443 \"HEAD /MTSAIR/Cotype-Nano/resolve/main/config.json HTTP/11\" 200 0\n",
      "2025-01-23 14:16:24,982 https://huggingface.co:443 \"HEAD /MTSAIR/Cotype-Nano/resolve/main/config.json HTTP/11\" 200 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('/app/models/bb_ru_Cotype-Nano_20250123-100550/tokenizer_config.json',\n",
       " '/app/models/bb_ru_Cotype-Nano_20250123-100550/special_tokens_map.json',\n",
       " '/app/models/bb_ru_Cotype-Nano_20250123-100550/vocab.json',\n",
       " '/app/models/bb_ru_Cotype-Nano_20250123-100550/merges.txt',\n",
       " '/app/models/bb_ru_Cotype-Nano_20250123-100550/added_tokens.json',\n",
       " '/app/models/bb_ru_Cotype-Nano_20250123-100550/tokenizer.json')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_path = sft_model_path + f\"_{timestr}\"\n",
    "model.save_pretrained(final_path)\n",
    "tokenizer.save_pretrained(final_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d6b8fd17-3cfa-4b8b-8277-083b69c57850",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/app/models/bb_ru_Cotype-Nano_20250123-100550'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8d8aa123-881c-4af1-bf8a-f44fd6631057",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-23 14:16:45,953 https://huggingface.co:443 \"HEAD /MTSAIR/Cotype-Nano/resolve/main/config.json HTTP/11\" 200 0\n",
      "2025-01-23 14:17:10,883 https://huggingface.co:443 \"HEAD /MTSAIR/Cotype-Nano/resolve/main/generation_config.json HTTP/11\" 200 0\n"
     ]
    }
   ],
   "source": [
    "reference_model = transformers.AutoModelForCausalLM.from_pretrained(model_name, device_map=device,quantization_config=bnb_config,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0226c3f9-1c41-4010-b00b-0fa0e125839f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML, display\n",
    "table_template = \"\"\"<table style=\"border:1px solid black\" >\n",
    "  <tr>\n",
    "    <th style=\"text-align: center; border:1px solid black\">PROMPT</th>\n",
    "    <th style=\"text-align: center; border:1px solid black\">BEFORE</th>\n",
    "    <th style=\"text-align: center; border:1px solid black\">AFTER</th>\n",
    "  </tr>\n",
    "{}\n",
    "</table>\"\"\"\n",
    "\n",
    "row_template = '''  <tr>\n",
    "    <td style=\"width:20%; border:1px solid black\"><pre align=\"left\">`{}`</pre></td>\n",
    "    <td style=\"width:40%; border:1px solid black\"><pre align=\"left\">{}</pre></td>\n",
    "    <td style=\"width:40%; border:1px solid black\"><pre align=\"left\">{}</pre></td>\n",
    "  </tr>'''\n",
    "\n",
    "def prompt_to_chat(prompt):\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    return text\n",
    "\n",
    "def infer(model, prompt, l=100, use_chat = True, temperature=0.4, top_p = 0.8):\n",
    "    if use_chat:\n",
    "        prompt = prompt_to_chat(prompt)\n",
    "    model_inputs = tokenizer([prompt], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    generated_ids = model.generate(\n",
    "        **model_inputs,\n",
    "        max_new_tokens=l,\n",
    "        temperature=temperature, \n",
    "        top_p=top_p,\n",
    "        do_sample=True ,  \n",
    "    )\n",
    "    generated_ids = [\n",
    "        output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "    ]\n",
    "    \n",
    "    response = tokenizer.batch_decode(generated_ids, skip_special_tokens=False)[0]\n",
    "    return response\n",
    "\n",
    "\n",
    "prompts =  [\n",
    "    'Как в BlackBox можно вывести данные на консоль?', \n",
    "    'Какой тип данных в Component Pascal используется для хранения целых чисел?', \n",
    "    'МОДУЛЬ i21егэDemo2010C4ru;', \n",
    "    'Component Pascal is Oberon microsystems refinement of?', \n",
    "    'Log.String(', \n",
    "    'Типом целой константы является'\n",
    "]  # feel free to add a few more that are not 100% assiciated with Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c5d4b9ec-9c96-4b55-b7d2-b4a1d8e1ddcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_diff(use_chat,l):\n",
    "    print(f\"USING CHAT TEMPLATE = {use_chat}\")\n",
    "        \n",
    "    baseline = [infer(reference_model, p, l = l, use_chat=use_chat) for p in prompts]\n",
    "    check = [infer(model, p, l = l, use_chat=use_chat) for p in prompts]\n",
    "    rows = []\n",
    "    for i, prompt in enumerate(prompts):\n",
    "        # replace placeholders in the format() arguments\n",
    "        rows.append(row_template.format(prompt, baseline[i], check[i]))\n",
    "    display(HTML(table_template.format('\\n'.join(rows))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d62e1c1d-9778-4fe4-85dd-05053174412e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USING CHAT TEMPLATE = True\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table style=\"border:1px solid black\" >\n",
       "  <tr>\n",
       "    <th style=\"text-align: center; border:1px solid black\">PROMPT</th>\n",
       "    <th style=\"text-align: center; border:1px solid black\">BEFORE</th>\n",
       "    <th style=\"text-align: center; border:1px solid black\">AFTER</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td style=\"width:20%; border:1px solid black\"><pre align=\"left\">`Как в BlackBox можно вывести данные на консоль?`</pre></td>\n",
       "    <td style=\"width:40%; border:1px solid black\"><pre align=\"left\">В BlackBox, который является инструментом для тестирования интерфейсов с использованием браузера и эмулятора устройств, вывод данных на кон</pre></td>\n",
       "    <td style=\"width:40%; border:1px solid black\"><pre align=\"left\">Чтобы вывести данные на консоль в BlackBox, используйте процедуры StdLog. Например, чтобы вывести строку \"Hello World\", выполните следующее</pre></td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td style=\"width:20%; border:1px solid black\"><pre align=\"left\">`Какой тип данных в Component Pascal используется для хранения целых чисел?`</pre></td>\n",
       "    <td style=\"width:40%; border:1px solid black\"><pre align=\"left\">В Pascal, как и во многих других языках программирования, для хранения целых чисел обычно используется тип данных `integer`. Этот тип поддерживает ноль, одно,</pre></td>\n",
       "    <td style=\"width:40%; border:1px solid black\"><pre align=\"left\">В языке программирования Component Pascal целые числа могут быть представлены с использованием различных типов. Например, 32-битные целые числа могут быть представлен</pre></td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td style=\"width:20%; border:1px solid black\"><pre align=\"left\">`МОДУЛЬ i21егэDemo2010C4ru;`</pre></td>\n",
       "    <td style=\"width:40%; border:1px solid black\"><pre align=\"left\">Ваш запрос \"MODUль i21egэDemo2010C4ru\" кажется неполным или содержит опечатки. Возможно, вы имели в</pre></td>\n",
       "    <td style=\"width:40%; border:1px solid black\"><pre align=\"left\">Этот модуль предназначен для демонстрации работы с модулем i21eduЧерепашка. Он не является обязательным к использованию, но может быть пол</pre></td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td style=\"width:20%; border:1px solid black\"><pre align=\"left\">`Component Pascal is Oberon microsystems refinement of?`</pre></td>\n",
       "    <td style=\"width:40%; border:1px solid black\"><pre align=\"left\">Oberon-MicroSystems is a company that specializes in the development of microsystem components for industrial applications. It appears there might be some confusion, as \"Pascal\" could refer to different</pre></td>\n",
       "    <td style=\"width:40%; border:1px solid black\"><pre align=\"left\">Oberon Microsystems, Inc. was a small company that specialized in the development of software for microcomputers. It was founded by Wirth and his colleagues at ETH Zurich (Eid</pre></td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td style=\"width:20%; border:1px solid black\"><pre align=\"left\">`Log.String(`</pre></td>\n",
       "    <td style=\"width:40%; border:1px solid black\"><pre align=\"left\">It seems like you're asking about the `String` method in a logging framework, but your question is incomplete. The `String` method typically refers to converting an object to a string representation in</pre></td>\n",
       "    <td style=\"width:40%; border:1px solid black\"><pre align=\"left\">String(\"log\"); Log.Ln\n",
       "\n",
       "String(\"log\") + Log.Ln\n",
       "\n",
       "String(\"log\") + String(\"log\") + Log.Ln\n",
       "\n",
       "String(\"log\") + String(\"log</pre></td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td style=\"width:20%; border:1px solid black\"><pre align=\"left\">`Типом целой константы является`</pre></td>\n",
       "    <td style=\"width:40%; border:1px solid black\"><pre align=\"left\">Вопрос о \"типом целой константы\" может быть немного неоднозначным, так как термин \"константа\" в контексте программирования обычно</pre></td>\n",
       "    <td style=\"width:40%; border:1px solid black\"><pre align=\"left\">тип INTEGER. Константа может быть объявлена как CONSTANT, например:\n",
       "\n",
       "\tCONST a = 10;\n",
       "\n",
       "Константам назначаются значения, которые не могут измен</pre></td>\n",
       "  </tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "draw_diff(True, 39)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2e557ba3-4ec0-40d7-9427-4728d44a4f87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USING CHAT TEMPLATE = False\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table style=\"border:1px solid black\" >\n",
       "  <tr>\n",
       "    <th style=\"text-align: center; border:1px solid black\">PROMPT</th>\n",
       "    <th style=\"text-align: center; border:1px solid black\">BEFORE</th>\n",
       "    <th style=\"text-align: center; border:1px solid black\">AFTER</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td style=\"width:20%; border:1px solid black\"><pre align=\"left\">`Как в BlackBox можно вывести данные на консоль?`</pre></td>\n",
       "    <td style=\"width:40%; border:1px solid black\"><pre align=\"left\"> In the BlackBox framework, how can I output data to the console?\n",
       "\n",
       "To achieve this in BlackBox, you can use the `ConsoleLogger` class which provides a convenient way to log messages</pre></td>\n",
       "    <td style=\"width:40%; border:1px solid black\"><pre align=\"left\"> Как это делается?\n",
       "В каких случаях не следует использовать консоль для вывода данных?\n",
       "\n",
       "Команды, которые позволяют работать с данными в консоли, наход</pre></td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td style=\"width:20%; border:1px solid black\"><pre align=\"left\">`Какой тип данных в Component Pascal используется для хранения целых чисел?`</pre></td>\n",
       "    <td style=\"width:40%; border:1px solid black\"><pre align=\"left\"> В компоненте Pascal, для хранения целых чисел обычно используется тип `Int`. Этот тип представляет собой 32-битное беззнаковое целое число. Если</pre></td>\n",
       "    <td style=\"width:40%; border:1px solid black\"><pre align=\"left\"> Это 32-битное или 64-битное целое?\n",
       "\n",
       "В языке программирования Компонентный Паскаль целые числа могут быть представлены</pre></td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td style=\"width:20%; border:1px solid black\"><pre align=\"left\">`МОДУЛЬ i21егэDemo2010C4ru;`</pre></td>\n",
       "    <td style=\"width:40%; border:1px solid black\"><pre align=\"left\">2010C4ru;2010C4ru;2010C4ru;2010C4ru;2010C4ru</pre></td>\n",
       "    <td style=\"width:40%; border:1px solid black\"><pre align=\"left\">  (* info21, 2010-03-18 *)\n",
       "\n",
       "\tПОДКЛЮЧИТЬ  Вывод := i21eduВыв</pre></td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td style=\"width:20%; border:1px solid black\"><pre align=\"left\">`Component Pascal is Oberon microsystems refinement of?`</pre></td>\n",
       "    <td style=\"width:40%; border:1px solid black\"><pre align=\"left\"> \n",
       "Oberon is a high-level, multi-paradigm object-oriented programming language. It was developed by the National Research Council of Canada (NRC) and has been used for various applications</pre></td>\n",
       "    <td style=\"width:40%; border:1px solid black\"><pre align=\"left\"> 1993.\n",
       "\n",
       "The following table gives a brief overview of the differences between Component Pascal and Oberon. The differences are marked with an asterisk (*).\n",
       "\n",
       " Oberon Component Pascal\n",
       "Object</pre></td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td style=\"width:20%; border:1px solid black\"><pre align=\"left\">`Log.String(`</pre></td>\n",
       "    <td style=\"width:40%; border:1px solid black\"><pre align=\"left\">1024)\n",
       "    .SetMaxSize(stringLen + 5) // Set the maximum size to the sum of the current length and a buffer size\n",
       "    .Build();\n",
       "\n",
       "```\n",
       "\n",
       "</pre></td>\n",
       "    <td style=\"width:40%; border:1px solid black\"><pre align=\"left\"> s );\n",
       "\t\tLog.Ln;\n",
       "\tEND Do;\n",
       "\n",
       "END ObxViews15.\n",
       "\n",
       "Листинг 1-15. Объявление и использование процедуры\n",
       "\n",
       "В этом пример</pre></td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td style=\"width:20%; border:1px solid black\"><pre align=\"left\">`Типом целой константы является`</pre></td>\n",
       "    <td style=\"width:40%; border:1px solid black\"><pre align=\"left\"> число 0. Which is correct?\n",
       "The statement \"The type of the whole number 0 is a constant\" is not entirely accurate because while 0 is indeed a whole number, it does</pre></td>\n",
       "    <td style=\"width:40%; border:1px solid black\"><pre align=\"left\"> INTEGER. Стандартные типы, которые являются дочерними к INTEGER, это BYTE, SHORTINT, LONGINT и REAL. Другие типы могут быть объ</pre></td>\n",
       "  </tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "draw_diff(False, 39)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3c970689-2e87-4b49-bee9-55978ee178b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Для вывода строки в окно лога нужно использовать процедуру StdLog.String, которая принимает параметр типа String и помещает его содержимое в окно лога. Следующий пример демонстрирует эту процедуру:\n",
      "\n",
      "MODULE  i21примЛог;\n",
      "\n",
      "\tIMPORT  Log := StdLog;\n",
      "\t\n",
      "\tPROCEDURE Показать* ( s: ARRAY OF CHAR );\n",
      "\tBEGIN\n",
      "\t\tLog.String( s ); (* или просто Log.out *)\n",
      "\tEND Показать;\n",
      "\t\n",
      "END  i21примЛог.\n",
      "\n",
      "Выводится следующее:\n",
      "\ti21примЛог.Показать(\"hello\")\n",
      "\n",
      "См. также модуль Log для более подробной информации о работе с логом.\n",
      "\u000e\u000e\u000e\u000e\u000e\u000e\u000e\u000e\u000e\u000e\u000e\u000e\u000e\u000e\u000e\u000e\u000e\u000e\u000e\u000e\u000e\u000e\u000e\u000e\u000e\u000e\u000e\u000e\u000e\u000e\u000e\u000e\u000e\u000e\u000e\u000e\u000e\u000e\u000e\u000e\u000e\u000e\u000e\u000e\u000e\u000e\u000e\u000e\u000e\u000e\u000e\u000e\u000e\u000e\u000e\u000e\u000e\u000e\u000e\u000e\u000e\u000e\u000e\u000e\u000e\u000e\u000e\u000e\u000e\u000e\u000e\u000e\u000e\u000e\u000e\u000e\u000e\u000e\u000e\u000e\u000e\u000e\u000e\u000e\u000e\u000e\u000e\u000e\u000e\u000e\u000e\u000e\n"
     ]
    }
   ],
   "source": [
    "print(infer(model, 'Ты помошник по среде Component Pascal. Ответь на вопрос: Как мне вывести строку hello в окно лога?', 256, use_chat = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "828dbb24-08bc-4ed3-9d90-365a0a5b4144",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = transformers.AutoModelForCausalLM.from_pretrained(\"./results/20250122-220441/checkpoint-1220/\", device_map=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "893ee176-9274-43dd-b577-db944a851289",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Для целых чисел обычно используют тип INTEGER. Однако, если значения могут быть только некоторым предопределенным количеством значений, следует использовать тип SHORTINT, чтобы избежать возможных потерь точности, которые могут возникнуть при делении на 2. В некоторых случаях лучше использовать LONGINT.\n",
      "\n",
      "Все последующие примеры использует LONGINT в качестве основного типа. Если результаты вычислений становятся непредсказуемыми, если используется другой тип.\n",
      "\n",
      "Пример:\n",
      "\t\n",
      "\tPROCEDURE Ddouble* (a, b: LONGINT);\n",
      "\tBEGIN\n",
      "\t\tDIAGNOSTIC;\n",
      "\t\tStdLog.Nl;\n",
      "\t\tStdLog.String(\"Ddouble \"); StdLog.Int(a + b, 0); StdLog.Ln;\n",
      "\t\tStdLog.String(DefaCT(a + b)); StdLog.Ln\n",
      "\tEND Ddouble;\n",
      "\n",
      "В этом примере использовано значение эффекта (defacto) стандартного модуля Math, который включает в себя итоговое значение.\n",
      "\u000eПосле применения команд DIAGNOSTIC (см. ниже), он будет выровнен по левому краю, а не\n"
     ]
    }
   ],
   "source": [
    "print(infer(checkpoint, 'Какой тип использовать для хранения целых чисел?', 256, use_chat = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee14080-d26e-4164-9080-a06910d3b6e1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
