{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e196b1c-8a06-4882-8f01-8e1685ad19c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import transformers\n",
    "import peft\n",
    "import datasets\n",
    "import evaluate\n",
    "import time\n",
    "assert torch.cuda.is_available(), \"you need cuda for this part\"\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "53950b34-3c35-4a88-8e4b-39760d48c539",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt_to_chat(prompt, system = None):\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "    if system is not None:\n",
    "        messages.insert(0, {\"role\": \"system\", \"content\": prompt})\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    return text\n",
    "\n",
    "def infer(model, prompt, l=100, use_chat = True, temperature=0.4, top_p = 0.8, system = None):\n",
    "    if use_chat:\n",
    "        prompt = prompt_to_chat(prompt, system)\n",
    "    model_inputs = tokenizer([prompt], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    generated_ids = model.generate(\n",
    "        **model_inputs,\n",
    "        max_new_tokens=l,\n",
    "        #temperature=temperature, \n",
    "        #top_p=top_p,\n",
    "        do_sample=True ,  \n",
    "        repetition_penalty = 1.1,\n",
    "        temperature = 0.1,\n",
    "        top_p = 0.3,\n",
    "        top_k = 20,\n",
    "    )\n",
    "    generated_ids = [\n",
    "        output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "    ]\n",
    "    \n",
    "    response = tokenizer.batch_decode(generated_ids, skip_special_tokens=False)[0]\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a63fbe28-b792-40ee-a543-74fd1ca24fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_name = 'bb_ru_Qwen2.5-Coder-3B-Instruct_20250125-212602'\n",
    "model_path = f'/app/models/{model_name}'\n",
    "base_model_name = 'Qwen/Qwen2.5-Coder-3B-Instruct'\n",
    "USE_QUANT = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "931c5ab7-caa1-4bd4-9644-fb8fac8949f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = transformers.AutoTokenizer.from_pretrained(base_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d9cd400b-2ed1-4d6b-a221-cf149317ba32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOADING Q MODEL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:44<00:00, 22.39s/it]\n"
     ]
    }
   ],
   "source": [
    "if USE_QUANT:\n",
    "    print(\"LOADING Q MODEL\")\n",
    "    bnb_config = transformers.BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16\n",
    "    )\n",
    "    base_model = transformers.AutoModelForCausalLM.from_pretrained(base_model_name, device_map=device,quantization_config=bnb_config,)\n",
    "else:\n",
    "    base_model  = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "        base_model_name,\n",
    "        low_cpu_mem_usage=True,\n",
    "        return_dict=True,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        device_map=\"auto\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe8b1ad8-b196-4f42-9b41-8a3055dfde45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fad0f993-cc6e-43fa-a1c2-bf0ee04f7e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = peft.PeftModel.from_pretrained(base_model, model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "56c47720-3045-408b-af39-a7da7d5dc8f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "adapter_model.safetensors: 100%|██████████| 120M/120M [00:07<00:00, 15.1MB/s] \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/hodza/BBCopilot/commit/c9ffca07547d98b71d93e16f681c60004fa9e951', commit_message='Upload model', commit_description='', oid='c9ffca07547d98b71d93e16f681c60004fa9e951', pr_url=None, repo_url=RepoUrl('https://huggingface.co/hodza/BBCopilot', endpoint='https://huggingface.co', repo_type='model', repo_id='hodza/BBCopilot'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.push_to_hub(\"hodza/BlackBox-Coder-3B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "191a34c0-a5e5-4491-922a-e47b76e8fef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "system = '''\n",
    "Ты помощник для работы в системе BlackBox с использованием языка Component Pascal. Твоя задача информативно отвечать на вопросы. \n",
    "Ответ необходимо предоставить в формате markdown и выделять код символами ```. \n",
    "\n",
    "Пример: \n",
    "\n",
    "**Вопрос**: Как реализовать сортировку пузырьком?\n",
    "**Ответ**: \n",
    "\n",
    "Cортировку пузырьком можно реализовать с помощью следующего кода:\n",
    "\n",
    "```\n",
    "\tPROCEDURE BubbleSort*;  \n",
    "\t\tVAR i, j: INTEGER; x: Item;\n",
    "\tBEGIN\n",
    "\t\tFOR i := 1 TO n-1 DO\n",
    "\t\t\tFOR j := n-1 TO i BY -1 DO\n",
    "\t\t\t\tIF a[j-1] > a[j] THEN\n",
    "\t\t\t\t\tx := a[j-1];  a[j-1] := a[j];  a[j] := x\n",
    "\t\t\t\tEND\n",
    "\t\t\tEND\n",
    "\t\tEND\n",
    "\tEND BubbleSort;\n",
    "```\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a863cfec-67a9-43a3-a59c-b0efdf88e873",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Чтобы вывести элементы массива, достаточно использовать цикл FOR. Например:\n",
      "\n",
      "VAR a: ARRAY 10 OF INTEGER;\n",
      "BEGIN\n",
      "\tFOR i := 0 TO LEN(a) - 1 DO\n",
      "\t\tLog.Int( a[i] )\n",
      "\tEND;\n",
      "\n",
      "Если нужно вывести несколько значений, то можно использовать процедуру Log.Ln (Ln = line number).<|im_end|>\n"
     ]
    }
   ],
   "source": [
    "print(infer(model, 'Как мне вывести массив в Log?', 512, use_chat = True, system = system))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed9fe84-4155-4d0c-abc8-37b5a2b564d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f434fa-f1b4-4112-9b42-897d5b1ba875",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
