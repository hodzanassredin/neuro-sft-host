{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "uADkArNHQDW6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q trl==0.8.3 transformers==4.45.2 datasets==2.21.0 peft==0.5.0 accelerate==0.28.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import transformers\n",
    "import trl\n",
    "import os\n",
    "import datasets\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_model_name = \"distilbert/distilbert-base-multilingual-cased\"\n",
    "reward_model_path = \"./models/toxic_reward\"\n",
    "\n",
    "dataset_name = \"AlexSham/Toxic_Russian_Comments\"\n",
    "\n",
    "model_name = \"MTSAIR/Cotype-Nano\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_original = datasets.load_dataset(dataset_name, split='train')\n",
    "main_tokenizer = transformers.AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_tokenizer = transformers.AutoTokenizer.from_pretrained(reward_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt_to_chat(prompt):\n",
    "    messages = [\n",
    "        #{\"role\": \"system\", \"content\": \"–¢—ã –≤–µ—Å–µ–ª—ã–π —Å–æ–±–µ—Å–µ–¥–Ω–∏–∫ –∏ –æ–±—â–∞–µ—à—å—Å—è –≤ —á–∞—Ç–µ.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "    text = main_tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "pHs22MXdPify"
   },
   "outputs": [],
   "source": [
    "\n",
    "ref_model = transformers.AutoModelForCausalLM.from_pretrained(model_name, device_map=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KE3jo7uhQrvK",
    "outputId": "6ae43c17-7ecc-4db7-c7c8-1e4975c621b3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated text: <|im_start|>user\n",
      "–ü—Ä–∏–≤–µ—Ç.<|im_end|>\n",
      "<|im_start|>assistant\n",
      "–ó–¥—Ä–∞–≤—Å—Ç–≤—É–π—Ç–µ! –ö–∞–∫ —è –º–æ–≥—É –≤–∞–º –ø–æ–º–æ—á—å —Å–µ–≥–æ–¥–Ω—è?<|im_end|>\n"
     ]
    }
   ],
   "source": [
    "inputs = main_tokenizer(prompt_to_chat(\"–ü—Ä–∏–≤–µ—Ç.\"), return_tensors='pt').to(device)\n",
    "generated_ids = ref_model.generate(**inputs, max_new_tokens=50, do_sample=True)\n",
    "print(\"\\nGenerated text:\", main_tokenizer.decode(generated_ids.flatten().cpu().numpy().tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "TTWR-48ZXQX6"
   },
   "outputs": [],
   "source": [
    "# To train a reward model, you need a dataset (or generator) of positive-negative pairs.\n",
    "# Each training sample should be a dict with 4 keys:\n",
    "#  - input_ids_chosen, attention_mask_chosen = tokenizer(\"A sentence that human labeler likes more\")\n",
    "#  - input_ids_rejected, attention_mask_rejected = tokenizer(\"A sentence that human labeler likes less\")\n",
    "\n",
    "\n",
    "\n",
    "class PairwiseDataset(torch.utils.data.Dataset):\n",
    "    \"\"\" A dataset of all possible pairs of chosen and texts in TRT reward training format \"\"\"\n",
    "    def __init__(self, data, tokenizer, accepted_label: int, max_number = None, label = 'label', text = 'text'):\n",
    "        super().__init__()\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "        self.chosen_texts = [row[text] for row in data if row[label] == accepted_label]\n",
    "        self.rejected_texts = [row[text] for row in data if row[label] != accepted_label]\n",
    "        if max_number is not None:\n",
    "            self.chosen_texts = self.chosen_texts[:max_number]\n",
    "            self.rejected_texts = self.rejected_texts[:max_number]\n",
    "        self.column_names = [\"input_ids_chosen\",\"input_ids_rejected\"]\n",
    "        assert self.chosen_texts, f\"no texts with label {accepted_label}\"\n",
    "        print(f\"Found {len(self.chosen_texts)} chosen and {len(self.rejected_texts)} rejected texts, {len(self)} pairs\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.chosen_texts) * len(self.rejected_texts)  # all pairs\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "        chosen = self.tokenizer(self.chosen_texts[index // len(self.chosen_texts)], truncation=True)\n",
    "        rejected = self.tokenizer(self.rejected_texts[index % len(self.chosen_texts)], truncation=True)\n",
    "        return dict(input_ids_chosen=chosen['input_ids'], attention_mask_chosen=chosen['attention_mask'],\n",
    "                    input_ids_rejected=rejected['input_ids'], attention_mask_rejected=rejected['attention_mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'PairwiseDataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#data_original = datasets.load_dataset(\"imdb\", split='train')#Found 12500 chosen and 12500 rejected texts, 156250000 pairs\u001b[39;00m\n\u001b[1;32m      4\u001b[0m data_original \u001b[38;5;241m=\u001b[39m datasets\u001b[38;5;241m.\u001b[39mload_dataset(dataset_name, split\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m reward_data \u001b[38;5;241m=\u001b[39m \u001b[43mPairwiseDataset\u001b[49m(data_original, reward_tokenizer, accepted_label\u001b[38;5;241m=\u001b[39mTARGET_LABEL, max_number\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m12500\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'PairwiseDataset' is not defined"
     ]
    }
   ],
   "source": [
    "TARGET_LABEL = 1   # toxic\n",
    "reward_data = PairwiseDataset(data_original, reward_tokenizer, accepted_label=TARGET_LABEL, max_number=12500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "olo-bvgNcwEC",
    "outputId": "16051d61-c450-4a3e-8689-f91f28f8280e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CHOSEN: [CLS] –Ω–∞ —Ö—É–π, –±–µ–∑–ª–∏–∫–∞—è [SEP]\n",
      "REJECTED: [CLS] –ø—Ä–∏–º–∏—Ç–µ —Å–æ–±–æ–ª–µ–∑–Ω–æ–≤–∞–Ω–∏—è –æ—Ç –≤—Å–µ–π –Ω–∞—à–µ–π —Å–µ–º—å–∏. [SEP]\n"
     ]
    }
   ],
   "source": [
    "sample = reward_data[31337]\n",
    "print('CHOSEN:', reward_tokenizer.decode(sample['input_ids_chosen']))\n",
    "print('REJECTED:', reward_tokenizer.decode(sample['input_ids_rejected']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1053
    },
    "id": "oaQ_-JAzakJs",
    "outputId": "4ffe023f-4773-4a47-8af2-86839db874b5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading pretrained reward_model\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def train_reward(reward_model, reward_tokenizer, reward_data):\n",
    "    training_args = trl.RewardConfig(  # like transformers.TrainingArguments\n",
    "        output_dir=\"reward_model\",\n",
    "        per_device_train_batch_size=32,\n",
    "        gradient_accumulation_steps=1,\n",
    "        learning_rate=1.41e-5,\n",
    "        max_steps=1_000,              # note: training may need more than 1k steps\n",
    "        logging_steps=50,\n",
    "        gradient_checkpointing=True,  # reduce memory usage but train ~30% slower\n",
    "        gradient_checkpointing_kwargs={\"use_reentrant\": False},\n",
    "        fp16=True,                     # disable this on CPU or on very old GPUs\n",
    "        # you may add any other hyperparameters that you found useful in weeks 5-7\n",
    "        #report_to=\"none\"\n",
    "    )\n",
    "    \n",
    "    trainer = trl.RewardTrainer(\n",
    "        model=reward_model,\n",
    "        args=training_args,\n",
    "        tokenizer=reward_tokenizer,\n",
    "        train_dataset=reward_data,\n",
    "        peft_config=None,  # optionally, you may tune with LoRA, prompt-tuning, etc\n",
    "    )\n",
    "    \n",
    "    trainer.train()\n",
    "    trainer.save_model(reward_model_path)\n",
    "\n",
    "\n",
    "if not os.path.isdir(reward_model_path):\n",
    "    reward_model = transformers.AutoModelForSequenceClassification.from_pretrained(reward_model_name, device_map=device)\n",
    "    train_reward(reward_model, reward_tokenizer, reward_data)\n",
    "else:\n",
    "    print(\"loading pretrained reward_model\")\n",
    "    reward_model = transformers.AutoModelForSequenceClassification.from_pretrained(reward_model_path, device_map=device)\n",
    "\n",
    "#c533837f4fb333a7a6371e4e3073ca3bafa0e9b3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CRk7z-2r4C-A",
    "outputId": "5b99e451-e2e7-44bb-cf11-c28a8eb8ae64"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistilBertForSequenceClassification(\n",
       "  (distilbert): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(119547, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0-5): 6 x TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reward_model.gradient_checkpointing_disable()\n",
    "reward_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IeQ108nOZ7nO",
    "outputId": "6de29424-1308-4677-e909-4f3f9d8ceb6b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEXT: —Ö–æ—á—É —á—Ç–æ–±—ã —É –º–æ–µ–π –¥–æ—á–µ—Ä–∏ –æ–ª—å–≥–∏ –±—ã–ª–∞ —Å–≤–æ—è –∫–≤–∞—Ä—Ç–∏—Ä–∞ –≤ –Ω–æ–≤–æ–º –¥–æ–º–µüéàüéàüéàüåªüåªüåºüòáüíñ\n",
      "REWARD: -4.1432342529296875\n",
      "LABEL: 0\n",
      "\n",
      "TEXT: —Ö–∞–±–∞—Ä–æ–≤—Å–∫ –≥–æ—Ä–æ–¥ –≥–µ—Ä–æ–π,–∞ –≤ –æ–¥–Ω–æ–∫–ª–∞—Å—Å–Ω–∏–∫–∞—Ö —Å–∏–¥—è—Ç –¥–µ–±–∏–ª—ã –∏ —É–ø–æ—Ä–æ—Ç—ã–µ –ø—É—Ç–∏–Ω—Ü—ã.\n",
      "REWARD: 4.705625534057617\n",
      "LABEL: 1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for sample_index in 45, 16000:\n",
    "  print('TEXT:', data_original[sample_index]['text'])\n",
    "  inputs = reward_tokenizer(data_original[sample_index]['text'], truncation=True, return_tensors='pt').to(device)\n",
    "  with torch.no_grad():\n",
    "    reward = reward_model(**inputs).logits[0, 0].item()\n",
    "    print(\"REWARD:\", reward)\n",
    "  print('LABEL:', data_original[sample_index]['label'])\n",
    "  print()\n",
    "\n",
    "# note: your reward model may produce different absolute rewards.\n",
    "# This is fine as long as the rewards are ordered correctly (most of the time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4460 chosen and 20369 rejected texts, 90845740 pairs\n"
     ]
    }
   ],
   "source": [
    "data_original_test = datasets.load_dataset(dataset_name, split='test')\n",
    "reward_test_data = PairwiseDataset(data_original_test, reward_tokenizer, accepted_label=TARGET_LABEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "aEevUrfqavnb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proccessed train: 10.000 freq:99.000\n",
      "Proccessed train: 20.000 freq:99.500\n",
      "Proccessed train: 30.000 freq:99.667\n",
      "Proccessed train: 40.000 freq:99.750\n",
      "Proccessed train: 50.000 freq:99.800\n",
      "Proccessed train: 60.000 freq:99.833\n",
      "Proccessed train: 70.000 freq:99.857\n",
      "Proccessed train: 80.000 freq:99.875\n",
      "Proccessed train: 90.000 freq:99.889\n",
      "Proccessed train: 100.000 freq:99.900\n",
      "Proccessed test: 10.000 freq:100.000\n",
      "Proccessed test: 20.000 freq:100.000\n",
      "Proccessed test: 30.000 freq:100.000\n",
      "Proccessed test: 40.000 freq:100.000\n",
      "Proccessed test: 50.000 freq:100.000\n",
      "Proccessed test: 60.000 freq:100.000\n",
      "Proccessed test: 70.000 freq:100.000\n",
      "Proccessed test: 80.000 freq:99.875\n",
      "Proccessed test: 90.000 freq:99.889\n",
      "Proccessed test: 100.000 freq:99.900\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def get_freq(data, name, max_len = None):\n",
    "    total = len(data)\n",
    "    if max_len is not None and max_len < total:\n",
    "        total = max_len\n",
    "    run_total = 0\n",
    "    run_ok = 0\n",
    "    for idx in range(total):\n",
    "      sample = data[idx]\n",
    "      chosen = {\"input_ids\" : torch.LongTensor([sample[\"input_ids_chosen\"]]).to(device), \"attention_mask\" : torch.LongTensor([sample[\"attention_mask_chosen\"]]).to(device)}\n",
    "      rejected = {\"input_ids\" : torch.LongTensor([sample[\"input_ids_rejected\"]]).to(device), \"attention_mask\" : torch.LongTensor([sample[\"attention_mask_rejected\"]]).to(device)}\n",
    "      with torch.no_grad():\n",
    "        chosen_reward = reward_model(**chosen).logits[0, 0].item()\n",
    "        rejected_reward = reward_model(**rejected).logits[0, 0].item()\n",
    "        if chosen_reward > rejected_reward:\n",
    "            run_ok += 1\n",
    "      run_total += 1  \n",
    "      if run_total % 100 == 0:\n",
    "          proc = run_total/total * 100\n",
    "          freq = run_ok / run_total * 100\n",
    "          print(f'Proccessed {name}: {proc:5.3f} freq:{freq:5.3f}')\n",
    "get_freq(reward_data, \"train\",1000)\n",
    "get_freq(reward_test_data, \"test\",1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8BRsyb2cq5dR",
    "outputId": "4d4c917c-4a79-4db8-fff4-a63167256044"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–≠—Ç–æ –±—ã–ª–æ –±—ã –∑–∞–º–µ—á–∞—Ç–µ–ª—å–Ω–æ, –Ω–æ —É –º–µ–Ω—è –Ω–µ—Ç –≤—Ä–µ–º–µ–Ω–∏ –Ω–∞ —Ç–∞–∫–∏–µ —Ä–∞–∑–≥–æ–≤–æ—Ä—ã. –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –Ω–µ –±–µ—Å–ø–æ–∫–æ–π—Ç–µ—Å—å –æ–± —ç—Ç–æ–º. –°–ø–∞—Å–∏–±–æ –∑–∞ –≤–∞—à–µ –≤—Ä–µ–º—è –∏ –ø–æ–º–æ—â—å! –ñ–µ–ª–∞—é –≤–∞–º –≤—Å–µ–≥–æ —Å–∞–º–æ–≥–æ —Ö–æ—Ä–æ—à–µ–≥–æ! –£–¥–∞—á–∏ –≤ —Ä–∞–±–æ—Ç–µ –∏ –≤ –∂–∏–∑–Ω–∏. \n",
      "\n",
      "---\n",
      "\n",
      "–°–ø–∞—Å–∏–±–æ –∑–∞ –≤–∞—à –æ—Ç–≤–µ—Ç. –Ø –ø–æ–Ω–∏–º–∞—é, —á—Ç–æ —É –≤–∞—Å –±—ã–≤–∞–µ—Ç –º–Ω–æ–≥–æ —Ä–∞–±–æ—Ç—ã –∏ –æ–±—è–∑–∞–Ω–Ω–æ—Å—Ç–µ–π. –ù–∞–¥–µ—é—Å—å, —á—Ç–æ –≤–∞—à–∏ –¥–µ–ª–∞ –±—É–¥—É—Ç —É—Å–ø–µ—à–Ω–æ –∑–∞–≤–µ—Ä—à–µ–Ω—ã, –∏ –≤—ã —Å–º–æ–∂–µ—Ç–µ —Ä–∞—Å—Å–ª–∞–±–∏—Ç—å—Å—è –ø–æ—Å–ª–µ —Ç—Ä—É–¥–æ–≤–æ–≥–æ –¥–Ω—è. –ï—Å–ª–∏ —É –≤–∞—Å –≤–æ–∑–Ω–∏–∫–Ω—É—Ç –∫–∞–∫–∏–µ-–ª–∏–±–æ –≤–æ–ø—Ä–æ—Å—ã –∏–ª–∏ –Ω—É–∂–Ω–∞ –±—É–¥–µ—Ç\n",
      "–≠—Ç–æ –±—ã–ª–æ –±—ã –∑–∞–º–µ—á–∞—Ç–µ–ª—å–Ω–æ, –µ—Å–ª–∏ –±—ã –º—ã –º–æ–≥–ª–∏ –ø—Ä–æ–≤–µ—Å—Ç–∏ —ç—Ç–æ—Ç –ø—Ä–æ–µ–∫—Ç –≤–º–µ—Å—Ç–µ. –ù–æ —É –º–µ–Ω—è –Ω–µ—Ç –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–Ω–æ–π —Ä–∞–±–æ—Ç—ã –∏–∑-–∑–∞ –ª–∏—á–Ω—ã—Ö –æ–±—Å—Ç–æ—è—Ç–µ–ª—å—Å—Ç–≤. –Ø –ø–æ–Ω–∏–º–∞—é –∏ —Ü–µ–Ω—é –≤–∞—à–µ –≤—Ä–µ–º—è –∏ –ø–æ–º–æ—â—å. –°–ø–∞—Å–∏–±–æ –∑–∞ –≤–∞—à–µ —Å–æ—Ç—Ä—É–¥–Ω–∏—á–µ—Å—Ç–≤–æ.\n",
      "\n",
      "---\n",
      "\n",
      "**–û—Ç–≤–µ—Ç –Ω–∞ –ø–∏—Å—å–º–æ:**\n",
      "\n",
      "–£–≤–∞–∂–∞–µ–º—ã–π(–∞—è) [–ò–º—è],\n",
      "\n",
      "–ë–ª–∞–≥–æ–¥–∞—Ä—é –í–∞—Å –∑–∞ –í–∞—à–µ –ø–∏—Å—å–º–æ –∏ –∑–∞ —Ç–æ, —á—Ç–æ –í—ã –ø–æ–¥–µ–ª–∏–ª–∏—Å—å —Å–≤–æ–∏–º–∏ –ø–ª–∞–Ω–∞–º–∏ –ø–æ –ø—Ä–æ–µ–∫—Ç—É. –ú–Ω–µ –æ—á–µ–Ω—å –ø—Ä–∏—è—Ç–Ω–æ —Å–ª—ã—à–∞—Ç—å –æ –í–∞—à–µ–º –∂–µ–ª–∞–Ω–∏–∏ —Ä–∞–±–æ—Ç\n",
      "–≠—Ç–æ –±—ã–ª–æ –±—ã –∑–∞–º–µ—á–∞—Ç–µ–ª—å–Ω–æ, –µ—Å–ª–∏ –±—ã –º—ã –º–æ–≥–ª–∏ –ø—Ä–æ–≤–µ—Å—Ç–∏ –≤—Ä–µ–º—è –≤–º–µ—Å—Ç–µ. –ù–æ —É –º–µ–Ω—è —Å–µ–π—á–∞—Å –æ—á–µ–Ω—å —Å—Ç—Ä–µ—Å—Å–æ–≤—ã–µ –¥–µ–ª–∞, –∏ —è –Ω–µ –º–æ–≥—É –ø–æ–∑–≤–æ–ª–∏—Ç—å —Å–µ–±–µ –æ—Ç–≤–ª–µ–∫–∞—Ç—å—Å—è –Ω–∞ –≤—Å—Ç—Ä–µ—á–∏ –∏–ª–∏ –æ–±—â–µ–Ω–∏–µ —Å –≤–∞–º–∏.\n",
      "\n",
      "–ö–æ–Ω–µ—á–Ω–æ, —è —Ü–µ–Ω—é –≤–∞—à–µ –º–Ω–µ–Ω–∏–µ –∏ –≥–æ—Ç–æ–≤ –≤—ã—Å–ª—É—à–∞—Ç—å –≤–∞—Å. –ï—Å–ª–∏ –≤–∞–º –±—É–¥–µ—Ç —É–¥–æ–±–Ω–æ –ø–æ–≥–æ–≤–æ—Ä–∏—Ç—å, —è –±—É–¥—É —Ä–∞–¥ —ç—Ç–æ —Å–¥–µ–ª–∞—Ç—å. –°–ø–∞—Å–∏–±–æ –∑–∞ –ø–æ–Ω–∏–º–∞–Ω–∏–µ.\n",
      "---\n",
      "\n",
      "–í–∞—à –æ—Ç–≤–µ—Ç:\n",
      "\n",
      "–Ø —Ü–µ–Ω—é –≤–∞—à—É –ø–æ–¥–¥–µ—Ä–∂–∫—É –∏ –≥–æ—Ç–æ–≤ –≤—ã—Å–ª—É—à–∞—Ç—å –≤–∞—Å. –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, —Ä–∞—Å—Å–∫–∞–∂–∏—Ç–µ –º–Ω–µ –ø–æ–¥—Ä–æ–±–Ω–µ–µ –æ –≤–∞—à–∏—Ö —Å—Ç—Ä–µ—Å—Å–æ–≤—ã—Ö –¥–µ–ª\n",
      "–≠—Ç–æ –±—ã–ª–æ –±—ã –∏–Ω—Ç–µ—Ä–µ—Å–Ω–æ, –µ—Å–ª–∏ –±—ã –≤—ã –º–æ–≥–ª–∏ –ø–æ–¥–µ–ª–∏—Ç—å—Å—è —Å–≤–æ–∏–º–∏ –º—ã—Å–ª—è–º–∏ –∏ –æ–ø—ã—Ç–æ–º –ø–æ —ç—Ç–æ–º—É –ø–æ–≤–æ–¥—É. –ö–∞–∫ –≤—ã –æ—Ç–Ω–æ—Å–∏—Ç–µ—Å—å –∫ —Ç–æ–º—É, —á—Ç–æ–±—ã —Å–æ–∑–¥–∞—Ç—å —Ç–∞–∫–æ–π –∫–æ–Ω—Ç–µ–Ω—Ç? –ï—Å—Ç—å –ª–∏ –∫–∞–∫–∏–µ-—Ç–æ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ –∞—Å–ø–µ–∫—Ç—ã –∏–ª–∏ –≤–æ–ø—Ä–æ—Å—ã, –∫–æ—Ç–æ—Ä—ã–µ –≤–∞–º —Ö–æ—Ç–µ–ª–æ—Å—å –±—ã –æ–±—Å—É–¥–∏—Ç—å? –ë—É–¥—É —Ä–∞–¥ —É–∑–Ω–∞—Ç—å –±–æ–ª—å—à–µ –æ –≤–∞—à–µ–º –ø–æ–¥—Ö–æ–¥–µ –∫ –Ω–∞–ø–∏—Å–∞–Ω–∏—é —Å—Ç–∞—Ç–µ–π –∏ –æ–±—â–µ–Ω–∏–∏ —Å —á–∏—Ç–∞—Ç–µ–ª—è–º–∏. –°–ø–∞—Å–∏–±–æ! \n",
      "\n",
      "–í–∞—à –≤–æ–ø—Ä–æ—Å –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –∑–∞–∏–Ω—Ç–µ—Ä–µ—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç—å –≤ –ø–æ–ª—É—á–µ–Ω–∏–∏ —Å–æ–≤–µ—Ç–æ–≤ –∏ –∑–Ω–∞–Ω–∏–π –æ—Ç —á–µ–ª–æ–≤–µ–∫–∞, –∫–æ—Ç–æ—Ä—ã–π —É–∂–µ –∏–º–µ–µ—Ç –æ–ø—ã—Ç –≤ –Ω–∞–ø–∏—Å–∞–Ω–∏–∏ —Å—Ç–∞—Ç–µ–π\n",
      "–≠—Ç–æ –±—ã–ª–æ –±—ã –∏–Ω—Ç–µ—Ä–µ—Å–Ω–æ, –µ—Å–ª–∏ –±—ã –≤—ã –º–æ–≥–ª–∏ –ø–æ–¥–µ–ª–∏—Ç—å—Å—è —Å–≤–æ–∏–º–∏ –º—ã—Å–ª—è–º–∏ –∏ –æ–ø—ã—Ç–æ–º –ø–æ —ç—Ç–æ–º—É –ø–æ–≤–æ–¥—É. –ö–∞–∫ –≤—ã –æ—Ç–Ω–æ—Å–∏—Ç–µ—Å—å –∫ —Ç–æ–º—É, —á—Ç–æ–±—ã —Å–æ–∑–¥–∞—Ç—å —Ç–∞–∫–æ–π –∫–æ–Ω—Ç–µ–Ω—Ç? –ï—Å—Ç—å –ª–∏ –∫–∞–∫–∏–µ-—Ç–æ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ –∞—Å–ø–µ–∫—Ç—ã –∏–ª–∏ –≤–æ–ø—Ä–æ—Å—ã, –∫–æ—Ç–æ—Ä—ã–µ –≤–∞–º —Ö–æ—Ç–µ–ª–æ—Å—å –±—ã –æ–±—Å—É–¥–∏—Ç—å –∏–ª–∏ —É–∑–Ω–∞—Ç—å –±–æ–ª—å—à–µ –æ?\n",
      "\n",
      "–ö–æ–Ω–µ—á–Ω–æ! –Ø —Å —É–¥–æ–≤–æ–ª—å—Å—Ç–≤–∏–µ–º –ø–æ–¥–µ–ª—é—Å—å —Å–≤–æ–∏–º–∏ –º—ã—Å–ª—è–º–∏ –∏ –æ–ø—ã—Ç–æ–º.\n",
      "\n",
      "1. **–í–∞–∂–Ω–æ—Å—Ç—å –∫–æ–Ω—Ç–µ–Ω—Ç–∞**: –°–æ–∑–¥–∞–Ω–∏–µ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ –∏–º–µ–µ—Ç –±–æ–ª—å—à–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ –¥–ª—è –ø—Ä–∏–≤–ª–µ—á–µ–Ω–∏—è –≤–Ω–∏–º–∞–Ω–∏—è –∏ —É–¥–µ—Ä–∂–∞–Ω–∏—è –∞—É–¥–∏—Ç–æ—Ä–∏–∏. –ö–æ–Ω—Ç\n"
     ]
    }
   ],
   "source": [
    "def generate(inputs):\n",
    "    return ref_model.generate(**inputs, \n",
    "                              min_length=-1, \n",
    "                              max_new_tokens=128, \n",
    "                              do_sample=True, \n",
    "                              top_k=0, \n",
    "                              top_p=1.0, \n",
    "                              pad_token_id=main_tokenizer.eos_token_id)\n",
    "                              \n",
    "                              \n",
    "                     #         max_new_tokens=50, \n",
    "                      #              temperature=0.8,  # Increased from 0.7\n",
    "                       #            top_k=50,         # Added top_k sampling\n",
    "                        #           top_p=0.95,       # Added nucleus sampling\n",
    "                         #          do_sample=True ,   # Enable sampling\n",
    "                          #         repetition_penalty=1.05)\n",
    "\n",
    "inputs = main_tokenizer([\"–≠—Ç–æ –±—ã–ª–æ\"] * 5, return_tensors='pt').to(device)\n",
    "cands = generate(inputs)\n",
    "for candidate in cands:\n",
    "  print(main_tokenizer.decode(candidate.flatten().cpu().numpy().tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "r08F4lz7yxE1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================\n",
      "–≠—Ç–æ—Ç —Ñ–∏–ª—å–º –±—ã–ª —Å–Ω—è—Ç –≤ 1950 –≥–æ–¥—É –∏ —è–≤–ª—è–µ—Ç—Å—è –∫–ª–∞—Å—Å–∏–∫–æ–π –∫–∏–Ω–µ–º–∞—Ç–æ–≥—Ä–∞—Ñ–∞. –û–Ω –ø–æ–ª—É—á–∏–ª –º–Ω–æ–∂–µ—Å—Ç–≤–æ –Ω–∞–≥—Ä–∞–¥ –∑–∞ —Å–≤–æ–∏ —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ —Å–ø–µ—Ü—ç—Ñ—Ñ–µ–∫—Ç—ã, –∫–æ—Ç–æ—Ä—ã–µ –±—ã–ª–∏ –Ω–∞ —Ç–æ—Ç –º–æ–º–µ–Ω—Ç –Ω–æ–≤–∞—Ç–æ—Ä—Å–∫–∏–º–∏. –í —Ñ–∏–ª—å–º–µ —Ä–∞—Å—Å–∫–∞–∑—ã–≤–∞–µ—Ç—Å—è –∏—Å—Ç–æ—Ä–∏—è –º–æ–ª–æ–¥–æ–≥–æ —á–µ–ª–æ–≤–µ–∫–∞, –∫–æ—Ç–æ—Ä—ã–π —Å—Ç–∞–ª–∫–∏–≤–∞–µ—Ç—Å—è —Å —Ä–∞–∑–ª–∏—á–Ω—ã–º–∏ —Ç—Ä—É–¥–Ω–æ—Å—Ç—è–º–∏ –∏ –∏—Å–ø—ã—Ç–∞–Ω–∏—è–º–∏ –≤ —Å–≤–æ–µ–π –∂–∏–∑–Ω–∏. –û–¥–Ω–∞–∫–æ, –Ω–µ—Å–º–æ—Ç—Ä—è –Ω–∞ –≤—Å–µ –ø—Ä–µ–ø—è—Ç—Å—Ç–≤–∏—è, –æ–Ω –≤—Å–µ–≥–¥–∞ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –æ–ø—Ç–∏–º–∏–∑–º –∏ –≤–µ—Ä–∏—Ç –≤ –ª—É—á—à–µ–µ –±—É–¥—É—â–µ–µ.\n",
      "\n",
      "–ò–∑–≤–µ—Å—Ç–Ω–æ, —á—Ç–æ —Ä–µ–∂–∏—Å—Å–µ—Ä —ç—Ç–æ–≥–æ —Ñ–∏–ª—å–º–∞ ‚Äî –≠—Ä–Ω–µ—Å—Ç –•–æ–ø–ø–µ—Ä\n",
      "====================\n",
      "–Ø –ø–æ—à–µ–ª –≥—É–ª—è—Ç—å –≤ –ø–∞—Ä–∫ –∏ –≤—Å—Ç—Ä–µ—Ç–∏–ª –∫–æ—Ç–∞. –ö–æ—Ç –±—ã–ª –æ—á–µ–Ω—å –º–∏–ª—ã–π, –µ–≥–æ –≥–ª–∞–∑–∞ —Å–≤–µ—Ç–∏–ª–∏—Å—å —Ä–∞–¥–æ—Å—Ç—å—é. –Ø —Ä–µ—à–∏–ª –æ—Å—Ç–∞–≤–∏—Ç—å –µ–º—É –µ–¥—É –∏ —É—à–µ–ª. –í–æ–∑–≤—Ä–∞—â–∞—è—Å—å –¥–æ–º–æ–π, —è –∑–∞–º–µ—Ç–∏–ª, —á—Ç–æ –Ω–∞ –¥–æ—Ä–æ–≥–µ –ª–µ–∂–∞–ª –º—è—á–∏–∫. –ù–µ –∑–Ω–∞—è, –∫—É–¥–∞ –µ–≥–æ –ø–æ–ª–æ–∂–∏—Ç—å, —è —Ä–µ—à–∏–ª—Å—è –≤–∑—è—Ç—å –µ–≥–æ —Å —Å–æ–±–æ–π. –ö–∞–∫ –º–Ω–µ –±—ã—Ç—å —Å —ç—Ç–∏–º –º—è—á–∏–∫–æ–º? \n",
      "\n",
      "–ö–∞–∫ –≤—ã –¥—É–º–∞–µ—Ç–µ, –∫–∞–∫–æ–µ —Ä–µ—à–µ–Ω–∏–µ –±—É–¥–µ—Ç –ª—É—á—à–µ: –æ—Ç–¥–∞—Ç—å –º—è—á–∏–∫ –∫–æ–º—É-—Ç–æ, –∫—Ç–æ —Å–º–æ–∂–µ—Ç –µ–≥–æ –ø—Ä–∏–Ω–µ—Å—Ç–∏ –¥–æ–º–æ–π –∏–ª–∏ –æ—Å—Ç–∞–≤–∏—Ç—å –µ–≥–æ –∑–¥–µ—Å—å, —á—Ç–æ–±—ã –æ–Ω –º–æ–≥ –Ω–∞–π—Ç–∏ —Ö–æ–∑—è–∏–Ω–∞?\n",
      "====================\n",
      "–ò —á—Ç–æ, –µ—Å–ª–∏ —è –Ω–∞—á–Ω—É —Å —Ç–æ–≥–æ, —á—Ç–æ —É –º–µ–Ω—è –µ—Å—Ç—å 100 –µ–¥–∏–Ω–∏—Ü –≤–ª–æ–∂–µ–Ω–Ω—ã—Ö –≤ –∞–∫—Ü–∏–∏ –∫–æ–º–ø–∞–Ω–∏–∏. –Ø —Ö–æ—á—É —É–∑–Ω–∞—Ç—å, –∫–∞–∫—É—é —Å—É–º–º—É –º–Ω–µ –Ω—É–∂–Ω–æ –±—É–¥–µ—Ç –≤—ã–≤–µ—Å—Ç–∏ –∏–∑ —ç—Ç–∏—Ö –∞–∫—Ü–∏–π —á–µ—Ä–µ–∑ 5 –ª–µ—Ç, —á—Ç–æ–±—ã –∏–º–µ—Ç—å –≤ –æ–±—â–µ–π —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ 200 –µ–¥–∏–Ω–∏—Ü? –ò –∫–∞–∫–æ–≤ –±—É–¥–µ—Ç –ø—Ä–æ—Ü–µ–Ω—Ç–Ω–æ–µ —Å–æ–æ—Ç–Ω–æ—à–µ–Ω–∏–µ –º–µ–∂–¥—É –∏—Å—Ö–æ–¥–Ω—ã–º–∏ –∏ –∂–µ–ª–∞–µ–º—ã–º–∏ —Å—É–º–º–∞–º–∏?\n",
      "\n",
      "–ß—Ç–æ–±—ã –æ—Ç–≤–µ—Ç–∏—Ç—å –Ω–∞ —ç—Ç–æ—Ç –≤–æ–ø—Ä–æ—Å, –º—ã –º–æ–∂–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ø—Ä–æ—Å—Ç–æ–µ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –≤—ã—Ä–∞–∂–µ–Ω–∏–µ –¥–ª—è —Ä–∞—Å—á–µ—Ç–∞ –±—É–¥—É—â–µ–π —Å—Ç–æ–∏–º–æ—Å—Ç–∏ –∏–Ω–≤–µ—Å—Ç–∏—Ü–∏–π —Å —É—á–µ—Ç–æ–º –µ–∂–µ–≥–æ–¥–Ω–æ–≥–æ —Ä–µ–∏–Ω–≤–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –¥\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "def infer(prompt, N = 16):\n",
    "    inputs = main_tokenizer([prompt] * 16, return_tensors='pt').to(device)\n",
    "    cands = generate(inputs);\n",
    "    texts = []\n",
    "    rewards = []\n",
    "    for candidate in cands:\n",
    "        cand_text = main_tokenizer.decode(candidate.flatten().cpu().numpy().tolist(), skip_special_tokens=True)\n",
    "        texts.append(cand_text)\n",
    "\n",
    "    for text in texts:\n",
    "        inputs = reward_tokenizer(text, truncation=True, return_tensors='pt').to(device)\n",
    "        with torch.no_grad():\n",
    "            reward = reward_model(**inputs).logits[0, 0].item()\n",
    "        rewards.append(reward)\n",
    "    return texts[np.argmax(rewards)]\n",
    "\n",
    "neutral_prompts = [\"–≠—Ç–æ—Ç —Ñ–∏–ª—å–º\", \"–Ø –ø–æ—à–µ–ª –≥—É–ª—è—Ç—å\", \"–ò —á—Ç–æ\"]\n",
    "\n",
    "for p in neutral_prompts:\n",
    "    res = infer(p).replace('<br />', '\\n')\n",
    "    print('='*20)\n",
    "    print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 86,
     "referenced_widgets": [
      "f6ba50eafdeb4b94a1e7c22c5027f709",
      "63e252cfd9f44ef79137473b618828dd",
      "4ad4da252cb64e818d35eb3869adc81c",
      "d3a8dcfad53b4de885b39ee83e6029ef",
      "44f3d7c434354a90bfa3d4750048b80f",
      "2ca9e640d5d549658e42fd73af8ea399",
      "c8ea1c2d3b5040378d0f2bd213933603",
      "1692cc1532df4c2695c729a964a31da8",
      "55c624445ca44090a2f109d3bf5f3f6a",
      "411cd47238a94edc8283e178e04d386f",
      "9604bff7c9414071a55d063fdf4174fa"
     ]
    },
    "id": "jm5IUrer0xd_",
    "outputId": "ea58969e-cb80-4ff5-8a34-7c6a2d14cab3"
   },
   "outputs": [],
   "source": [
    "# Note: this code is specific to IMDB; you will need to re-write it for other tasks\n",
    "data_for_rlhf = data_original.filter(lambda row: len(row['text']) > 200, batched=False)\n",
    "data_for_rlhf = data_for_rlhf.remove_columns(['label'])\n",
    "sample_length = trl.core.LengthSampler(2, 8)  # use the first 2-8 tokens as query\n",
    "\n",
    "def select_query_and_tokenize(sample):\n",
    "    query_ids = main_tokenizer.encode(sample[\"text\"])[: sample_length()]\n",
    "    sample[\"query\"] = main_tokenizer.decode(query_ids)  # query is the only required column\n",
    "    sample[\"input_ids\"] = query_ids  # to avoid re-tokenizing later\n",
    "    return sample  # we do not need the rest - it will be generated by the model\n",
    "\n",
    "data_for_rlhf = data_for_rlhf.map(select_query_and_tokenize, batched=False)\n",
    "data_for_rlhf.set_format(type=\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "kkm4MLOr20Jk"
   },
   "outputs": [],
   "source": [
    "from typing import List\n",
    "def compute_reward(texts: List[str]) -> torch.Tensor:\n",
    "  inputs = reward_tokenizer(texts, truncation=True, padding=True, return_tensors='pt').to(device)\n",
    "  with torch.no_grad():\n",
    "    return reward_model(**inputs).logits[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7wJto13M3vWu",
    "outputId": "0214bd72-21e0-49ea-ec33-12f9e9d587d9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-4.1432,  4.7056], device='cuda:0')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_reward([data_original[45]['text'], data_original[16000]['text']])  # test on human-written reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U3buACYV4QLJ"
   },
   "source": [
    "Finally, we move to RL training. In this tutorial, we'll train LoRA adapters and not the full model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nar1yXgl4KQa",
    "outputId": "0dfa2e71-48ef-497b-90cf-e156fa921c7e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:A <class 'transformers.models.qwen2.modeling_qwen2.Qwen2ForCausalLM'> model is loaded from 'MTSAIR/Cotype-Nano', and no v_head weight is found. This IS expected if you are not resuming PPO training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 18,464,768 || all params: 1,562,180,609 || trainable%: 1.1819867621977376\n"
     ]
    }
   ],
   "source": [
    "import peft\n",
    "#peft_config = peft.LoraConfig(\n",
    "#    task_type=peft.TaskType.CAUSAL_LM, r=32, lora_alpha=32, lora_dropout=0.0, inference_mode=False\n",
    "#)\n",
    "# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏–∏\n",
    "bnb_config = transformers.BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ LoRA\n",
    "peft_config = peft.LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\n",
    "        \"mlp.down_proj\",\n",
    "        \"self_attn.k_proj\",\n",
    "        \"self_attn.o_proj\",\n",
    "        \"mlp.up_proj\",\n",
    "        \"self_attn.v_proj\",\n",
    "        \"mlp.gate_proj\",\n",
    "        \"self_attn.q_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=peft.TaskType.CAUSAL_LM\n",
    ")\n",
    "# reload main model as AutoModelForCausalLMWithValueHead - with an extra head needed for PPO\n",
    "main_tokenizer = transformers.AutoTokenizer.from_pretrained(model_name)\n",
    "main_tokenizer.pad_token = main_tokenizer.eos_token\n",
    "\n",
    "main_model = trl.AutoModelForCausalLMWithValueHead.from_pretrained(model_name, device_map=device,quantization_config=bnb_config,)\n",
    "main_model.generation_config = transformers.GenerationConfig()\n",
    "main_model = peft.get_peft_model(main_model, peft_config, adapter_name='default')\n",
    "main_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "EvTtiLs94txE"
   },
   "outputs": [],
   "source": [
    "training_args = trl.PPOConfig(\n",
    "    model_name=main_model.config._name_or_path,\n",
    "    gradient_accumulation_steps=4,\n",
    "    learning_rate=1.41e-4,\n",
    "    batch_size=16,\n",
    "    ppo_epochs=4,                 # PPO performs this many updates per training batch\n",
    "    mini_batch_size = 4,\n",
    "    \n",
    ")\n",
    "\n",
    "ppo_trainer = trl.PPOTrainer(\n",
    "    training_args, model=main_model.model, tokenizer=main_tokenizer,\n",
    "    dataset=data_for_rlhf, data_collator=lambda data: dict((key, [d[key] for d in data]) for key in data[0]),\n",
    "    \n",
    ")  # note: we pass main_model.model because PPOTrainer checks for one of several supported model types ...\n",
    "# ... main_model.model is a model with adapters, which is supported. main_model itself is a wrapper that is not supported\n",
    "\n",
    "#training_args = trl.PPOConfig(\n",
    "#    output_dir = \"./models/ppotrain\",\n",
    "#    gradient_accumulation_steps=1,\n",
    "#    learning_rate=1.41e-5,\n",
    "#    batch_size=64,\n",
    "#    num_ppo_epochs =4,                 # PPO performs this many updates per training batch\n",
    "#)\n",
    "\n",
    "#ppo_trainer = trl.PPOTrainer(\n",
    "#    training_args, model=main_model.model, tokenizer=main_tokenizer,\n",
    "#    train_dataset=data_for_rlhf, data_collator=lambda data: dict((key, [d[key] for d in data]) for key in data[0]),\n",
    "#    reward_model = reward_model,\n",
    "#    ref_model = ref_model\n",
    "#)  # note: we pass main_model.model because PPOTrainer checks for one of several supported model types ...\n",
    "# ... main_model.model is a model with adapters, which is supported. main_model itself is a wrapper that is not supported"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "74b0875944e047e6a4d09cc988013ae8",
      "bc859c36e95646b78e9a08111ce1735b",
      "98156b41fab9493cac7eb8edfd1f611a",
      "f0adf0ab77d74ff3857ffa5b9b1a0373",
      "6d3c3f5bfad345f68b6e62ab870e69bf",
      "5cdc9539cca3499abb4958d40142d77c",
      "a984b403d0464e88b4539d586dfc4cc2",
      "23db3f8d31cc4c5d98af465767af45af",
      "4922d11311b846f59278623ec5b6dd39",
      "c487b0154d434955ba8070253af01e1c",
      "c270953012ea437fa8ff299292a41837"
     ]
    },
    "id": "eYr-w666-QfK",
    "outputId": "96206a86-ee25-4e74-a950-f52be855cc24"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a Qwen2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n",
      "  0%|          | 1/400 [00:36<4:00:25, 36.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------ STEP 0 ------------------------------\n",
      "rewards/mean:\t-4.175922394\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t-1.166327119\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t0.000000000\t<---- how far we are from the original model (regularizer)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2/400 [01:12<3:59:00, 36.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------ STEP 1 ------------------------------\n",
      "rewards/mean:\t-3.899804592\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t-1.071911097\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t0.872269511\t<---- how far we are from the original model (regularizer)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 3/400 [01:48<3:58:06, 35.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------ STEP 2 ------------------------------\n",
      "rewards/mean:\t-3.265693188\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t-1.289139748\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t2.414529324\t<---- how far we are from the original model (regularizer)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 4/400 [02:24<3:57:33, 35.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------ STEP 3 ------------------------------\n",
      "rewards/mean:\t-4.302227020\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t-1.448977470\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t2.672339439\t<---- how far we are from the original model (regularizer)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|‚ñè         | 5/400 [02:59<3:56:50, 35.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------ STEP 4 ------------------------------\n",
      "rewards/mean:\t-4.048124790\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t-1.649021268\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t2.877550602\t<---- how far we are from the original model (regularizer)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|‚ñè         | 6/400 [03:35<3:56:07, 35.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------ STEP 5 ------------------------------\n",
      "rewards/mean:\t-4.035696030\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t-1.791702032\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t5.509695053\t<---- how far we are from the original model (regularizer)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|‚ñè         | 7/400 [04:11<3:55:18, 35.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------ STEP 6 ------------------------------\n",
      "rewards/mean:\t-4.132367134\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t-2.161994457\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t4.030705929\t<---- how far we are from the original model (regularizer)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|‚ñè         | 8/400 [04:47<3:54:03, 35.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------ STEP 7 ------------------------------\n",
      "rewards/mean:\t-4.124881744\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t-2.386799335\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t7.456966400\t<---- how far we are from the original model (regularizer)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|‚ñè         | 9/400 [05:22<3:53:01, 35.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------ STEP 8 ------------------------------\n",
      "rewards/mean:\t-4.332904339\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t-2.838080406\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t9.744253159\t<---- how far we are from the original model (regularizer)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|‚ñé         | 10/400 [05:58<3:52:07, 35.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------ STEP 9 ------------------------------\n",
      "rewards/mean:\t-4.208827496\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t-3.112238884\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t7.909017086\t<---- how far we are from the original model (regularizer)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|‚ñé         | 11/400 [06:34<3:51:11, 35.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------ STEP 10 ------------------------------\n",
      "rewards/mean:\t-4.175552368\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t-3.303472519\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t8.268140793\t<---- how far we are from the original model (regularizer)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|‚ñé         | 12/400 [07:09<3:50:19, 35.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------ STEP 11 ------------------------------\n",
      "rewards/mean:\t-4.322421074\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t-3.446052074\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t6.064348221\t<---- how far we are from the original model (regularizer)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|‚ñé         | 13/400 [07:45<3:49:30, 35.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------ STEP 12 ------------------------------\n",
      "rewards/mean:\t-4.166773319\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t-3.614625931\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t4.555072784\t<---- how far we are from the original model (regularizer)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|‚ñé         | 14/400 [08:20<3:48:48, 35.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------ STEP 13 ------------------------------\n",
      "rewards/mean:\t-3.709139347\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t-3.640481949\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t2.755241632\t<---- how far we are from the original model (regularizer)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|‚ñç         | 15/400 [08:56<3:48:05, 35.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------ STEP 14 ------------------------------\n",
      "rewards/mean:\t-4.289274693\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t-3.788975239\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t2.622034311\t<---- how far we are from the original model (regularizer)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|‚ñç         | 16/400 [09:31<3:47:27, 35.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------ STEP 15 ------------------------------\n",
      "rewards/mean:\t-4.050623417\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t-3.893282175\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t5.670722008\t<---- how far we are from the original model (regularizer)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|‚ñç         | 17/400 [10:07<3:46:48, 35.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------ STEP 16 ------------------------------\n",
      "rewards/mean:\t-4.149399757\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t-3.920900822\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t-0.839067042\t<---- how far we are from the original model (regularizer)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|‚ñç         | 18/400 [10:42<3:46:10, 35.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------ STEP 17 ------------------------------\n",
      "rewards/mean:\t-3.699618816\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t-3.982594490\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t5.080220222\t<---- how far we are from the original model (regularizer)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|‚ñç         | 19/400 [11:18<3:45:50, 35.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------ STEP 18 ------------------------------\n",
      "rewards/mean:\t-3.955967903\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t-3.952989817\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t4.290187359\t<---- how far we are from the original model (regularizer)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|‚ñå         | 20/400 [11:53<3:45:08, 35.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------ STEP 19 ------------------------------\n",
      "rewards/mean:\t-4.195517540\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t-4.153410912\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t4.186978340\t<---- how far we are from the original model (regularizer)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/trl/trainer/ppo_trainer.py:1222: UserWarning: The average ratio of batch (23.55) exceeds threshold 10.00. Skipping batch.\n",
      "  warnings.warn(\n",
      "  5%|‚ñå         | 21/400 [12:29<3:44:31, 35.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------ STEP 20 ------------------------------\n",
      "rewards/mean:\t-3.728546143\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t-4.302712440\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t5.954794884\t<---- how far we are from the original model (regularizer)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|‚ñå         | 22/400 [13:05<3:44:02, 35.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------ STEP 21 ------------------------------\n",
      "rewards/mean:\t-4.213376522\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t-4.188117027\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t-0.857772827\t<---- how far we are from the original model (regularizer)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|‚ñå         | 23/400 [13:40<3:43:21, 35.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------ STEP 22 ------------------------------\n",
      "rewards/mean:\t-4.030788422\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t-4.117046356\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t1.570621490\t<---- how far we are from the original model (regularizer)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|‚ñå         | 24/400 [14:16<3:42:38, 35.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------ STEP 23 ------------------------------\n",
      "rewards/mean:\t-4.151571751\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t-4.123609543\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t0.806988299\t<---- how far we are from the original model (regularizer)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|‚ñã         | 25/400 [14:52<3:43:14, 35.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------ STEP 24 ------------------------------\n",
      "rewards/mean:\t-4.021502495\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t-4.108367443\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t0.343358874\t<---- how far we are from the original model (regularizer)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|‚ñã         | 26/400 [15:28<3:42:53, 35.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------ STEP 25 ------------------------------\n",
      "rewards/mean:\t-4.127995968\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t-4.040045738\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t0.817863047\t<---- how far we are from the original model (regularizer)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|‚ñã         | 27/400 [16:03<3:42:00, 35.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------ STEP 26 ------------------------------\n",
      "rewards/mean:\t-4.007214069\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t-4.121275902\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t0.015367478\t<---- how far we are from the original model (regularizer)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|‚ñã         | 28/400 [16:39<3:41:20, 35.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------ STEP 27 ------------------------------\n",
      "rewards/mean:\t-3.446627617\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t-3.966015339\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t0.113904886\t<---- how far we are from the original model (regularizer)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|‚ñã         | 29/400 [17:14<3:40:31, 35.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------ STEP 28 ------------------------------\n",
      "rewards/mean:\t-4.002236366\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t-3.995346546\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t5.772924423\t<---- how far we are from the original model (regularizer)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|‚ñä         | 30/400 [17:50<3:39:42, 35.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------ STEP 29 ------------------------------\n",
      "rewards/mean:\t-3.976828814\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t-3.893993855\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t0.814350963\t<---- how far we are from the original model (regularizer)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|‚ñä         | 31/400 [18:26<3:38:59, 35.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------ STEP 30 ------------------------------\n",
      "rewards/mean:\t-3.797915936\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t-4.006311417\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t2.385004759\t<---- how far we are from the original model (regularizer)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|‚ñä         | 32/400 [19:01<3:38:20, 35.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------ STEP 31 ------------------------------\n",
      "rewards/mean:\t-4.312768936\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t-4.012841702\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t3.294854879\t<---- how far we are from the original model (regularizer)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|‚ñä         | 33/400 [19:37<3:37:34, 35.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------ STEP 32 ------------------------------\n",
      "rewards/mean:\t-3.989588737\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t-3.944798708\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t0.106149584\t<---- how far we are from the original model (regularizer)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|‚ñä         | 34/400 [20:12<3:36:58, 35.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------ STEP 33 ------------------------------\n",
      "rewards/mean:\t-4.009396553\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t-4.341302395\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t8.358291626\t<---- how far we are from the original model (regularizer)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|‚ñâ         | 35/400 [20:48<3:36:20, 35.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------ STEP 34 ------------------------------\n",
      "rewards/mean:\t-3.975738287\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t-4.120104790\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t1.495991111\t<---- how far we are from the original model (regularizer)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|‚ñâ         | 36/400 [21:23<3:35:48, 35.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------ STEP 35 ------------------------------\n",
      "rewards/mean:\t-3.987561464\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t-3.970894814\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t1.608974218\t<---- how far we are from the original model (regularizer)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|‚ñâ         | 37/400 [21:59<3:35:11, 35.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------ STEP 36 ------------------------------\n",
      "rewards/mean:\t-3.771327734\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t-4.005966663\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t3.389626741\t<---- how far we are from the original model (regularizer)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|‚ñâ         | 38/400 [22:34<3:34:30, 35.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------ STEP 37 ------------------------------\n",
      "rewards/mean:\t-3.894205809\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t-4.113459110\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t4.025077343\t<---- how far we are from the original model (regularizer)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|‚ñâ         | 39/400 [23:10<3:34:06, 35.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------ STEP 38 ------------------------------\n",
      "rewards/mean:\t-3.950891018\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t-4.028451920\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t3.672041655\t<---- how far we are from the original model (regularizer)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|‚ñà         | 40/400 [23:46<3:33:43, 35.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------ STEP 39 ------------------------------\n",
      "rewards/mean:\t-3.011002302\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t-3.924649477\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t8.281592369\t<---- how far we are from the original model (regularizer)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|‚ñà         | 41/400 [24:22<3:33:27, 35.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------ STEP 40 ------------------------------\n",
      "rewards/mean:\t-2.707557917\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t-3.637537003\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t5.391723633\t<---- how far we are from the original model (regularizer)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|‚ñà         | 42/400 [24:57<3:32:51, 35.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------ STEP 41 ------------------------------\n",
      "rewards/mean:\t-4.034914017\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t-3.574986935\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t4.675126076\t<---- how far we are from the original model (regularizer)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|‚ñà         | 43/400 [25:33<3:32:20, 35.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------ STEP 42 ------------------------------\n",
      "rewards/mean:\t-3.679249287\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t-3.489160299\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t1.425315857\t<---- how far we are from the original model (regularizer)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|‚ñà         | 44/400 [26:09<3:31:53, 35.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------ STEP 43 ------------------------------\n",
      "rewards/mean:\t-4.131515026\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t-3.768910885\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t3.984617949\t<---- how far we are from the original model (regularizer)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|‚ñà‚ñè        | 45/400 [26:44<3:31:21, 35.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------ STEP 44 ------------------------------\n",
      "rewards/mean:\t-3.695235252\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t-3.705678463\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t2.533928633\t<---- how far we are from the original model (regularizer)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|‚ñà‚ñè        | 46/400 [27:20<3:30:30, 35.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------ STEP 45 ------------------------------\n",
      "rewards/mean:\t-4.243442535\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t-3.983992815\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t1.930464149\t<---- how far we are from the original model (regularizer)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|‚ñà‚ñè        | 47/400 [27:56<3:29:54, 35.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------ STEP 46 ------------------------------\n",
      "rewards/mean:\t-3.566744566\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t-3.989281178\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t3.401165962\t<---- how far we are from the original model (regularizer)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|‚ñà‚ñè        | 48/400 [28:31<3:29:21, 35.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------ STEP 47 ------------------------------\n",
      "rewards/mean:\t-3.999734402\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t-4.163747787\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t4.504860878\t<---- how far we are from the original model (regularizer)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|‚ñà‚ñè        | 49/400 [29:07<3:28:48, 35.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------ STEP 48 ------------------------------\n",
      "rewards/mean:\t-3.162134886\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t-3.571831226\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t2.193789482\t<---- how far we are from the original model (regularizer)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|‚ñà‚ñé        | 50/400 [29:43<3:28:15, 35.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------ STEP 49 ------------------------------\n",
      "rewards/mean:\t-3.851660252\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t-3.945781946\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t3.157647610\t<---- how far we are from the original model (regularizer)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|‚ñà‚ñé        | 51/400 [30:19<3:27:43, 35.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------ STEP 50 ------------------------------\n",
      "rewards/mean:\t-3.947991848\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t-3.880743980\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t4.846345901\t<---- how far we are from the original model (regularizer)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|‚ñà‚ñé        | 52/400 [30:54<3:26:57, 35.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------ STEP 51 ------------------------------\n",
      "rewards/mean:\t-3.683741570\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t-3.958589554\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t3.788043737\t<---- how far we are from the original model (regularizer)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|‚ñà‚ñé        | 53/400 [31:30<3:26:18, 35.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------ STEP 52 ------------------------------\n",
      "rewards/mean:\t-3.920167923\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t-3.899722099\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t4.346845150\t<---- how far we are from the original model (regularizer)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|‚ñà‚ñé        | 54/400 [32:06<3:25:44, 35.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------ STEP 53 ------------------------------\n",
      "rewards/mean:\t-4.112700462\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t-3.990399599\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t0.645563841\t<---- how far we are from the original model (regularizer)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|‚ñà‚ñç        | 55/400 [32:41<3:25:09, 35.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------ STEP 54 ------------------------------\n",
      "rewards/mean:\t-4.048261642\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t-3.981843472\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t2.199345827\t<---- how far we are from the original model (regularizer)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|‚ñà‚ñç        | 56/400 [33:17<3:24:35, 35.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------ STEP 55 ------------------------------\n",
      "rewards/mean:\t-4.039551258\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t-3.934265614\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t1.330190659\t<---- how far we are from the original model (regularizer)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|‚ñà‚ñç        | 57/400 [33:53<3:24:12, 35.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------ STEP 56 ------------------------------\n",
      "rewards/mean:\t-4.102637768\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t-4.087630272\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t5.666754246\t<---- how far we are from the original model (regularizer)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|‚ñà‚ñç        | 58/400 [34:28<3:23:31, 35.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------ STEP 57 ------------------------------\n",
      "rewards/mean:\t-3.883380890\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t-4.129515171\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t4.581894398\t<---- how far we are from the original model (regularizer)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|‚ñà‚ñç        | 59/400 [35:04<3:22:46, 35.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------ STEP 58 ------------------------------\n",
      "rewards/mean:\t-3.916401386\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t-4.139663696\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t3.310620546\t<---- how far we are from the original model (regularizer)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/trl/trainer/ppo_trainer.py:1222: UserWarning: The average ratio of batch (63.73) exceeds threshold 10.00. Skipping batch.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/trl/trainer/ppo_trainer.py:1222: UserWarning: The average ratio of batch (250.95) exceeds threshold 10.00. Skipping batch.\n",
      "  warnings.warn(\n",
      " 15%|‚ñà‚ñå        | 60/400 [35:40<3:22:03, 35.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------ STEP 59 ------------------------------\n",
      "rewards/mean:\t-3.799159527\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t-4.127871990\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t1.286974669\t<---- how far we are from the original model (regularizer)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|‚ñà‚ñå        | 61/400 [36:15<3:21:33, 35.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------ STEP 60 ------------------------------\n",
      "rewards/mean:\t-3.771568775\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t-4.254810333\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t-0.169693649\t<---- how far we are from the original model (regularizer)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|‚ñà‚ñå        | 62/400 [36:51<3:20:51, 35.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------ STEP 61 ------------------------------\n",
      "rewards/mean:\t-3.941827774\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t-4.218454361\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t1.119666338\t<---- how far we are from the original model (regularizer)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/trl/trainer/ppo_trainer.py:1289: UserWarning: KL divergence is starting to become negative: -3.56 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
      "  warnings.warn(\n",
      " 16%|‚ñà‚ñå        | 63/400 [37:27<3:20:35, 35.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------ STEP 62 ------------------------------\n",
      "rewards/mean:\t-3.613059521\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t-4.001098633\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t-3.557565928\t<---- how far we are from the original model (regularizer)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|‚ñà‚ñå        | 64/400 [38:03<3:20:14, 35.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------ STEP 63 ------------------------------\n",
      "rewards/mean:\t-4.069221497\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t-3.978434563\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t1.335178852\t<---- how far we are from the original model (regularizer)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|‚ñà‚ñã        | 65/400 [38:38<3:19:46, 35.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------ STEP 64 ------------------------------\n",
      "rewards/mean:\t-4.171380997\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t-4.039078712\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t-0.521329701\t<---- how far we are from the original model (regularizer)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|‚ñà‚ñã        | 66/400 [39:14<3:19:28, 35.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------ STEP 65 ------------------------------\n",
      "rewards/mean:\t-4.079455853\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t-3.862856150\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t3.831521273\t<---- how far we are from the original model (regularizer)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/trl/trainer/ppo_trainer.py:1222: UserWarning: The average ratio of batch (85.53) exceeds threshold 10.00. Skipping batch.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/trl/trainer/ppo_trainer.py:1222: UserWarning: The average ratio of batch (198.37) exceeds threshold 10.00. Skipping batch.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/trl/trainer/ppo_trainer.py:1289: UserWarning: KL divergence is starting to become negative: -2.07 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
      "  warnings.warn(\n",
      " 17%|‚ñà‚ñã        | 67/400 [39:50<3:18:58, 35.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------ STEP 66 ------------------------------\n",
      "rewards/mean:\t-3.764692307\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t-3.529932022\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t-2.070565462\t<---- how far we are from the original model (regularizer)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|‚ñà‚ñã        | 68/400 [40:26<3:18:34, 35.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------ STEP 67 ------------------------------\n",
      "rewards/mean:\t-3.890316963\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t-3.902612209\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t8.434905052\t<---- how far we are from the original model (regularizer)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|‚ñà‚ñã        | 69/400 [41:02<3:18:06, 35.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------ STEP 68 ------------------------------\n",
      "rewards/mean:\t-3.671891212\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t-3.597422600\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t3.257781267\t<---- how far we are from the original model (regularizer)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|‚ñà‚ñä        | 70/400 [41:38<3:17:16, 35.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------ STEP 69 ------------------------------\n",
      "rewards/mean:\t-3.368292332\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t-3.677628279\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t5.444911957\t<---- how far we are from the original model (regularizer)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|‚ñà‚ñä        | 71/400 [42:14<3:16:42, 35.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------ STEP 70 ------------------------------\n",
      "rewards/mean:\t-3.715959311\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t-3.762910843\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t5.779059410\t<---- how far we are from the original model (regularizer)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|‚ñà‚ñä        | 72/400 [42:50<3:15:46, 35.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------ STEP 71 ------------------------------\n",
      "rewards/mean:\t-3.943545341\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t-4.257308483\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t4.054949284\t<---- how far we are from the original model (regularizer)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|‚ñà‚ñä        | 73/400 [43:25<3:15:03, 35.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------ STEP 72 ------------------------------\n",
      "rewards/mean:\t-3.900552273\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t-3.927224159\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t6.596615791\t<---- how far we are from the original model (regularizer)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|‚ñà‚ñä        | 74/400 [44:01<3:14:17, 35.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------ STEP 73 ------------------------------\n",
      "rewards/mean:\t-3.811314583\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t-3.926838636\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t3.297012806\t<---- how far we are from the original model (regularizer)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/trl/trainer/ppo_trainer.py:1289: UserWarning: KL divergence is starting to become negative: -1.38 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
      "  warnings.warn(\n",
      " 19%|‚ñà‚ñâ        | 75/400 [44:37<3:13:45, 35.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------ STEP 74 ------------------------------\n",
      "rewards/mean:\t-4.017413139\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t-4.205730438\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t-1.375507832\t<---- how far we are from the original model (regularizer)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|‚ñà‚ñâ        | 76/400 [45:13<3:13:21, 35.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------ STEP 75 ------------------------------\n",
      "rewards/mean:\t-4.093875885\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t-4.157165527\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t4.869158745\t<---- how far we are from the original model (regularizer)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|‚ñà‚ñâ        | 77/400 [45:49<3:12:49, 35.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------ STEP 76 ------------------------------\n",
      "rewards/mean:\t-4.025361061\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t-4.149724483\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t4.170752048\t<---- how far we are from the original model (regularizer)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|‚ñà‚ñâ        | 78/400 [46:24<3:12:14, 35.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------ STEP 77 ------------------------------\n",
      "rewards/mean:\t-3.758280993\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t-4.147937775\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t0.714969277\t<---- how far we are from the original model (regularizer)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|‚ñà‚ñâ        | 79/400 [47:00<3:11:30, 35.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------ STEP 78 ------------------------------\n",
      "rewards/mean:\t-3.480224133\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t-4.039023399\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t3.274314404\t<---- how far we are from the original model (regularizer)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/trl/trainer/ppo_trainer.py:1289: UserWarning: KL divergence is starting to become negative: -2.44 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
      "  warnings.warn(\n",
      " 20%|‚ñà‚ñà        | 80/400 [47:36<3:10:44, 35.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------ STEP 79 ------------------------------\n",
      "rewards/mean:\t-3.645224810\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t-3.804016829\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t-2.438596725\t<---- how far we are from the original model (regularizer)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|‚ñà‚ñà        | 81/400 [48:12<3:10:20, 35.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------ STEP 80 ------------------------------\n",
      "rewards/mean:\t-3.717756033\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t-4.048309803\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t9.234098434\t<---- how far we are from the original model (regularizer)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|‚ñà‚ñà        | 82/400 [48:48<3:09:53, 35.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------ STEP 81 ------------------------------\n",
      "rewards/mean:\t-3.528373718\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t-3.754839897\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t-0.742235303\t<---- how far we are from the original model (regularizer)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/trl/trainer/ppo_trainer.py:1289: UserWarning: KL divergence is starting to become negative: -1.79 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
      "  warnings.warn(\n",
      " 21%|‚ñà‚ñà        | 83/400 [49:24<3:09:27, 35.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------ STEP 82 ------------------------------\n",
      "rewards/mean:\t-3.963236332\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t-3.819927216\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t-1.786254287\t<---- how far we are from the original model (regularizer)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|‚ñà‚ñà        | 84/400 [49:59<3:08:48, 35.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------ STEP 83 ------------------------------\n",
      "rewards/mean:\t-3.338109016\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t-3.669633389\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t0.052164748\t<---- how far we are from the original model (regularizer)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|‚ñà‚ñà‚ñè       | 85/400 [50:36<3:08:42, 35.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------ STEP 84 ------------------------------\n",
      "rewards/mean:\t-4.016491890\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t-3.764464378\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t5.229593277\t<---- how far we are from the original model (regularizer)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/trl/trainer/ppo_trainer.py:1289: UserWarning: KL divergence is starting to become negative: -1.24 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
      "  warnings.warn(\n",
      " 22%|‚ñà‚ñà‚ñè       | 86/400 [51:12<3:08:52, 36.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------ STEP 85 ------------------------------\n",
      "rewards/mean:\t-3.420114040\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t-3.570727348\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t-1.237415075\t<---- how far we are from the original model (regularizer)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|‚ñà‚ñà‚ñè       | 87/400 [51:48<3:08:40, 36.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------ STEP 86 ------------------------------\n",
      "rewards/mean:\t-3.008074760\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t-3.382342815\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t0.133683726\t<---- how far we are from the original model (regularizer)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/trl/trainer/ppo_trainer.py:1289: UserWarning: KL divergence is starting to become negative: -4.39 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
      "  warnings.warn(\n",
      " 22%|‚ñà‚ñà‚ñè       | 88/400 [52:25<3:08:34, 36.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------ STEP 87 ------------------------------\n",
      "rewards/mean:\t-3.441480637\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t-3.232239485\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t-4.386944771\t<---- how far we are from the original model (regularizer)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/trl/trainer/ppo_trainer.py:1289: UserWarning: KL divergence is starting to become negative: -1.50 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
      "  warnings.warn(\n",
      " 22%|‚ñà‚ñà‚ñè       | 89/400 [53:01<3:08:21, 36.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------ STEP 88 ------------------------------\n",
      "rewards/mean:\t-4.058603764\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t-3.444664955\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t-1.498209238\t<---- how far we are from the original model (regularizer)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|‚ñà‚ñà‚ñè       | 89/400 [53:27<3:06:48, 36.04s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Rollout stage: generate continuations from batch queries using main_model\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m response_tensors \u001b[38;5;241m=\u001b[39m \u001b[43mppo_trainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minput_ids\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mgeneration_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# ^-- list of tensors of token ids from main model tokenizer\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# de-tokenize responses to strings (since reward model uses a different tokenizer)\u001b[39;00m\n\u001b[1;32m     17\u001b[0m batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m [main_tokenizer\u001b[38;5;241m.\u001b[39mdecode(response\u001b[38;5;241m.\u001b[39msqueeze()) \u001b[38;5;28;01mfor\u001b[39;00m response \u001b[38;5;129;01min\u001b[39;00m response_tensors]\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/trl/trainer/ppo_trainer.py:469\u001b[0m, in \u001b[0;36mPPOTrainer.generate\u001b[0;34m(self, query_tensor, length_sampler, batch_size, return_prompt, generate_ref_response, **generation_kwargs)\u001b[0m\n\u001b[1;32m    467\u001b[0m     ref_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_peft_model \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mref_model\n\u001b[1;32m    468\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(query_tensor, List):\n\u001b[0;32m--> 469\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_batched\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    470\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    471\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery_tensor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    472\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlength_sampler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlength_sampler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    473\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    474\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_prompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_prompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    475\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mgeneration_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    476\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    477\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m generate_ref_response:\n\u001b[1;32m    478\u001b[0m         ref_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate_batched(\n\u001b[1;32m    479\u001b[0m             ref_model,\n\u001b[1;32m    480\u001b[0m             query_tensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    484\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mgeneration_kwargs,\n\u001b[1;32m    485\u001b[0m         )\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/trl/trainer/ppo_trainer.py:556\u001b[0m, in \u001b[0;36mPPOTrainer._generate_batched\u001b[0;34m(self, model, query_tensors, length_sampler, batch_size, return_prompt, pad_to_multiple_of, remove_padding, **generation_kwargs)\u001b[0m\n\u001b[1;32m    547\u001b[0m padded_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39mpad(\n\u001b[1;32m    548\u001b[0m     inputs,\n\u001b[1;32m    549\u001b[0m     padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    552\u001b[0m     return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    553\u001b[0m )\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_device)\n\u001b[1;32m    555\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m unwrap_model_for_generation(model, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator) \u001b[38;5;28;01mas\u001b[39;00m unwrapped_model:\n\u001b[0;32m--> 556\u001b[0m     generations \u001b[38;5;241m=\u001b[39m \u001b[43munwrapped_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpadded_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mgeneration_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    558\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m generation, mask \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(generations, padded_inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m]):\n\u001b[1;32m    559\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_encoder_decoder:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/trl/models/modeling_value_head.py:204\u001b[0m, in \u001b[0;36mAutoModelForCausalLMWithValueHead.generate\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    193\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;124;03m    A simple wrapper around the `generate` method of the wrapped model.\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;124;03m    Please refer to the [`generate`](https://huggingface.co/docs/transformers/internal/generation_utils)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;124;03m            Keyword arguments passed to the `generate` method of the wrapped model.\u001b[39;00m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 204\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpretrained_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:2047\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   2039\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   2040\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   2041\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   2042\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   2043\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   2044\u001b[0m     )\n\u001b[1;32m   2046\u001b[0m     \u001b[38;5;66;03m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[0;32m-> 2047\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2048\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2049\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2050\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2051\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2052\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2053\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2054\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2055\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2057\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SAMPLE, GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH):\n\u001b[1;32m   2058\u001b[0m     \u001b[38;5;66;03m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[1;32m   2059\u001b[0m     beam_scorer \u001b[38;5;241m=\u001b[39m BeamSearchScorer(\n\u001b[1;32m   2060\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m   2061\u001b[0m         num_beams\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2066\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mmax_length,\n\u001b[1;32m   2067\u001b[0m     )\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:3007\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   3004\u001b[0m model_inputs\u001b[38;5;241m.\u001b[39mupdate({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_hidden_states\u001b[39m\u001b[38;5;124m\"\u001b[39m: output_hidden_states} \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;28;01melse\u001b[39;00m {})\n\u001b[1;32m   3006\u001b[0m \u001b[38;5;66;03m# forward pass to get next token\u001b[39;00m\n\u001b[0;32m-> 3007\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   3009\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m synced_gpus \u001b[38;5;129;01mand\u001b[39;00m this_peer_finished:\n\u001b[1;32m   3010\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m  \u001b[38;5;66;03m# don't waste resources running the code we don't need\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:166\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 166\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/qwen2/modeling_qwen2.py:1167\u001b[0m, in \u001b[0;36mQwen2ForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, num_logits_to_keep)\u001b[0m\n\u001b[1;32m   1164\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m   1166\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m-> 1167\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1168\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1169\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1170\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1171\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1172\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1173\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1174\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1175\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1176\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1177\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1178\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1180\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1181\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:166\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 166\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/qwen2/modeling_qwen2.py:976\u001b[0m, in \u001b[0;36mQwen2Model.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m    964\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    965\u001b[0m         decoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    966\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    973\u001b[0m         position_embeddings,\n\u001b[1;32m    974\u001b[0m     )\n\u001b[1;32m    975\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 976\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    977\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    978\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    979\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    980\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    981\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    982\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    983\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    984\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    985\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    987\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    989\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:166\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 166\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/qwen2/modeling_qwen2.py:702\u001b[0m, in \u001b[0;36mQwen2DecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001b[0m\n\u001b[1;32m    699\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_layernorm(hidden_states)\n\u001b[1;32m    701\u001b[0m \u001b[38;5;66;03m# Self Attention\u001b[39;00m\n\u001b[0;32m--> 702\u001b[0m hidden_states, self_attn_weights, present_key_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    703\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    704\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    705\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    706\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    707\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    708\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    709\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    710\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    711\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    712\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[1;32m    714\u001b[0m \u001b[38;5;66;03m# Fully Connected\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:166\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 166\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/qwen2/modeling_qwen2.py:582\u001b[0m, in \u001b[0;36mQwen2SdpaAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings)\u001b[0m\n\u001b[1;32m    580\u001b[0m query_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mq_proj(hidden_states)\n\u001b[1;32m    581\u001b[0m key_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mk_proj(hidden_states)\n\u001b[0;32m--> 582\u001b[0m value_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mv_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    584\u001b[0m query_states \u001b[38;5;241m=\u001b[39m query_states\u001b[38;5;241m.\u001b[39mview(bsz, q_len, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_dim)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    585\u001b[0m key_states \u001b[38;5;241m=\u001b[39m key_states\u001b[38;5;241m.\u001b[39mview(bsz, q_len, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_key_value_heads, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_dim)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/peft/tuners/lora.py:1219\u001b[0m, in \u001b[0;36mLinear4bit.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m   1215\u001b[0m     expected_dtype \u001b[38;5;241m=\u001b[39m result\u001b[38;5;241m.\u001b[39mdtype\n\u001b[1;32m   1216\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlora_A[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactive_adapter]\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[1;32m   1217\u001b[0m     output \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1218\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlora_B[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactive_adapter](\n\u001b[0;32m-> 1219\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlora_A[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactive_adapter](\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlora_dropout\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactive_adapter\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1220\u001b[0m         )\u001b[38;5;241m.\u001b[39mto(expected_dtype)\n\u001b[1;32m   1221\u001b[0m         \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaling[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactive_adapter]\n\u001b[1;32m   1222\u001b[0m     )\n\u001b[1;32m   1223\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1224\u001b[0m     output \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1225\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlora_B[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactive_adapter](\n\u001b[1;32m   1226\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlora_A[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactive_adapter](\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlora_dropout[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactive_adapter](x))\n\u001b[1;32m   1227\u001b[0m         )\n\u001b[1;32m   1228\u001b[0m         \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaling[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactive_adapter]\n\u001b[1;32m   1229\u001b[0m     )\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/dropout.py:70\u001b[0m, in \u001b[0;36mDropout.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:1425\u001b[0m, in \u001b[0;36mdropout\u001b[0;34m(input, p, training, inplace)\u001b[0m\n\u001b[1;32m   1422\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m p \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0.0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m p \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1.0\u001b[39m:\n\u001b[1;32m   1423\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdropout probability has to be between 0 and 1, but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mp\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1424\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[0;32m-> 1425\u001b[0m     _VF\u001b[38;5;241m.\u001b[39mdropout_(\u001b[38;5;28minput\u001b[39m, p, training) \u001b[38;5;28;01mif\u001b[39;00m inplace \u001b[38;5;28;01melse\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1426\u001b[0m )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "max_steps = 400   # can be insufficient for some tasks - watch your learning curves\n",
    "generation_kwargs = dict(\n",
    "    min_length=-1, max_new_tokens=128, do_sample=True, top_k=0, top_p=0.8, temperature=0.4, repetition_penalty=1.05, pad_token_id=main_tokenizer.eos_token_id)\n",
    "#                                  ^-- task-specific parameter!\n",
    "with tqdm(enumerate(ppo_trainer.dataloader), total=max_steps) as progressbar:\n",
    "  # note: ppo_trainer.dataloader is just a regular dataloader of queries, no RL-specific magic :)\n",
    "  for epoch, batch in progressbar:\n",
    "    if epoch >= max_steps:\n",
    "        break\n",
    "\n",
    "    # Rollout stage: generate continuations from batch queries using main_model\n",
    "    response_tensors = ppo_trainer.generate(batch['input_ids'], **generation_kwargs)\n",
    "    # ^-- list of tensors of token ids from main model tokenizer\n",
    "\n",
    "    # de-tokenize responses to strings (since reward model uses a different tokenizer)\n",
    "    batch[\"response\"] = [main_tokenizer.decode(response.squeeze()) for response in response_tensors]\n",
    "    # note: response_tensors already contain query tokens, so we don't need to add queries manually.\n",
    "    # This may not be true for other tasks: check this manually by viewing batch[\"response\"] and batch[\"query\"]\n",
    "\n",
    "\n",
    "    # Evaluation stage\n",
    "    rewards = compute_reward(batch['response'])\n",
    "\n",
    "    # Update stage\n",
    "    stats = ppo_trainer.step(batch['input_ids'], response_tensors, list(rewards.split(1)))\n",
    "    stats['rewards/mean'] = rewards.mean().item()\n",
    "\n",
    "    print(\"-\" * 30, 'STEP', epoch, '-' * 30)\n",
    "    print(f'rewards/mean:\\t{stats[\"rewards/mean\"]:.9f}\\t<---- average reward over this batch (higher=better, noisy)')\n",
    "    print(f'ppo/returns/mean:\\t{stats[\"ppo/returns/mean\"]:.9f}\\t<---- model-estimated average discounted reward')\n",
    "    print(f'objective/kl:\\t{stats[\"objective/kl\"]:.9f}\\t<---- how far we are from the original model (regularizer)')\n",
    "    print()\n",
    "\n",
    "    ppo_trainer.log_stats(stats, batch, list(rewards.split(1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–í–∞—à –≤–æ–ø—Ä–æ—Å —Å–æ–¥–µ—Ä–∂–∏—Ç –Ω–µ–Ω–æ—Ä–º–∞—Ç–∏–≤–Ω—É—é –ª–µ–∫—Å–∏–∫—É, –∫–æ—Ç–æ—Ä–∞—è –º–æ–∂–µ—Ç –±—ã—Ç—å –≤–æ—Å–ø—Ä–∏–Ω—è—Ç–∞ –∫–∞–∫ –æ—Å–∫–æ—Ä–±–∏—Ç–µ–ª—å–Ω–∞—è. –ï—Å–ª–∏ –≤–∞—à –∑–∞–ø—Ä–æ—Å –∏–º–µ–µ—Ç –¥—Ä—É–≥–æ–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –∏–ª–∏ —Ü–µ–ª—å, –ø–æ–∂–∞–ª—É–π—Å—Ç–∞, —É—Ç–æ—á–Ω–∏—Ç–µ –µ–≥–æ, —á—Ç–æ–±—ã —è –º–æ–≥ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–∏—Ç—å –±–æ–ª–µ–µ —Ç–æ—á–Ω—ã–π –æ—Ç–≤–µ—Ç.\n",
      "\n",
      "–ï—Å–ª–∏ –∂–µ —ç—Ç–æ –±—ã–ª–∞ –ø–æ–ø—ã—Ç–∫–∞ –∑–∞–¥–∞—Ç—å –∫–∞–∫–æ–π-—Ç–æ –≤–æ–ø—Ä–æ—Å –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ, –∫–æ—Ç–æ—Ä—ã–π –Ω–µ—Å–µ—Ç –≤ —Å–µ–±–µ –Ω–µ–≥–∞—Ç–∏–≤–Ω—ã–π —Å–º—ã—Å–ª (–Ω–∞–ø—Ä–∏–º–µ—Ä, \"—Ç—ã –∫—É–¥–∞ –ø–æ—à–µ–ª?\" —Å –Ω–∞–º—ë–∫–æ–º –Ω–∞ –Ω–µ–∫–∏–π –Ω–µ–≥–∞—Ç–∏–≤–Ω—ã–π –æ–ø—ã—Ç), —Ç–æ —Å—Ç–æ–∏—Ç –æ—Ç–º–µ—Ç–∏—Ç—å, —á—Ç–æ —Ç–∞–∫–∏–µ –≤–æ–ø—Ä–æ—Å—ã –º–æ–≥—É—Ç –±—ã—Ç—å –Ω–µ—É–º–µ—Å—Ç–Ω—ã –≤ –¥–∏—Å–∫—É—Å—Å–∏–∏ –∏ –ª—É—á—à–µ –ø—Ä–æ–º–æ–ª—á–∞—Ç—å.\n",
      "\n",
      "–î–ª—è —Ç–æ–≥–æ —á—Ç–æ–±—ã –ø–æ–ª—É—á–∏—Ç—å –ø–æ–ª–µ–∑–Ω—ã–π –æ—Ç–≤–µ—Ç, –ø–æ–∂–∞–ª—É–π—Å—Ç–∞, –ø–µ—Ä–µ—Ñ–æ—Ä–º—É–ª–∏—Ä—É–π—Ç–µ —Å–≤–æ–π –≤–æ–ø—Ä–æ—Å, –∏–∑–±–µ–≥–∞—è –Ω–µ–ø—Ä–∏–µ–º–ª–µ–º—ã—Ö –≤—ã—Ä–∞–∂–µ–Ω–∏–π –∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–≤.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "text= prompt_to_chat(\"–ø–æ—à–µ–ª —Ç—ã –≤ –∂–æ–ø—É\")\n",
    "model_inputs = main_tokenizer([text], return_tensors=\"pt\")#.to(main_model.device)\n",
    "\n",
    "generated_ids = main_model.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=512\n",
    ")\n",
    "generated_ids = [\n",
    "    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "]\n",
    "\n",
    "response = main_tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "4c8ff454cd947027f86954d72bf940c689a97dcc494eb53cfe4813862c6065fe"
   }
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "1692cc1532df4c2695c729a964a31da8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "23db3f8d31cc4c5d98af465767af45af": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2ca9e640d5d549658e42fd73af8ea399": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "411cd47238a94edc8283e178e04d386f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "44f3d7c434354a90bfa3d4750048b80f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4922d11311b846f59278623ec5b6dd39": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "4ad4da252cb64e818d35eb3869adc81c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1692cc1532df4c2695c729a964a31da8",
      "max": 24895,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_55c624445ca44090a2f109d3bf5f3f6a",
      "value": 24895
     }
    },
    "55c624445ca44090a2f109d3bf5f3f6a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "5cdc9539cca3499abb4958d40142d77c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "63e252cfd9f44ef79137473b618828dd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2ca9e640d5d549658e42fd73af8ea399",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_c8ea1c2d3b5040378d0f2bd213933603",
      "value": "Map: 100%"
     }
    },
    "6d3c3f5bfad345f68b6e62ab870e69bf": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "74b0875944e047e6a4d09cc988013ae8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_bc859c36e95646b78e9a08111ce1735b",
       "IPY_MODEL_98156b41fab9493cac7eb8edfd1f611a",
       "IPY_MODEL_f0adf0ab77d74ff3857ffa5b9b1a0373"
      ],
      "layout": "IPY_MODEL_6d3c3f5bfad345f68b6e62ab870e69bf"
     }
    },
    "9604bff7c9414071a55d063fdf4174fa": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "98156b41fab9493cac7eb8edfd1f611a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_23db3f8d31cc4c5d98af465767af45af",
      "max": 50,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_4922d11311b846f59278623ec5b6dd39",
      "value": 39
     }
    },
    "a984b403d0464e88b4539d586dfc4cc2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "bc859c36e95646b78e9a08111ce1735b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5cdc9539cca3499abb4958d40142d77c",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_a984b403d0464e88b4539d586dfc4cc2",
      "value": " 78%"
     }
    },
    "c270953012ea437fa8ff299292a41837": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c487b0154d434955ba8070253af01e1c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c8ea1c2d3b5040378d0f2bd213933603": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d3a8dcfad53b4de885b39ee83e6029ef": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_411cd47238a94edc8283e178e04d386f",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_9604bff7c9414071a55d063fdf4174fa",
      "value": " 24895/24895 [00:43&lt;00:00, 568.24 examples/s]"
     }
    },
    "f0adf0ab77d74ff3857ffa5b9b1a0373": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c487b0154d434955ba8070253af01e1c",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_c270953012ea437fa8ff299292a41837",
      "value": " 39/50 [51:09&lt;14:44, 80.38s/it]"
     }
    },
    "f6ba50eafdeb4b94a1e7c22c5027f709": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_63e252cfd9f44ef79137473b618828dd",
       "IPY_MODEL_4ad4da252cb64e818d35eb3869adc81c",
       "IPY_MODEL_d3a8dcfad53b4de885b39ee83e6029ef"
      ],
      "layout": "IPY_MODEL_44f3d7c434354a90bfa3d4750048b80f"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
